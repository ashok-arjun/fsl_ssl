diff --git a/io_utils.py b/io_utils.py
index 13440d0..a64ca2e 100644
--- a/io_utils.py
+++ b/io_utils.py
@@ -59,7 +59,7 @@ def parse_args(script):
         parser.add_argument('--num_classes' , default=200, type=int,help='total number of classes in softmax, only used in baseline') #make it larger than the maximum label value in base class
         parser.add_argument('--save_freq'   , default=100, type=int,help='Save frequency')
         parser.add_argument('--start_epoch' , default=0, type=int,  help='Starting epoch')
-        parser.add_argument('--stop_epoch'  , default=600, type=int,help='Stopping epoch') # for meta-learning methods, each epoch contains 100 episodes
+        parser.add_argument('--stop_epoch'  , default=400, type=int,help='Stopping epoch') # for meta-learning methods, each epoch contains 100 episodes
         parser.add_argument('--resume'      , action='store_true',  help='continue from previous trained model with largest epoch')
         parser.add_argument('--warmup'      , action='store_true',  help='continue from baseline, neglected if resume is true') #never used in the paper
 
diff --git a/methods/protonet.py b/methods/protonet.py
index aea3106..62c77b5 100644
--- a/methods/protonet.py
+++ b/methods/protonet.py
@@ -10,6 +10,8 @@ from methods.meta_template import MetaTemplate
 from model_resnet import *
 from itertools import cycle
 
+import wandb
+
 class ProtoNet(MetaTemplate):
     def __init__(self, model_func,  n_way, n_support, jigsaw=False, lbda=0.0, rotation=False, tracking=False, use_bn=True, pretrain=False):
         super(ProtoNet, self).__init__(model_func,  n_way, n_support, use_bn, pretrain)
@@ -67,31 +69,32 @@ class ProtoNet(MetaTemplate):
                 if self.jigsaw:
                     loss_jigsaw, acc_jigsaw = self.set_forward_loss_unlabel(inputs[1][2], inputs[1][3])# torch.Size([5, 21, 9, 3, 75, 75]), torch.Size([5, 21])
                     loss = (1.0-self.lbda) * loss_proto + self.lbda * loss_jigsaw
-                    writer.add_scalar('train/loss_proto', float(loss_proto.data.item()), self.global_count)
-                    writer.add_scalar('train/loss_jigsaw', float(loss_jigsaw.data.item()), self.global_count)
+                    wandb.log({'train/loss_proto': float(loss_proto.data.item())}, step=self.global_count)
+                    wandb.log({'train/loss_jigsaw': float(loss_jigsaw.data.item())}, step=self.global_count)
+
                 elif self.rotation:
                     loss_rotation, acc_rotation = self.set_forward_loss_unlabel(inputs[1][2], inputs[1][3])# torch.Size([5, 21, 9, 3, 75, 75]), torch.Size([5, 21])
                     loss = (1.0-self.lbda) * loss_proto + self.lbda * loss_rotation
-                    writer.add_scalar('train/loss_proto', float(loss_proto.data.item()), self.global_count)
-                    writer.add_scalar('train/loss_rotation', float(loss_rotation.data.item()), self.global_count)
+                    wandb.log({'train/loss_proto': float(loss_proto.data.item())}, step=self.global_count)
+                    wandb.log({'train/loss_rotation': float(loss_rotation.data.item())}, step=self.global_count)
+
                 else:
                     loss = loss_proto
                 loss.backward()
                 optimizer.step()
                 avg_loss = avg_loss+loss.data
-                writer.add_scalar('train/loss', float(loss.data.item()), self.global_count)
+                wandb.log({'train/loss': float(loss.data.item())}, step=self.global_count)
 
                 if self.jigsaw:
                     avg_loss_proto += loss_proto.data
                     avg_loss_jigsaw += loss_jigsaw.data
-                    writer.add_scalar('train/acc_proto', acc, self.global_count)
-                    writer.add_scalar('train/acc_jigsaw', acc_jigsaw, self.global_count)
+                    wandb.log({'train/acc_proto': acc}, step=self.global_count)
+                    wandb.log({'train/acc_jigsaw': acc_jigsaw}, step=self.global_count)
                 elif self.rotation:
                     avg_loss_proto += loss_proto.data
                     avg_loss_rotation += loss_rotation.data
-                    writer.add_scalar('train/acc_proto', acc, self.global_count)
-                    writer.add_scalar('train/acc_rotation', acc_rotation, self.global_count)
-
+                    wandb.log({'train/acc_proto': acc}, step=self.global_count)
+                    wandb.log({'train/acc_rotation': acc_rotation}, step=self.global_count)
                 if (i+1) % print_freq==0:
                     if self.jigsaw:
                         print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f} | Loss Proto {:f} | Loss Jigsaw {:f}'.\
@@ -113,30 +116,30 @@ class ProtoNet(MetaTemplate):
                 if self.jigsaw:
                     loss_jigsaw, acc_jigsaw = self.set_forward_loss_unlabel(inputs[2], inputs[3])# torch.Size([5, 21, 9, 3, 75, 75]), torch.Size([5, 21])
                     loss = (1.0-self.lbda) * loss_proto + self.lbda * loss_jigsaw
-                    writer.add_scalar('train/loss_proto', float(loss_proto.data.item()), self.global_count)
-                    writer.add_scalar('train/loss_jigsaw', float(loss_jigsaw.data.item()), self.global_count)
+                    wandb.log({'train/loss_proto': float(loss_proto.data.item())}, step=self.global_count)
+                    wandb.log({'train/loss_jigsaw': float(loss_jigsaw.data.item())}, step=self.global_count)
                 elif self.rotation:
                     loss_rotation, acc_rotation = self.set_forward_loss_unlabel(inputs[2], inputs[3])# torch.Size([5, 21, 9, 3, 75, 75]), torch.Size([5, 21])
                     loss = (1.0-self.lbda) * loss_proto + self.lbda * loss_rotation
-                    writer.add_scalar('train/loss_proto', float(loss_proto.data.item()), self.global_count)
-                    writer.add_scalar('train/loss_rotation', float(loss_rotation.data.item()), self.global_count)
+                    wandb.log({'train/loss_proto': float(loss_proto.data.item())}, step=self.global_count)
+                    wandb.log({'train/loss_rotation': float(loss_rotation.data.item())}, step=self.global_count)
                 else:
                     loss = loss_proto
                 loss.backward()
                 optimizer.step()
                 avg_loss = avg_loss+loss.item()
-                writer.add_scalar('train/loss', float(loss.data.item()), self.global_count)
+                wandb.log({'train/loss': float(loss.data.item())}, step=self.global_count)
 
                 if self.jigsaw:
                     avg_loss_proto += loss_proto.data
                     avg_loss_jigsaw += loss_jigsaw.data
-                    writer.add_scalar('train/acc_proto', acc, self.global_count)
-                    writer.add_scalar('train/acc_jigsaw', acc_jigsaw, self.global_count)
+                    wandb.log({'train/acc_proto': acc}, step=self.global_count)
+                    wandb.log({'train/acc_jigsaw': acc_jigsaw}, step=self.global_count)
                 elif self.rotation:
                     avg_loss_proto += loss_proto.data
                     avg_loss_rotation += loss_rotation.data
-                    writer.add_scalar('train/acc_proto', acc, self.global_count)
-                    writer.add_scalar('train/acc_rotation', acc_rotation, self.global_count)
+                    wandb.log({'train/acc_proto': acc}, step=self.global_count)
+                    wandb.log({'train/acc_rotation': acc_rotation}, step=self.global_count)
 
                 if (i+1) % print_freq==0:
                     #print(optimizer.state_dict()['param_groups'][0]['lr'])
diff --git a/train.py b/train.py
index 3ebb185..1187df9 100644
--- a/train.py
+++ b/train.py
@@ -21,6 +21,11 @@ from tensorboardX import SummaryWriter
 import json
 from model_resnet import *
 
+
+import wandb
+wandb.init(entity="meta-learners", project="fsl_ssl")
+
+
 def train(base_loader, val_loader, model, start_epoch, stop_epoch, params):    
     if params.optimization == 'Adam':
         optimizer = torch.optim.Adam(model.parameters(), lr=params.lr)
