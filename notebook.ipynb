{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link './filelists/flowers/images/images': Read-only file system\n"
     ]
    }
   ],
   "source": [
    "# %cd ./filelists/CUB/\n",
    "# !ln -s  ../../../CUB_200_2011/images ./images\n",
    "# %cd ../../\n",
    "\n",
    "# !ln -s /kaggle/input/caltech-birds-2011-dataset/CUB_200_2011/images ./filelists/CUB/images\n",
    "\n",
    "# !ln -s /kaggle/input/miniimagenet/miniImageNet/images ./filelists/miniImagenet/images\n",
    "# !ln -s /kaggle/input/d/arjun2000ashok/tieredimagenet/tiered_imagenet/ ./filelists/tieredImagenet/images\n",
    "# !ln -s /kaggle/input/stanford-dogs-dataset/images/Images ./filelists/dogs/Images\n",
    "# !ln -s /kaggle/input/stanford-cars-dataset/cars_train/cars_train ./filelists/cars/images \n",
    "# !ln -s /kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data ./filelists/cars/images \n",
    "# !ln -s /kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images ./filelists/aircrafts/images\n",
    "# !ln -s /kaggle/input/vggflowers/images ./filelists/flowers/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/fsl_ssl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_jigsaw_lbda0.50Adam_lr0.0010\n",
      "Fresh wandb run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20210608_075433-2otl6oqp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgood-sea-65\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl/runs/2otl6oqp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "About to start training. Last model and best model will be saved in wandb at every model save. \n",
      " Run save_features.py and test.py after the training completes, with the same arguments\n",
      "Epoch 0 | Batch 10/100 | Loss 12.797639 | Loss Proto 21.995384 | Loss Jigsaw 3.599893\n",
      "Epoch 0 | Batch 20/100 | Loss 8.834058 | Loss Proto 14.097727 | Loss Jigsaw 3.570389\n",
      "Epoch 0 | Batch 30/100 | Loss 7.000640 | Loss Proto 10.440374 | Loss Jigsaw 3.560908\n",
      "Epoch 0 | Batch 40/100 | Loss 5.926154 | Loss Proto 8.295093 | Loss Jigsaw 3.557215\n",
      "Epoch 0 | Batch 50/100 | Loss 5.227403 | Loss Proto 6.901468 | Loss Jigsaw 3.553335\n",
      "Epoch 0 | Batch 60/100 | Loss 4.766126 | Loss Proto 5.978988 | Loss Jigsaw 3.553263\n",
      "Epoch 0 | Batch 70/100 | Loss 4.427951 | Loss Proto 5.302829 | Loss Jigsaw 3.553074\n",
      "Epoch 0 | Batch 80/100 | Loss 4.177951 | Loss Proto 4.801950 | Loss Jigsaw 3.553952\n",
      "Epoch 0 | Batch 90/100 | Loss 3.976990 | Loss Proto 4.398911 | Loss Jigsaw 3.555070\n",
      "Epoch 0 | Batch 100/100 | Loss 3.816976 | Loss Proto 4.078184 | Loss Jigsaw 3.555768\n",
      "Epoch 1 | Batch 10/100 | Loss 2.333742 | Loss Proto 1.118193 | Loss Jigsaw 3.549290\n",
      "Epoch 1 | Batch 20/100 | Loss 2.321323 | Loss Proto 1.091826 | Loss Jigsaw 3.550819\n",
      "Epoch 1 | Batch 30/100 | Loss 2.313749 | Loss Proto 1.076132 | Loss Jigsaw 3.551366\n",
      "Epoch 1 | Batch 40/100 | Loss 2.325126 | Loss Proto 1.096921 | Loss Jigsaw 3.553331\n",
      "Epoch 1 | Batch 50/100 | Loss 2.328167 | Loss Proto 1.104093 | Loss Jigsaw 3.552240\n",
      "Epoch 1 | Batch 60/100 | Loss 2.325093 | Loss Proto 1.097928 | Loss Jigsaw 3.552258\n",
      "Epoch 1 | Batch 70/100 | Loss 2.325119 | Loss Proto 1.098055 | Loss Jigsaw 3.552182\n",
      "Epoch 1 | Batch 80/100 | Loss 2.324527 | Loss Proto 1.096615 | Loss Jigsaw 3.552439\n",
      "Epoch 1 | Batch 90/100 | Loss 2.321889 | Loss Proto 1.090510 | Loss Jigsaw 3.553269\n",
      "Epoch 1 | Batch 100/100 | Loss 2.312350 | Loss Proto 1.070696 | Loss Jigsaw 3.554004\n",
      "100 Test Protonet Acc = 63.44% +- 1.93%\n",
      "100 Test Jigsaw Acc = 3.66% +- 0.32%\n",
      "best model! save...\n",
      "Epoch 2 | Batch 10/100 | Loss 2.258460 | Loss Proto 0.973392 | Loss Jigsaw 3.543528\n",
      "Epoch 2 | Batch 20/100 | Loss 2.307984 | Loss Proto 1.069916 | Loss Jigsaw 3.546053\n",
      "Epoch 2 | Batch 30/100 | Loss 2.296966 | Loss Proto 1.046898 | Loss Jigsaw 3.547035\n",
      "Epoch 2 | Batch 40/100 | Loss 2.301122 | Loss Proto 1.052527 | Loss Jigsaw 3.549717\n",
      "Epoch 2 | Batch 50/100 | Loss 2.303423 | Loss Proto 1.058268 | Loss Jigsaw 3.548579\n",
      "Epoch 2 | Batch 60/100 | Loss 2.293672 | Loss Proto 1.037915 | Loss Jigsaw 3.549430\n",
      "Epoch 2 | Batch 70/100 | Loss 2.285028 | Loss Proto 1.020366 | Loss Jigsaw 3.549689\n",
      "Epoch 2 | Batch 80/100 | Loss 2.278735 | Loss Proto 1.007620 | Loss Jigsaw 3.549851\n",
      "Epoch 2 | Batch 90/100 | Loss 2.274804 | Loss Proto 0.998753 | Loss Jigsaw 3.550856\n",
      "Epoch 2 | Batch 100/100 | Loss 2.278841 | Loss Proto 1.006015 | Loss Jigsaw 3.551667\n",
      "Epoch 3 | Batch 10/100 | Loss 2.280269 | Loss Proto 1.019644 | Loss Jigsaw 3.540894\n",
      "Epoch 3 | Batch 20/100 | Loss 2.214992 | Loss Proto 0.886452 | Loss Jigsaw 3.543532\n",
      "Epoch 3 | Batch 30/100 | Loss 2.237593 | Loss Proto 0.930447 | Loss Jigsaw 3.544739\n",
      "Epoch 3 | Batch 40/100 | Loss 2.251423 | Loss Proto 0.955182 | Loss Jigsaw 3.547663\n",
      "Epoch 3 | Batch 50/100 | Loss 2.252905 | Loss Proto 0.959337 | Loss Jigsaw 3.546474\n",
      "Epoch 3 | Batch 60/100 | Loss 2.258155 | Loss Proto 0.968542 | Loss Jigsaw 3.547768\n",
      "Epoch 3 | Batch 70/100 | Loss 2.258624 | Loss Proto 0.968850 | Loss Jigsaw 3.548397\n",
      "Epoch 3 | Batch 80/100 | Loss 2.248989 | Loss Proto 0.949370 | Loss Jigsaw 3.548607\n",
      "Epoch 3 | Batch 90/100 | Loss 2.245485 | Loss Proto 0.941445 | Loss Jigsaw 3.549522\n",
      "Epoch 3 | Batch 100/100 | Loss 2.245565 | Loss Proto 0.940745 | Loss Jigsaw 3.550385\n",
      "Epoch 4 | Batch 10/100 | Loss 2.249469 | Loss Proto 0.959760 | Loss Jigsaw 3.539177\n",
      "Epoch 4 | Batch 20/100 | Loss 2.231377 | Loss Proto 0.920863 | Loss Jigsaw 3.541891\n",
      "Epoch 4 | Batch 30/100 | Loss 2.221968 | Loss Proto 0.900614 | Loss Jigsaw 3.543322\n",
      "Epoch 4 | Batch 40/100 | Loss 2.225255 | Loss Proto 0.904177 | Loss Jigsaw 3.546333\n",
      "Epoch 4 | Batch 50/100 | Loss 2.228047 | Loss Proto 0.910970 | Loss Jigsaw 3.545125\n",
      "Epoch 4 | Batch 60/100 | Loss 2.228322 | Loss Proto 0.910117 | Loss Jigsaw 3.546528\n",
      "Epoch 4 | Batch 70/100 | Loss 2.226491 | Loss Proto 0.905676 | Loss Jigsaw 3.547308\n",
      "Epoch 4 | Batch 80/100 | Loss 2.234053 | Loss Proto 0.920594 | Loss Jigsaw 3.547514\n",
      "Epoch 4 | Batch 90/100 | Loss 2.236927 | Loss Proto 0.925327 | Loss Jigsaw 3.548528\n",
      "Epoch 4 | Batch 100/100 | Loss 2.229244 | Loss Proto 0.909003 | Loss Jigsaw 3.549486\n",
      "Epoch 5 | Batch 10/100 | Loss 2.267877 | Loss Proto 0.997095 | Loss Jigsaw 3.538659\n",
      "Epoch 5 | Batch 20/100 | Loss 2.237943 | Loss Proto 0.934320 | Loss Jigsaw 3.541566\n",
      "Epoch 5 | Batch 30/100 | Loss 2.208110 | Loss Proto 0.873112 | Loss Jigsaw 3.543108\n",
      "Epoch 5 | Batch 40/100 | Loss 2.204216 | Loss Proto 0.862290 | Loss Jigsaw 3.546141\n",
      "Epoch 5 | Batch 50/100 | Loss 2.205818 | Loss Proto 0.866807 | Loss Jigsaw 3.544829\n",
      "Epoch 5 | Batch 60/100 | Loss 2.209945 | Loss Proto 0.873573 | Loss Jigsaw 3.546318\n",
      "Epoch 5 | Batch 70/100 | Loss 2.214976 | Loss Proto 0.883072 | Loss Jigsaw 3.546880\n",
      "Epoch 5 | Batch 80/100 | Loss 2.220677 | Loss Proto 0.894308 | Loss Jigsaw 3.547046\n",
      "Epoch 5 | Batch 90/100 | Loss 2.218356 | Loss Proto 0.888606 | Loss Jigsaw 3.548105\n",
      "Epoch 5 | Batch 100/100 | Loss 2.217762 | Loss Proto 0.886419 | Loss Jigsaw 3.549104\n",
      "Epoch 6 | Batch 10/100 | Loss 2.241591 | Loss Proto 0.945481 | Loss Jigsaw 3.537700\n",
      "Epoch 6 | Batch 20/100 | Loss 2.235038 | Loss Proto 0.929372 | Loss Jigsaw 3.540704\n",
      "Epoch 6 | Batch 30/100 | Loss 2.225448 | Loss Proto 0.908506 | Loss Jigsaw 3.542391\n",
      "Epoch 6 | Batch 40/100 | Loss 2.203742 | Loss Proto 0.862292 | Loss Jigsaw 3.545191\n",
      "Epoch 6 | Batch 50/100 | Loss 2.207405 | Loss Proto 0.870654 | Loss Jigsaw 3.544155\n",
      "Epoch 6 | Batch 60/100 | Loss 2.200425 | Loss Proto 0.855101 | Loss Jigsaw 3.545749\n",
      "Epoch 6 | Batch 70/100 | Loss 2.205520 | Loss Proto 0.864537 | Loss Jigsaw 3.546504\n",
      "Epoch 6 | Batch 80/100 | Loss 2.200180 | Loss Proto 0.853662 | Loss Jigsaw 3.546699\n",
      "Epoch 6 | Batch 90/100 | Loss 2.196778 | Loss Proto 0.845688 | Loss Jigsaw 3.547868\n",
      "Epoch 6 | Batch 100/100 | Loss 2.193834 | Loss Proto 0.838769 | Loss Jigsaw 3.548899\n",
      "Epoch 7 | Batch 10/100 | Loss 2.234192 | Loss Proto 0.934626 | Loss Jigsaw 3.533758\n",
      "Epoch 7 | Batch 20/100 | Loss 2.208091 | Loss Proto 0.877495 | Loss Jigsaw 3.538687\n",
      "Epoch 7 | Batch 30/100 | Loss 2.210796 | Loss Proto 0.880105 | Loss Jigsaw 3.541487\n",
      "Epoch 7 | Batch 40/100 | Loss 2.197907 | Loss Proto 0.850180 | Loss Jigsaw 3.545635\n",
      "Epoch 7 | Batch 50/100 | Loss 2.200097 | Loss Proto 0.854584 | Loss Jigsaw 3.545610\n",
      "Epoch 7 | Batch 60/100 | Loss 2.198081 | Loss Proto 0.848872 | Loss Jigsaw 3.547289\n",
      "Epoch 7 | Batch 70/100 | Loss 2.198476 | Loss Proto 0.849077 | Loss Jigsaw 3.547875\n",
      "Epoch 7 | Batch 80/100 | Loss 2.201657 | Loss Proto 0.854923 | Loss Jigsaw 3.548391\n",
      "Epoch 7 | Batch 90/100 | Loss 2.197480 | Loss Proto 0.845678 | Loss Jigsaw 3.549282\n",
      "Epoch 7 | Batch 100/100 | Loss 2.194831 | Loss Proto 0.839351 | Loss Jigsaw 3.550311\n",
      "Epoch 8 | Batch 10/100 | Loss 2.155467 | Loss Proto 0.773097 | Loss Jigsaw 3.537837\n",
      "Epoch 8 | Batch 20/100 | Loss 2.164509 | Loss Proto 0.787706 | Loss Jigsaw 3.541312\n",
      "Epoch 8 | Batch 30/100 | Loss 2.164283 | Loss Proto 0.785328 | Loss Jigsaw 3.543237\n",
      "Epoch 8 | Batch 40/100 | Loss 2.166016 | Loss Proto 0.785952 | Loss Jigsaw 3.546081\n",
      "Epoch 8 | Batch 50/100 | Loss 2.161231 | Loss Proto 0.777633 | Loss Jigsaw 3.544828\n",
      "Epoch 8 | Batch 60/100 | Loss 2.165129 | Loss Proto 0.783896 | Loss Jigsaw 3.546363\n",
      "Epoch 8 | Batch 70/100 | Loss 2.174800 | Loss Proto 0.802713 | Loss Jigsaw 3.546888\n",
      "Epoch 8 | Batch 80/100 | Loss 2.179937 | Loss Proto 0.812753 | Loss Jigsaw 3.547122\n",
      "Epoch 8 | Batch 90/100 | Loss 2.181531 | Loss Proto 0.815036 | Loss Jigsaw 3.548026\n",
      "Epoch 8 | Batch 100/100 | Loss 2.175928 | Loss Proto 0.802807 | Loss Jigsaw 3.549048\n",
      "Epoch 9 | Batch 10/100 | Loss 2.238434 | Loss Proto 0.942678 | Loss Jigsaw 3.534190\n",
      "Epoch 9 | Batch 20/100 | Loss 2.201643 | Loss Proto 0.865055 | Loss Jigsaw 3.538232\n",
      "Epoch 9 | Batch 30/100 | Loss 2.196012 | Loss Proto 0.851501 | Loss Jigsaw 3.540522\n",
      "Epoch 9 | Batch 40/100 | Loss 2.192174 | Loss Proto 0.840558 | Loss Jigsaw 3.543789\n",
      "Epoch 9 | Batch 50/100 | Loss 2.184736 | Loss Proto 0.825953 | Loss Jigsaw 3.543518\n",
      "Epoch 9 | Batch 60/100 | Loss 2.183821 | Loss Proto 0.821501 | Loss Jigsaw 3.546142\n",
      "Epoch 9 | Batch 70/100 | Loss 2.184961 | Loss Proto 0.822655 | Loss Jigsaw 3.547267\n",
      "Epoch 9 | Batch 80/100 | Loss 2.184395 | Loss Proto 0.820782 | Loss Jigsaw 3.548008\n",
      "Epoch 9 | Batch 90/100 | Loss 2.182223 | Loss Proto 0.815606 | Loss Jigsaw 3.548841\n",
      "Epoch 9 | Batch 100/100 | Loss 2.184865 | Loss Proto 0.820051 | Loss Jigsaw 3.549680\n",
      "Epoch 10 | Batch 10/100 | Loss 2.161634 | Loss Proto 0.790147 | Loss Jigsaw 3.533122\n",
      "Epoch 10 | Batch 20/100 | Loss 2.150087 | Loss Proto 0.761013 | Loss Jigsaw 3.539160\n",
      "Epoch 10 | Batch 30/100 | Loss 2.121473 | Loss Proto 0.700848 | Loss Jigsaw 3.542098\n",
      "Epoch 10 | Batch 40/100 | Loss 2.130071 | Loss Proto 0.714409 | Loss Jigsaw 3.545732\n",
      "Epoch 10 | Batch 50/100 | Loss 2.143355 | Loss Proto 0.741847 | Loss Jigsaw 3.544862\n",
      "Epoch 10 | Batch 60/100 | Loss 2.154423 | Loss Proto 0.762418 | Loss Jigsaw 3.546427\n",
      "Epoch 10 | Batch 70/100 | Loss 2.155738 | Loss Proto 0.764531 | Loss Jigsaw 3.546946\n",
      "Epoch 10 | Batch 80/100 | Loss 2.150828 | Loss Proto 0.754654 | Loss Jigsaw 3.547003\n",
      "Epoch 10 | Batch 90/100 | Loss 2.148699 | Loss Proto 0.749629 | Loss Jigsaw 3.547770\n",
      "Epoch 10 | Batch 100/100 | Loss 2.148054 | Loss Proto 0.747499 | Loss Jigsaw 3.548609\n",
      "Epoch 11 | Batch 10/100 | Loss 2.132496 | Loss Proto 0.731507 | Loss Jigsaw 3.533485\n",
      "Epoch 11 | Batch 20/100 | Loss 2.131091 | Loss Proto 0.724258 | Loss Jigsaw 3.537924\n",
      "Epoch 11 | Batch 30/100 | Loss 2.124579 | Loss Proto 0.708386 | Loss Jigsaw 3.540772\n",
      "Epoch 11 | Batch 40/100 | Loss 2.121581 | Loss Proto 0.698751 | Loss Jigsaw 3.544410\n",
      "Epoch 11 | Batch 50/100 | Loss 2.129919 | Loss Proto 0.716285 | Loss Jigsaw 3.543554\n",
      "Epoch 11 | Batch 60/100 | Loss 2.132499 | Loss Proto 0.719735 | Loss Jigsaw 3.545263\n",
      "Epoch 11 | Batch 70/100 | Loss 2.126748 | Loss Proto 0.707590 | Loss Jigsaw 3.545907\n",
      "Epoch 11 | Batch 80/100 | Loss 2.126339 | Loss Proto 0.706888 | Loss Jigsaw 3.545789\n",
      "Epoch 11 | Batch 90/100 | Loss 2.136889 | Loss Proto 0.727243 | Loss Jigsaw 3.546536\n",
      "Epoch 11 | Batch 100/100 | Loss 2.142166 | Loss Proto 0.736882 | Loss Jigsaw 3.547451\n",
      "Epoch 12 | Batch 10/100 | Loss 2.164740 | Loss Proto 0.796560 | Loss Jigsaw 3.532920\n",
      "Epoch 12 | Batch 20/100 | Loss 2.183226 | Loss Proto 0.828755 | Loss Jigsaw 3.537697\n",
      "Epoch 12 | Batch 30/100 | Loss 2.165801 | Loss Proto 0.793455 | Loss Jigsaw 3.538148\n",
      "Epoch 12 | Batch 40/100 | Loss 2.158445 | Loss Proto 0.776401 | Loss Jigsaw 3.540488\n",
      "Epoch 12 | Batch 50/100 | Loss 2.152420 | Loss Proto 0.764799 | Loss Jigsaw 3.540041\n",
      "Epoch 12 | Batch 60/100 | Loss 2.158714 | Loss Proto 0.774743 | Loss Jigsaw 3.542685\n",
      "Epoch 12 | Batch 70/100 | Loss 2.156486 | Loss Proto 0.769989 | Loss Jigsaw 3.542984\n",
      "Epoch 12 | Batch 80/100 | Loss 2.149370 | Loss Proto 0.753394 | Loss Jigsaw 3.545346\n",
      "Epoch 12 | Batch 90/100 | Loss 2.146566 | Loss Proto 0.746103 | Loss Jigsaw 3.547031\n",
      "Epoch 12 | Batch 100/100 | Loss 2.148449 | Loss Proto 0.748576 | Loss Jigsaw 3.548323\n",
      "Epoch 13 | Batch 10/100 | Loss 2.117167 | Loss Proto 0.705874 | Loss Jigsaw 3.528460\n",
      "Epoch 13 | Batch 20/100 | Loss 2.118023 | Loss Proto 0.699887 | Loss Jigsaw 3.536160\n",
      "Epoch 13 | Batch 30/100 | Loss 2.119544 | Loss Proto 0.699174 | Loss Jigsaw 3.539916\n",
      "Epoch 13 | Batch 40/100 | Loss 2.130362 | Loss Proto 0.715926 | Loss Jigsaw 3.544799\n",
      "Epoch 13 | Batch 50/100 | Loss 2.128766 | Loss Proto 0.712624 | Loss Jigsaw 3.544909\n",
      "Epoch 13 | Batch 60/100 | Loss 2.124973 | Loss Proto 0.702942 | Loss Jigsaw 3.547006\n",
      "Epoch 13 | Batch 70/100 | Loss 2.126029 | Loss Proto 0.704339 | Loss Jigsaw 3.547720\n",
      "Epoch 13 | Batch 80/100 | Loss 2.121710 | Loss Proto 0.695806 | Loss Jigsaw 3.547616\n",
      "Epoch 13 | Batch 90/100 | Loss 2.124218 | Loss Proto 0.699964 | Loss Jigsaw 3.548474\n",
      "Epoch 13 | Batch 100/100 | Loss 2.130317 | Loss Proto 0.711375 | Loss Jigsaw 3.549261\n",
      "Epoch 14 | Batch 10/100 | Loss 2.113105 | Loss Proto 0.692209 | Loss Jigsaw 3.534000\n",
      "Epoch 14 | Batch 20/100 | Loss 2.095529 | Loss Proto 0.653577 | Loss Jigsaw 3.537481\n",
      "Epoch 14 | Batch 30/100 | Loss 2.112801 | Loss Proto 0.685855 | Loss Jigsaw 3.539747\n",
      "Epoch 14 | Batch 40/100 | Loss 2.133547 | Loss Proto 0.723659 | Loss Jigsaw 3.543435\n",
      "Epoch 14 | Batch 50/100 | Loss 2.139922 | Loss Proto 0.736899 | Loss Jigsaw 3.542945\n",
      "Epoch 14 | Batch 60/100 | Loss 2.139066 | Loss Proto 0.733107 | Loss Jigsaw 3.545026\n",
      "Epoch 14 | Batch 70/100 | Loss 2.141507 | Loss Proto 0.736966 | Loss Jigsaw 3.546049\n",
      "Epoch 14 | Batch 80/100 | Loss 2.135918 | Loss Proto 0.725922 | Loss Jigsaw 3.545915\n",
      "Epoch 14 | Batch 90/100 | Loss 2.142125 | Loss Proto 0.737502 | Loss Jigsaw 3.546749\n",
      "Epoch 14 | Batch 100/100 | Loss 2.143610 | Loss Proto 0.739651 | Loss Jigsaw 3.547568\n",
      "Epoch 15 | Batch 10/100 | Loss 2.104031 | Loss Proto 0.675436 | Loss Jigsaw 3.532626\n",
      "Epoch 15 | Batch 20/100 | Loss 2.144532 | Loss Proto 0.752270 | Loss Jigsaw 3.536793\n",
      "Epoch 15 | Batch 30/100 | Loss 2.145281 | Loss Proto 0.751144 | Loss Jigsaw 3.539418\n",
      "Epoch 15 | Batch 40/100 | Loss 2.151088 | Loss Proto 0.758970 | Loss Jigsaw 3.543205\n",
      "Epoch 15 | Batch 50/100 | Loss 2.152318 | Loss Proto 0.762014 | Loss Jigsaw 3.542621\n",
      "Epoch 15 | Batch 60/100 | Loss 2.152298 | Loss Proto 0.760070 | Loss Jigsaw 3.544526\n",
      "Epoch 15 | Batch 70/100 | Loss 2.147385 | Loss Proto 0.749323 | Loss Jigsaw 3.545448\n",
      "Epoch 15 | Batch 80/100 | Loss 2.147342 | Loss Proto 0.749276 | Loss Jigsaw 3.545408\n",
      "Epoch 15 | Batch 90/100 | Loss 2.141457 | Loss Proto 0.736604 | Loss Jigsaw 3.546309\n",
      "Epoch 15 | Batch 100/100 | Loss 2.143714 | Loss Proto 0.740236 | Loss Jigsaw 3.547192\n",
      "Epoch 16 | Batch 10/100 | Loss 2.105436 | Loss Proto 0.678573 | Loss Jigsaw 3.532299\n",
      "Epoch 16 | Batch 20/100 | Loss 2.113804 | Loss Proto 0.690754 | Loss Jigsaw 3.536854\n",
      "Epoch 16 | Batch 30/100 | Loss 2.124020 | Loss Proto 0.708300 | Loss Jigsaw 3.539740\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset flowers --train_aug --method protonet --committed --stop_epoch 400 --jigsaw --lbda 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20210608_051230-14zlsjib\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mflowers protonet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl/runs/14zlsjib\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Checkpoint restored at  ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/last_model.tar\n",
      "The model's epoch is  319\n",
      "Please rename it to continue training\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 837\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20210608_051230-14zlsjib/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20210608_051230-14zlsjib/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _step 32589\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val/acc 90.4125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime 29963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp 1623121580\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/loss 0.05676136165857315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mflowers protonet\u001b[0m: \u001b[34mhttps://wandb.ai/multi-input/fsl_ssl/runs/14zlsjib\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !python wandb_restore.py --id 14zlsjib --path ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/last_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit after saving features and testing.\n",
      "checkpoint_dir: ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010\n",
      "USE BN: True\n",
      "outfile is features/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/novel.hdf5\n",
      "0/31\n",
      "10/31\n",
      "20/31\n",
      "30/31\n"
     ]
    }
   ],
   "source": [
    "# Do again with 200 dimensions\n",
    "# Do again with 399.tar\n",
    "\n",
    "!python save_features.py --dataset flowers --train_aug --method protonet --committed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing involves fine-tuning. Commit after testing.\n",
      "novel_file features/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/novel.hdf5\n",
      "600 Test Acc = 89.92% +- 0.51%\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataset flowers --train_aug --method protonet --committed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
