{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ./filelists/CUB/\n",
    "# !ln -s  ../../../CUB_200_2011/images ./images\n",
    "# %cd ../../\n",
    "\n",
    "# !ln -s /kaggle/input/caltech-birds-2011-dataset/CUB_200_2011/images ./filelists/CUB/images\n",
    "\n",
    "# !ln -s /kaggle/input/miniimagenet/miniImageNet/images ./filelists/miniImagenet/images\n",
    "# !ln -s /kaggle/input/d/arjun2000ashok/tieredimagenet/tiered_imagenet/ ./filelists/tieredImagenet/images\n",
    "# !ln -s /kaggle/input/stanford-dogs-dataset/images/Images ./filelists/dogs/Images\n",
    "# !ln -s /kaggle/input/stanford-cars-dataset/cars_train/cars_train ./filelists/cars/images \n",
    "# !ln -s /kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images ./filelists/aircrafts/images\n",
    "!ln -s /kaggle/input/vggflowers/images ./filelists/flowers/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010\n",
      "Resuming from wandb ID:  14zlsjib\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 14zlsjib but id 14zlsjib is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20210608_051327-14zlsjib\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mflowers protonet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl/runs/14zlsjib\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "About to start training. Last model and best model will be saved in wandb at every model save. \n",
      " Run save_features.py and test.py after the training completes, with the same arguments\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32001 < 32590; dropping {'train/loss': 0.02033142000436783}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32002 < 32590; dropping {'train/loss': 0.004809066653251648}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32003 < 32590; dropping {'train/loss': 0.12361441552639008}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32004 < 32590; dropping {'train/loss': 0.002916068769991398}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32005 < 32590; dropping {'train/loss': 0.04939417913556099}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32006 < 32590; dropping {'train/loss': 0.07306893914937973}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32007 < 32590; dropping {'train/loss': 0.012782189063727856}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32008 < 32590; dropping {'train/loss': 0.05111633986234665}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32009 < 32590; dropping {'train/loss': 0.1251228302717209}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32010 < 32590; dropping {'train/loss': 0.009111904539167881}.\n",
      "Epoch 320 | Batch 10/100 | Loss 0.047227\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32011 < 32590; dropping {'train/loss': 0.011230846866965294}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32012 < 32590; dropping {'train/loss': 0.09904927015304565}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32013 < 32590; dropping {'train/loss': 0.03541381284594536}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32014 < 32590; dropping {'train/loss': 0.027202242985367775}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32015 < 32590; dropping {'train/loss': 0.05367515608668327}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32016 < 32590; dropping {'train/loss': 0.026915937662124634}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32017 < 32590; dropping {'train/loss': 0.033329449594020844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32018 < 32590; dropping {'train/loss': 0.05597713589668274}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32019 < 32590; dropping {'train/loss': 0.005111213773488998}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32020 < 32590; dropping {'train/loss': 0.02551868185400963}.\n",
      "Epoch 320 | Batch 20/100 | Loss 0.042285\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32021 < 32590; dropping {'train/loss': 0.042820125818252563}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32022 < 32590; dropping {'train/loss': 0.10062233358621597}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32023 < 32590; dropping {'train/loss': 0.10527368634939194}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32024 < 32590; dropping {'train/loss': 0.2369067370891571}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32025 < 32590; dropping {'train/loss': 0.013521196320652962}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32026 < 32590; dropping {'train/loss': 0.07739423215389252}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32027 < 32590; dropping {'train/loss': 0.01010584831237793}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32028 < 32590; dropping {'train/loss': 0.15041325986385345}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32029 < 32590; dropping {'train/loss': 0.04700607433915138}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32030 < 32590; dropping {'train/loss': 0.10289760679006577}.\n",
      "Epoch 320 | Batch 30/100 | Loss 0.057755\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32031 < 32590; dropping {'train/loss': 0.0312383770942688}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32032 < 32590; dropping {'train/loss': 0.10982646048069}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32033 < 32590; dropping {'train/loss': 0.059374742209911346}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32034 < 32590; dropping {'train/loss': 0.10846266895532608}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32035 < 32590; dropping {'train/loss': 0.05351710319519043}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32036 < 32590; dropping {'train/loss': 0.03237345069646835}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32037 < 32590; dropping {'train/loss': 0.061087243258953094}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32038 < 32590; dropping {'train/loss': 0.05562068149447441}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32039 < 32590; dropping {'train/loss': 0.008421347476541996}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32040 < 32590; dropping {'train/loss': 0.10834778845310211}.\n",
      "Epoch 320 | Batch 40/100 | Loss 0.059023\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32041 < 32590; dropping {'train/loss': 0.1949484944343567}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32042 < 32590; dropping {'train/loss': 0.019980091601610184}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32043 < 32590; dropping {'train/loss': 0.08585827052593231}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32044 < 32590; dropping {'train/loss': 0.03419726714491844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32045 < 32590; dropping {'train/loss': 0.08850029110908508}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32046 < 32590; dropping {'train/loss': 0.01756843365728855}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32047 < 32590; dropping {'train/loss': 0.0024201178457587957}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32048 < 32590; dropping {'train/loss': 0.045189134776592255}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32049 < 32590; dropping {'train/loss': 0.009456127882003784}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32050 < 32590; dropping {'train/loss': 0.0654168576002121}.\n",
      "Epoch 320 | Batch 50/100 | Loss 0.058489\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32051 < 32590; dropping {'train/loss': 0.12397631257772446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32052 < 32590; dropping {'train/loss': 0.02005898393690586}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32053 < 32590; dropping {'train/loss': 0.027302365750074387}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32054 < 32590; dropping {'train/loss': 0.0061355032958090305}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32055 < 32590; dropping {'train/loss': 0.041844602674245834}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32056 < 32590; dropping {'train/loss': 0.033128805458545685}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32057 < 32590; dropping {'train/loss': 0.08308719098567963}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32058 < 32590; dropping {'train/loss': 0.05922205373644829}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32059 < 32590; dropping {'train/loss': 0.05193426460027695}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32060 < 32590; dropping {'train/loss': 0.022726519033312798}.\n",
      "Epoch 320 | Batch 60/100 | Loss 0.056565\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32061 < 32590; dropping {'train/loss': 0.024376600980758667}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32062 < 32590; dropping {'train/loss': 0.03468824550509453}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32063 < 32590; dropping {'train/loss': 0.12494923174381256}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32064 < 32590; dropping {'train/loss': 0.0019375768024474382}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32065 < 32590; dropping {'train/loss': 0.037198506295681}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32066 < 32590; dropping {'train/loss': 0.016061801463365555}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32067 < 32590; dropping {'train/loss': 0.0037741742562502623}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32068 < 32590; dropping {'train/loss': 0.020359378308057785}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32069 < 32590; dropping {'train/loss': 0.034798238426446915}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32070 < 32590; dropping {'train/loss': 0.0002687534433789551}.\n",
      "Epoch 320 | Batch 70/100 | Loss 0.052747\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32071 < 32590; dropping {'train/loss': 0.002743383403867483}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32072 < 32590; dropping {'train/loss': 0.009481391869485378}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32073 < 32590; dropping {'train/loss': 0.1045079380273819}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32074 < 32590; dropping {'train/loss': 0.011404488235712051}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32075 < 32590; dropping {'train/loss': 0.014951172284781933}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32076 < 32590; dropping {'train/loss': 0.003420436754822731}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32077 < 32590; dropping {'train/loss': 0.01012103259563446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32078 < 32590; dropping {'train/loss': 0.04038621857762337}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32079 < 32590; dropping {'train/loss': 0.038571614772081375}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32080 < 32590; dropping {'train/loss': 0.051730990409851074}.\n",
      "Epoch 320 | Batch 80/100 | Loss 0.049745\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32081 < 32590; dropping {'train/loss': 0.018013760447502136}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32082 < 32590; dropping {'train/loss': 0.06331010907888412}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32083 < 32590; dropping {'train/loss': 0.01939108967781067}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32084 < 32590; dropping {'train/loss': 0.03114418312907219}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32085 < 32590; dropping {'train/loss': 0.22090966999530792}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32086 < 32590; dropping {'train/loss': 0.05519414693117142}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32087 < 32590; dropping {'train/loss': 0.010011410340666771}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32088 < 32590; dropping {'train/loss': 0.06681710481643677}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32089 < 32590; dropping {'train/loss': 0.059955794364213943}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32090 < 32590; dropping {'train/loss': 0.19375862181186676}.\n",
      "Epoch 320 | Batch 90/100 | Loss 0.052423\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32091 < 32590; dropping {'train/loss': 0.08564046025276184}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32092 < 32590; dropping {'train/loss': 0.008509384468197823}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32093 < 32590; dropping {'train/loss': 0.05391831323504448}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32094 < 32590; dropping {'train/loss': 0.016445670276880264}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32095 < 32590; dropping {'train/loss': 0.13601520657539368}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32096 < 32590; dropping {'train/loss': 0.009207996539771557}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32097 < 32590; dropping {'train/loss': 0.04233991727232933}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32098 < 32590; dropping {'train/loss': 0.047537289559841156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32099 < 32590; dropping {'train/loss': 0.010480174794793129}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32100 < 32590; dropping {'train/loss': 0.03778435289859772}.\n",
      "Epoch 320 | Batch 100/100 | Loss 0.051660\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32101 < 32590; dropping {'train/loss': 0.04658745229244232}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32102 < 32590; dropping {'train/loss': 0.015750443562865257}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32103 < 32590; dropping {'train/loss': 0.07737860083580017}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32104 < 32590; dropping {'train/loss': 0.02993808686733246}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32105 < 32590; dropping {'train/loss': 0.0006385614397004247}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32106 < 32590; dropping {'train/loss': 0.05288645625114441}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32107 < 32590; dropping {'train/loss': 0.09095428884029388}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32108 < 32590; dropping {'train/loss': 0.03707767277956009}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32109 < 32590; dropping {'train/loss': 0.0035956031642854214}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32110 < 32590; dropping {'train/loss': 0.025891384109854698}.\n",
      "Epoch 321 | Batch 10/100 | Loss 0.038070\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32111 < 32590; dropping {'train/loss': 0.021113920956850052}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32112 < 32590; dropping {'train/loss': 0.0008127445471473038}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32113 < 32590; dropping {'train/loss': 0.020404059439897537}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32114 < 32590; dropping {'train/loss': 0.08853783458471298}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32115 < 32590; dropping {'train/loss': 0.04548951983451843}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32116 < 32590; dropping {'train/loss': 0.010753573849797249}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32117 < 32590; dropping {'train/loss': 0.17895475029945374}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32118 < 32590; dropping {'train/loss': 0.008901456370949745}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32119 < 32590; dropping {'train/loss': 0.04824312403798103}.\n",
      "Epoch 321 | Batch 20/100 | Loss 0.040933wandb: WARNING Step must only increase in log calls.  Step 32120 < 32590; dropping {'train/loss': 0.014747974462807178}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32121 < 32590; dropping {'train/loss': 0.03890768438577652}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32122 < 32590; dropping {'train/loss': 0.02198249101638794}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32123 < 32590; dropping {'train/loss': 0.016722436994314194}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32124 < 32590; dropping {'train/loss': 0.008129657246172428}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32125 < 32590; dropping {'train/loss': 0.0059949373826384544}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32126 < 32590; dropping {'train/loss': 0.06030070036649704}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32127 < 32590; dropping {'train/loss': 0.008150158450007439}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32128 < 32590; dropping {'train/loss': 0.005672111175954342}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32129 < 32590; dropping {'train/loss': 0.050623584538698196}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32130 < 32590; dropping {'train/loss': 0.16324731707572937}.\n",
      "Epoch 321 | Batch 30/100 | Loss 0.039946\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32131 < 32590; dropping {'train/loss': 0.010754073038697243}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32132 < 32590; dropping {'train/loss': 0.030381396412849426}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32133 < 32590; dropping {'train/loss': 0.06329818069934845}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32134 < 32590; dropping {'train/loss': 0.0029463062528520823}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32135 < 32590; dropping {'train/loss': 0.09950929135084152}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32136 < 32590; dropping {'train/loss': 0.013649907894432545}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32137 < 32590; dropping {'train/loss': 0.02619319036602974}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32138 < 32590; dropping {'train/loss': 0.00090048648416996}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32139 < 32590; dropping {'train/loss': 0.10808305442333221}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32140 < 32590; dropping {'train/loss': 0.06177733466029167}.\n",
      "Epoch 321 | Batch 40/100 | Loss 0.040397\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32141 < 32590; dropping {'train/loss': 0.06876785308122635}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32142 < 32590; dropping {'train/loss': 0.013207389041781425}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32143 < 32590; dropping {'train/loss': 0.0801413506269455}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32144 < 32590; dropping {'train/loss': 0.01932116597890854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32145 < 32590; dropping {'train/loss': 0.02081212028861046}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32146 < 32590; dropping {'train/loss': 0.08500193804502487}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32147 < 32590; dropping {'train/loss': 0.09447813779115677}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32148 < 32590; dropping {'train/loss': 0.07310671359300613}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32149 < 32590; dropping {'train/loss': 0.07032101601362228}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32150 < 32590; dropping {'train/loss': 0.0009942096658051014}.\n",
      "Epoch 321 | Batch 50/100 | Loss 0.042841\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32151 < 32590; dropping {'train/loss': 0.09234193712472916}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32152 < 32590; dropping {'train/loss': 0.10881112515926361}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32153 < 32590; dropping {'train/loss': 0.002670679474249482}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32154 < 32590; dropping {'train/loss': 0.0006845713942311704}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32155 < 32590; dropping {'train/loss': 0.010059421882033348}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32156 < 32590; dropping {'train/loss': 0.01995089463889599}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32157 < 32590; dropping {'train/loss': 0.00960023608058691}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32158 < 32590; dropping {'train/loss': 0.028419826179742813}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32159 < 32590; dropping {'train/loss': 0.07140158116817474}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32160 < 32590; dropping {'train/loss': 0.018858693540096283}.\n",
      "Epoch 321 | Batch 60/100 | Loss 0.041747\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32161 < 32590; dropping {'train/loss': 0.012115195393562317}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32162 < 32590; dropping {'train/loss': 0.013652143999934196}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32163 < 32590; dropping {'train/loss': 0.035853829234838486}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32164 < 32590; dropping {'train/loss': 0.07819990813732147}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32165 < 32590; dropping {'train/loss': 0.045686110854148865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32166 < 32590; dropping {'train/loss': 0.02736920490860939}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32167 < 32590; dropping {'train/loss': 0.008765583857893944}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32168 < 32590; dropping {'train/loss': 0.002428666455671191}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32169 < 32590; dropping {'train/loss': 0.12706395983695984}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32170 < 32590; dropping {'train/loss': 0.07898518443107605}.\n",
      "Epoch 321 | Batch 70/100 | Loss 0.041928\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32171 < 32590; dropping {'train/loss': 0.0032432652078568935}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32172 < 32590; dropping {'train/loss': 0.06675920635461807}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32173 < 32590; dropping {'train/loss': 0.054917801171541214}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32174 < 32590; dropping {'train/loss': 0.05535395070910454}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32175 < 32590; dropping {'train/loss': 0.005930088460445404}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32176 < 32590; dropping {'train/loss': 0.07349789887666702}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32177 < 32590; dropping {'train/loss': 0.03186547011137009}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32178 < 32590; dropping {'train/loss': 0.03989137336611748}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32179 < 32590; dropping {'train/loss': 0.027281612157821655}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32180 < 32590; dropping {'train/loss': 0.030623773112893105}.\n",
      "Epoch 321 | Batch 80/100 | Loss 0.041554\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32181 < 32590; dropping {'train/loss': 0.05603446438908577}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32182 < 32590; dropping {'train/loss': 0.00461245235055685}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32183 < 32590; dropping {'train/loss': 0.014741254970431328}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32184 < 32590; dropping {'train/loss': 0.16115343570709229}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32185 < 32590; dropping {'train/loss': 0.0909976065158844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32186 < 32590; dropping {'train/loss': 0.08230587840080261}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32187 < 32590; dropping {'train/loss': 0.020460516214370728}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32188 < 32590; dropping {'train/loss': 0.1295962631702423}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32189 < 32590; dropping {'train/loss': 0.10204780101776123}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32190 < 32590; dropping {'train/loss': 0.031279001384973526}.\n",
      "Epoch 321 | Batch 90/100 | Loss 0.044639\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32191 < 32590; dropping {'train/loss': 0.01717536710202694}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32192 < 32590; dropping {'train/loss': 0.014944685623049736}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32193 < 32590; dropping {'train/loss': 0.023582035675644875}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32194 < 32590; dropping {'train/loss': 0.003053009742870927}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32195 < 32590; dropping {'train/loss': 0.005032017827033997}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32196 < 32590; dropping {'train/loss': 0.034137286245822906}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32197 < 32590; dropping {'train/loss': 0.017544616013765335}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32198 < 32590; dropping {'train/loss': 0.003927594982087612}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32199 < 32590; dropping {'train/loss': 0.007084133569151163}.\n",
      "Epoch 321 | Batch 100/100 | Loss 0.042023wandb: WARNING Step must only increase in log calls.  Step 32200 < 32590; dropping {'train/loss': 0.05824105814099312}.\n",
      "\n",
      "100 Test Protonet Acc = 92.12% +- 0.89%\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32200 < 32590; dropping {'val/acc': 92.125}.\n",
      "best model! save...\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32201 < 32590; dropping {'train/loss': 0.004478255286812782}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32202 < 32590; dropping {'train/loss': 0.020548028871417046}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32203 < 32590; dropping {'train/loss': 0.057525746524333954}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32204 < 32590; dropping {'train/loss': 0.08734868466854095}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32205 < 32590; dropping {'train/loss': 0.00791255570948124}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32206 < 32590; dropping {'train/loss': 0.015180237591266632}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32207 < 32590; dropping {'train/loss': 0.03222792595624924}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32208 < 32590; dropping {'train/loss': 0.0690193623304367}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32209 < 32590; dropping {'train/loss': 0.009166425094008446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32210 < 32590; dropping {'train/loss': 0.012936428189277649}.\n",
      "Epoch 322 | Batch 10/100 | Loss 0.031634\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32211 < 32590; dropping {'train/loss': 0.03455289453268051}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32212 < 32590; dropping {'train/loss': 0.15872381627559662}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32213 < 32590; dropping {'train/loss': 0.09106466174125671}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32214 < 32590; dropping {'train/loss': 0.01775551773607731}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32215 < 32590; dropping {'train/loss': 0.09866494685411453}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32216 < 32590; dropping {'train/loss': 0.07967375218868256}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32217 < 32590; dropping {'train/loss': 0.1575780212879181}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32218 < 32590; dropping {'train/loss': 0.0965747982263565}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32219 < 32590; dropping {'train/loss': 0.002252191537991166}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32220 < 32590; dropping {'train/loss': 0.014547658152878284}.\n",
      "Epoch 322 | Batch 20/100 | Loss 0.053387\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32221 < 32590; dropping {'train/loss': 0.01041836105287075}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32222 < 32590; dropping {'train/loss': 0.03991911560297012}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32223 < 32590; dropping {'train/loss': 0.021604640409350395}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32224 < 32590; dropping {'train/loss': 0.0019624694250524044}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32225 < 32590; dropping {'train/loss': 0.019095629453659058}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32226 < 32590; dropping {'train/loss': 0.03308187425136566}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32227 < 32590; dropping {'train/loss': 0.06127799302339554}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32228 < 32590; dropping {'train/loss': 0.01105199009180069}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32229 < 32590; dropping {'train/loss': 0.008503136225044727}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32230 < 32590; dropping {'train/loss': 0.004229990765452385}.\n",
      "Epoch 322 | Batch 30/100 | Loss 0.042629\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32231 < 32590; dropping {'train/loss': 0.024182269349694252}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32232 < 32590; dropping {'train/loss': 0.027100950479507446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32233 < 32590; dropping {'train/loss': 0.01687709614634514}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32234 < 32590; dropping {'train/loss': 0.06601765006780624}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32235 < 32590; dropping {'train/loss': 0.024755822494626045}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32236 < 32590; dropping {'train/loss': 0.012734740972518921}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32237 < 32590; dropping {'train/loss': 0.05010553449392319}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32238 < 32590; dropping {'train/loss': 0.05801435187458992}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32239 < 32590; dropping {'train/loss': 0.0301168505102396}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32240 < 32590; dropping {'train/loss': 0.03759663179516792}.\n",
      "Epoch 322 | Batch 40/100 | Loss 0.040659\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32241 < 32590; dropping {'train/loss': 0.043795712292194366}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32242 < 32590; dropping {'train/loss': 0.009418226778507233}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32243 < 32590; dropping {'train/loss': 0.06945250928401947}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32244 < 32590; dropping {'train/loss': 0.054831426590681076}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32245 < 32590; dropping {'train/loss': 0.025411400943994522}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32246 < 32590; dropping {'train/loss': 0.1263515055179596}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32247 < 32590; dropping {'train/loss': 0.1486493945121765}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32248 < 32590; dropping {'train/loss': 0.038526106625795364}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32249 < 32590; dropping {'train/loss': 0.0025768238119781017}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32250 < 32590; dropping {'train/loss': 0.11811445653438568}.\n",
      "Epoch 322 | Batch 50/100 | Loss 0.045270\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32251 < 32590; dropping {'train/loss': 0.0034907087683677673}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32252 < 32590; dropping {'train/loss': 0.0499054379761219}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32253 < 32590; dropping {'train/loss': 0.000968687585555017}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32254 < 32590; dropping {'train/loss': 0.02960490621626377}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32255 < 32590; dropping {'train/loss': 0.09032116085290909}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32256 < 32590; dropping {'train/loss': 0.004745570011436939}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32257 < 32590; dropping {'train/loss': 0.08026385307312012}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32258 < 32590; dropping {'train/loss': 0.04761333018541336}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32259 < 32590; dropping {'train/loss': 0.02043035253882408}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32260 < 32590; dropping {'train/loss': 0.02907506190240383}.\n",
      "Epoch 322 | Batch 60/100 | Loss 0.043665\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32261 < 32590; dropping {'train/loss': 0.009649484418332577}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32262 < 32590; dropping {'train/loss': 0.03661610558629036}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32263 < 32590; dropping {'train/loss': 0.013307338580489159}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32264 < 32590; dropping {'train/loss': 0.11807825416326523}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32265 < 32590; dropping {'train/loss': 0.20523300766944885}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32266 < 32590; dropping {'train/loss': 0.1550740897655487}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32267 < 32590; dropping {'train/loss': 0.04628170281648636}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32268 < 32590; dropping {'train/loss': 0.0634942352771759}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32269 < 32590; dropping {'train/loss': 0.07354385405778885}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32270 < 32590; dropping {'train/loss': 0.053155720233917236}.\n",
      "Epoch 322 | Batch 70/100 | Loss 0.048491\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32271 < 32590; dropping {'train/loss': 0.026812711730599403}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32272 < 32590; dropping {'train/loss': 0.06798109412193298}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32273 < 32590; dropping {'train/loss': 0.017363494262099266}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32274 < 32590; dropping {'train/loss': 0.014318753965198994}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32275 < 32590; dropping {'train/loss': 0.051229678094387054}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32276 < 32590; dropping {'train/loss': 0.01922013983130455}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32277 < 32590; dropping {'train/loss': 0.003858085023239255}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32278 < 32590; dropping {'train/loss': 0.023245450109243393}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32279 < 32590; dropping {'train/loss': 0.0404479093849659}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32280 < 32590; dropping {'train/loss': 0.0017701813485473394}.\n",
      "Epoch 322 | Batch 80/100 | Loss 0.045758\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32281 < 32590; dropping {'train/loss': 0.05300932005047798}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32282 < 32590; dropping {'train/loss': 0.1753779947757721}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32283 < 32590; dropping {'train/loss': 0.03328258916735649}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32284 < 32590; dropping {'train/loss': 0.09120715409517288}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32285 < 32590; dropping {'train/loss': 0.03721953183412552}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32286 < 32590; dropping {'train/loss': 0.03500256687402725}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32287 < 32590; dropping {'train/loss': 0.047132719308137894}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32288 < 32590; dropping {'train/loss': 0.054048508405685425}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32289 < 32590; dropping {'train/loss': 0.028754625469446182}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32290 < 32590; dropping {'train/loss': 0.1679973304271698}.\n",
      "Epoch 322 | Batch 90/100 | Loss 0.048707\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32291 < 32590; dropping {'train/loss': 0.011925996281206608}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32292 < 32590; dropping {'train/loss': 0.009913796558976173}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32293 < 32590; dropping {'train/loss': 0.0040114992298185825}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32294 < 32590; dropping {'train/loss': 0.1543392837047577}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32295 < 32590; dropping {'train/loss': 0.10202975571155548}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32296 < 32590; dropping {'train/loss': 0.02489340677857399}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32297 < 32590; dropping {'train/loss': 0.004302698653191328}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32298 < 32590; dropping {'train/loss': 0.11753346025943756}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32299 < 32590; dropping {'train/loss': 0.07721371948719025}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32300 < 32590; dropping {'train/loss': 0.054558150470256805}.\n",
      "Epoch 322 | Batch 100/100 | Loss 0.049444\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32301 < 32590; dropping {'train/loss': 0.001602842821739614}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32302 < 32590; dropping {'train/loss': 0.055552057921886444}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32303 < 32590; dropping {'train/loss': 0.053453605622053146}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32304 < 32590; dropping {'train/loss': 0.01387869007885456}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32305 < 32590; dropping {'train/loss': 0.05986518785357475}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32306 < 32590; dropping {'train/loss': 0.0133737251162529}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32307 < 32590; dropping {'train/loss': 0.06978309899568558}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32308 < 32590; dropping {'train/loss': 0.026361802592873573}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32309 < 32590; dropping {'train/loss': 0.07974448800086975}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32310 < 32590; dropping {'train/loss': 0.07624251395463943}.\n",
      "Epoch 323 | Batch 10/100 | Loss 0.044986\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32311 < 32590; dropping {'train/loss': 0.04403308033943176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32312 < 32590; dropping {'train/loss': 0.0033447877503931522}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32313 < 32590; dropping {'train/loss': 0.07093542814254761}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32314 < 32590; dropping {'train/loss': 0.010745318606495857}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32315 < 32590; dropping {'train/loss': 0.02426917478442192}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32316 < 32590; dropping {'train/loss': 0.04214777052402496}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32317 < 32590; dropping {'train/loss': 0.06185869127511978}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32318 < 32590; dropping {'train/loss': 0.11404309421777725}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32319 < 32590; dropping {'train/loss': 0.022333797067403793}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32320 < 32590; dropping {'train/loss': 0.11089233309030533}.\n",
      "Epoch 323 | Batch 20/100 | Loss 0.047723\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32321 < 32590; dropping {'train/loss': 0.05636955425143242}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32322 < 32590; dropping {'train/loss': 0.012647124007344246}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32323 < 32590; dropping {'train/loss': 0.0038121906109154224}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32324 < 32590; dropping {'train/loss': 0.058168668299913406}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32325 < 32590; dropping {'train/loss': 0.05890808254480362}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32326 < 32590; dropping {'train/loss': 0.08011164516210556}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32327 < 32590; dropping {'train/loss': 0.10951858758926392}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32328 < 32590; dropping {'train/loss': 0.00752553204074502}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32329 < 32590; dropping {'train/loss': 0.13437332212924957}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32330 < 32590; dropping {'train/loss': 0.04787830263376236}.\n",
      "Epoch 323 | Batch 30/100 | Loss 0.050792\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32331 < 32590; dropping {'train/loss': 0.001229000510647893}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32332 < 32590; dropping {'train/loss': 0.0625874325633049}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32333 < 32590; dropping {'train/loss': 0.011895056813955307}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32334 < 32590; dropping {'train/loss': 0.006380441598594189}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32335 < 32590; dropping {'train/loss': 0.007417445071041584}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32336 < 32590; dropping {'train/loss': 0.05196796730160713}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32337 < 32590; dropping {'train/loss': 0.03313926234841347}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32338 < 32590; dropping {'train/loss': 0.0026641637086868286}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32339 < 32590; dropping {'train/loss': 0.024189908057451248}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32340 < 32590; dropping {'train/loss': 0.008027755655348301}.\n",
      "Epoch 323 | Batch 40/100 | Loss 0.043332\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32341 < 32590; dropping {'train/loss': 0.10063102096319199}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32342 < 32590; dropping {'train/loss': 0.06503267586231232}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32343 < 32590; dropping {'train/loss': 0.07797293365001678}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32344 < 32590; dropping {'train/loss': 0.022657526656985283}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32345 < 32590; dropping {'train/loss': 0.0009578618919476867}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32346 < 32590; dropping {'train/loss': 0.006532950792461634}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32347 < 32590; dropping {'train/loss': 0.03135605901479721}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32348 < 32590; dropping {'train/loss': 0.011402120813727379}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32349 < 32590; dropping {'train/loss': 0.09771638363599777}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32350 < 32590; dropping {'train/loss': 0.06899946182966232}.\n",
      "Epoch 323 | Batch 50/100 | Loss 0.044331\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32351 < 32590; dropping {'train/loss': 0.01674141176044941}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32352 < 32590; dropping {'train/loss': 0.017627088353037834}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32353 < 32590; dropping {'train/loss': 0.02716277539730072}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32354 < 32590; dropping {'train/loss': 0.04652922600507736}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32355 < 32590; dropping {'train/loss': 0.021175958216190338}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32356 < 32590; dropping {'train/loss': 0.03830333799123764}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32357 < 32590; dropping {'train/loss': 0.0577896311879158}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32358 < 32590; dropping {'train/loss': 0.0450860820710659}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32359 < 32590; dropping {'train/loss': 0.008563988842070103}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32360 < 32590; dropping {'train/loss': 0.022669825702905655}.\n",
      "Epoch 323 | Batch 60/100 | Loss 0.041970\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32361 < 32590; dropping {'train/loss': 0.01484803669154644}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32362 < 32590; dropping {'train/loss': 0.11260034888982773}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32363 < 32590; dropping {'train/loss': 0.042769331485033035}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32364 < 32590; dropping {'train/loss': 0.002221070695668459}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32365 < 32590; dropping {'train/loss': 0.0489373654127121}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32366 < 32590; dropping {'train/loss': 0.017970416694879532}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32367 < 32590; dropping {'train/loss': 0.023557404056191444}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32368 < 32590; dropping {'train/loss': 0.015472491271793842}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32369 < 32590; dropping {'train/loss': 0.06558852642774582}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32370 < 32590; dropping {'train/loss': 0.04935207962989807}.\n",
      "Epoch 323 | Batch 70/100 | Loss 0.041593\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32371 < 32590; dropping {'train/loss': 0.12405476719141006}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32372 < 32590; dropping {'train/loss': 0.013925219886004925}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32373 < 32590; dropping {'train/loss': 0.028871813789010048}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32374 < 32590; dropping {'train/loss': 0.2530129849910736}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32375 < 32590; dropping {'train/loss': 0.004556622356176376}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32376 < 32590; dropping {'train/loss': 0.07594393193721771}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32377 < 32590; dropping {'train/loss': 0.1268649846315384}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32378 < 32590; dropping {'train/loss': 0.08856740593910217}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32379 < 32590; dropping {'train/loss': 0.016942497342824936}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32380 < 32590; dropping {'train/loss': 0.1251799762248993}.\n",
      "Epoch 323 | Batch 80/100 | Loss 0.047118\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32381 < 32590; dropping {'train/loss': 0.03041568025946617}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32382 < 32590; dropping {'train/loss': 0.008308617398142815}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32383 < 32590; dropping {'train/loss': 0.08451759070158005}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32384 < 32590; dropping {'train/loss': 0.11055324226617813}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32385 < 32590; dropping {'train/loss': 0.013676455244421959}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32386 < 32590; dropping {'train/loss': 0.1257740557193756}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32387 < 32590; dropping {'train/loss': 0.053507499396800995}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32388 < 32590; dropping {'train/loss': 0.006722405552864075}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32389 < 32590; dropping {'train/loss': 0.012410018593072891}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32390 < 32590; dropping {'train/loss': 0.06169657036662102}.\n",
      "Epoch 323 | Batch 90/100 | Loss 0.047522\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32391 < 32590; dropping {'train/loss': 0.013967198319733143}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32392 < 32590; dropping {'train/loss': 0.02238612063229084}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32393 < 32590; dropping {'train/loss': 0.00048388485447503626}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32394 < 32590; dropping {'train/loss': 0.0005746794631704688}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32395 < 32590; dropping {'train/loss': 0.04134126752614975}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32396 < 32590; dropping {'train/loss': 0.030851874500513077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32397 < 32590; dropping {'train/loss': 0.004802093841135502}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32398 < 32590; dropping {'train/loss': 0.02001722902059555}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32399 < 32590; dropping {'train/loss': 0.09495867043733597}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32400 < 32590; dropping {'train/loss': 0.006902025081217289}.\n",
      "Epoch 323 | Batch 100/100 | Loss 0.045133\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32401 < 32590; dropping {'train/loss': 0.07174675166606903}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32402 < 32590; dropping {'train/loss': 0.017459945753216743}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32403 < 32590; dropping {'train/loss': 0.04216456785798073}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32404 < 32590; dropping {'train/loss': 0.07405395060777664}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32405 < 32590; dropping {'train/loss': 0.02760513685643673}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32406 < 32590; dropping {'train/loss': 0.01861032284796238}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32407 < 32590; dropping {'train/loss': 0.017038412392139435}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32408 < 32590; dropping {'train/loss': 0.05259726196527481}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32409 < 32590; dropping {'train/loss': 0.0028648737352341413}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32410 < 32590; dropping {'train/loss': 0.03765954449772835}.\n",
      "Epoch 324 | Batch 10/100 | Loss 0.036180\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32411 < 32590; dropping {'train/loss': 0.04969586431980133}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32412 < 32590; dropping {'train/loss': 0.024854522198438644}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32413 < 32590; dropping {'train/loss': 0.022342516109347343}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32414 < 32590; dropping {'train/loss': 0.02417668327689171}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32415 < 32590; dropping {'train/loss': 0.014817267656326294}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32416 < 32590; dropping {'train/loss': 0.052896589040756226}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32417 < 32590; dropping {'train/loss': 0.018037674948573112}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32418 < 32590; dropping {'train/loss': 0.04067673534154892}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32419 < 32590; dropping {'train/loss': 0.015617932192981243}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32420 < 32590; dropping {'train/loss': 0.005853611510246992}.\n",
      "Epoch 324 | Batch 20/100 | Loss 0.031539\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32421 < 32590; dropping {'train/loss': 0.12038320302963257}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32422 < 32590; dropping {'train/loss': 0.1038367748260498}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32423 < 32590; dropping {'train/loss': 0.09996971487998962}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32424 < 32590; dropping {'train/loss': 0.012441964820027351}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32425 < 32590; dropping {'train/loss': 0.015061224810779095}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32426 < 32590; dropping {'train/loss': 0.009231914766132832}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32427 < 32590; dropping {'train/loss': 0.07263831794261932}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32428 < 32590; dropping {'train/loss': 0.00048637730651535094}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32429 < 32590; dropping {'train/loss': 0.040716685354709625}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32430 < 32590; dropping {'train/loss': 0.042394936084747314}.\n",
      "Epoch 324 | Batch 30/100 | Loss 0.038264\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32431 < 32590; dropping {'train/loss': 0.02429879643023014}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32432 < 32590; dropping {'train/loss': 0.007710298988968134}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32433 < 32590; dropping {'train/loss': 0.04241221025586128}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32434 < 32590; dropping {'train/loss': 0.014402173459529877}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32435 < 32590; dropping {'train/loss': 0.012408634647727013}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32436 < 32590; dropping {'train/loss': 0.0030499862041324377}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32437 < 32590; dropping {'train/loss': 0.07182495296001434}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32438 < 32590; dropping {'train/loss': 0.07211880385875702}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32439 < 32590; dropping {'train/loss': 0.02026420459151268}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32440 < 32590; dropping {'train/loss': 0.178852841258049}.\n",
      "Epoch 324 | Batch 40/100 | Loss 0.039882\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32441 < 32590; dropping {'train/loss': 0.023786494508385658}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32442 < 32590; dropping {'train/loss': 0.06742927432060242}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32443 < 32590; dropping {'train/loss': 0.03670799359679222}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32444 < 32590; dropping {'train/loss': 0.008017024025321007}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32445 < 32590; dropping {'train/loss': 0.0385257788002491}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32446 < 32590; dropping {'train/loss': 0.014230924658477306}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32447 < 32590; dropping {'train/loss': 0.15884721279144287}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32448 < 32590; dropping {'train/loss': 0.015547377057373524}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32449 < 32590; dropping {'train/loss': 0.052186816930770874}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32450 < 32590; dropping {'train/loss': 0.019150253385305405}.\n",
      "Epoch 324 | Batch 50/100 | Loss 0.040594\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32451 < 32590; dropping {'train/loss': 0.005725744180381298}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32452 < 32590; dropping {'train/loss': 0.04534227028489113}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32453 < 32590; dropping {'train/loss': 0.1582016944885254}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32454 < 32590; dropping {'train/loss': 0.001844399026595056}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32455 < 32590; dropping {'train/loss': 0.011999696493148804}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32456 < 32590; dropping {'train/loss': 0.031402673572301865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32457 < 32590; dropping {'train/loss': 0.006847350858151913}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32458 < 32590; dropping {'train/loss': 0.03035815991461277}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32459 < 32590; dropping {'train/loss': 0.10733417421579361}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32460 < 32590; dropping {'train/loss': 0.00020430018776096404}.\n",
      "Epoch 324 | Batch 60/100 | Loss 0.040483\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32461 < 32590; dropping {'train/loss': 0.03839016705751419}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32462 < 32590; dropping {'train/loss': 0.2091563194990158}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32463 < 32590; dropping {'train/loss': 0.06250624358654022}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32464 < 32590; dropping {'train/loss': 0.019942624494433403}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32465 < 32590; dropping {'train/loss': 0.0023262924514710903}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32466 < 32590; dropping {'train/loss': 0.0028027542866766453}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32467 < 32590; dropping {'train/loss': 0.1702464371919632}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32468 < 32590; dropping {'train/loss': 0.08145193755626678}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32469 < 32590; dropping {'train/loss': 0.10843044519424438}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32470 < 32590; dropping {'train/loss': 0.005368595942854881}.\n",
      "Epoch 324 | Batch 70/100 | Loss 0.044708\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32471 < 32590; dropping {'train/loss': 0.010288859717547894}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32472 < 32590; dropping {'train/loss': 0.004644664470106363}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32473 < 32590; dropping {'train/loss': 0.0017925091087818146}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32474 < 32590; dropping {'train/loss': 0.0307577196508646}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32475 < 32590; dropping {'train/loss': 0.0715523511171341}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32476 < 32590; dropping {'train/loss': 0.051473330706357956}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32477 < 32590; dropping {'train/loss': 0.02738313004374504}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32478 < 32590; dropping {'train/loss': 0.08509129285812378}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32479 < 32590; dropping {'train/loss': 0.12424417585134506}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32480 < 32590; dropping {'train/loss': 0.011944320984184742}.\n",
      "Epoch 324 | Batch 80/100 | Loss 0.044359\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32481 < 32590; dropping {'train/loss': 0.003888113424181938}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32482 < 32590; dropping {'train/loss': 0.018096499145030975}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32483 < 32590; dropping {'train/loss': 0.04608101025223732}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32484 < 32590; dropping {'train/loss': 0.11768706887960434}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32485 < 32590; dropping {'train/loss': 0.03687356039881706}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32486 < 32590; dropping {'train/loss': 0.034365035593509674}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32487 < 32590; dropping {'train/loss': 0.184617280960083}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32488 < 32590; dropping {'train/loss': 0.058740872889757156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32489 < 32590; dropping {'train/loss': 0.004242171999067068}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32490 < 32590; dropping {'train/loss': 0.021409805864095688}.\n",
      "Epoch 324 | Batch 90/100 | Loss 0.045275\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32491 < 32590; dropping {'train/loss': 0.002287736628204584}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32492 < 32590; dropping {'train/loss': 0.023821765556931496}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32493 < 32590; dropping {'train/loss': 0.012224257923662663}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32494 < 32590; dropping {'train/loss': 0.00901944749057293}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32495 < 32590; dropping {'train/loss': 0.09218386560678482}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32496 < 32590; dropping {'train/loss': 0.06395658105611801}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32497 < 32590; dropping {'train/loss': 0.10986380279064178}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32498 < 32590; dropping {'train/loss': 0.01078745722770691}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32499 < 32590; dropping {'train/loss': 0.016394736245274544}.\n",
      "Epoch 324 | Batch 100/100 | Loss 0.044973wandb: WARNING Step must only increase in log calls.  Step 32500 < 32590; dropping {'train/loss': 0.08201076835393906}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32501 < 32590; dropping {'train/loss': 0.011414160020649433}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32502 < 32590; dropping {'train/loss': 0.04503314942121506}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32503 < 32590; dropping {'train/loss': 0.009657777845859528}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32504 < 32590; dropping {'train/loss': 0.033582642674446106}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32505 < 32590; dropping {'train/loss': 0.01882149651646614}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32506 < 32590; dropping {'train/loss': 0.01146830152720213}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32507 < 32590; dropping {'train/loss': 0.015351912006735802}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32508 < 32590; dropping {'train/loss': 0.0043522147461771965}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32509 < 32590; dropping {'train/loss': 0.02140221744775772}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32510 < 32590; dropping {'train/loss': 0.013887534849345684}.\n",
      "Epoch 325 | Batch 10/100 | Loss 0.018497\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32511 < 32590; dropping {'train/loss': 0.005742915440350771}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32512 < 32590; dropping {'train/loss': 0.010257027111947536}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32513 < 32590; dropping {'train/loss': 0.10668939352035522}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32514 < 32590; dropping {'train/loss': 0.0017540876287966967}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32515 < 32590; dropping {'train/loss': 0.02234106883406639}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32516 < 32590; dropping {'train/loss': 0.10967537015676498}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32517 < 32590; dropping {'train/loss': 0.028255645185709}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32518 < 32590; dropping {'train/loss': 0.07578431069850922}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32519 < 32590; dropping {'train/loss': 0.06908190250396729}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32520 < 32590; dropping {'train/loss': 0.11676032841205597}.\n",
      "Epoch 325 | Batch 20/100 | Loss 0.036566\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32521 < 32590; dropping {'train/loss': 0.011384830810129642}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32522 < 32590; dropping {'train/loss': 0.06715039163827896}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32523 < 32590; dropping {'train/loss': 0.18062467873096466}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32524 < 32590; dropping {'train/loss': 0.07781989872455597}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32525 < 32590; dropping {'train/loss': 0.0200108103454113}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32526 < 32590; dropping {'train/loss': 0.04808558151125908}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32527 < 32590; dropping {'train/loss': 0.010777151212096214}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32528 < 32590; dropping {'train/loss': 0.030611108988523483}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32529 < 32590; dropping {'train/loss': 0.012162601575255394}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32530 < 32590; dropping {'train/loss': 0.0855763852596283}.\n",
      "Epoch 325 | Batch 30/100 | Loss 0.042517\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32531 < 32590; dropping {'train/loss': 0.08058364689350128}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32532 < 32590; dropping {'train/loss': 0.010996276512742043}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32533 < 32590; dropping {'train/loss': 0.009015223011374474}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32534 < 32590; dropping {'train/loss': 0.018491122871637344}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32535 < 32590; dropping {'train/loss': 0.006188713014125824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32536 < 32590; dropping {'train/loss': 0.02380676008760929}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32537 < 32590; dropping {'train/loss': 0.10030826181173325}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32538 < 32590; dropping {'train/loss': 0.0008003214607015252}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32539 < 32590; dropping {'train/loss': 0.05086285620927811}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32540 < 32590; dropping {'train/loss': 0.015094895847141743}.\n",
      "Epoch 325 | Batch 40/100 | Loss 0.039792\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32541 < 32590; dropping {'train/loss': 0.003581828670576215}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32542 < 32590; dropping {'train/loss': 0.07795871794223785}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32543 < 32590; dropping {'train/loss': 0.05240681767463684}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32544 < 32590; dropping {'train/loss': 0.0171811543405056}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32545 < 32590; dropping {'train/loss': 0.008933409117162228}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32546 < 32590; dropping {'train/loss': 0.0867050364613533}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32547 < 32590; dropping {'train/loss': 0.04867612570524216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32548 < 32590; dropping {'train/loss': 0.1416727602481842}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32549 < 32590; dropping {'train/loss': 0.018665198236703873}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32550 < 32590; dropping {'train/loss': 0.000867571507114917}.\n",
      "Epoch 325 | Batch 50/100 | Loss 0.040966\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32551 < 32590; dropping {'train/loss': 0.042255304753780365}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32552 < 32590; dropping {'train/loss': 0.020613037049770355}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32553 < 32590; dropping {'train/loss': 0.018034478649497032}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32554 < 32590; dropping {'train/loss': 0.006392826791852713}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32555 < 32590; dropping {'train/loss': 0.0851181149482727}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32556 < 32590; dropping {'train/loss': 0.014043666422367096}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32557 < 32590; dropping {'train/loss': 0.0011383420787751675}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32558 < 32590; dropping {'train/loss': 0.05877833813428879}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32559 < 32590; dropping {'train/loss': 0.011385091580450535}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32560 < 32590; dropping {'train/loss': 0.0020273353438824415}.\n",
      "Epoch 325 | Batch 60/100 | Loss 0.038468\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32561 < 32590; dropping {'train/loss': 0.045324526727199554}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32562 < 32590; dropping {'train/loss': 0.0899253785610199}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32563 < 32590; dropping {'train/loss': 0.042161885648965836}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32564 < 32590; dropping {'train/loss': 0.030219126492738724}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32565 < 32590; dropping {'train/loss': 0.020250145345926285}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32566 < 32590; dropping {'train/loss': 0.013881281018257141}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32567 < 32590; dropping {'train/loss': 0.0003307361912447959}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32568 < 32590; dropping {'train/loss': 0.007695590145885944}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32569 < 32590; dropping {'train/loss': 0.027115920558571815}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32570 < 32590; dropping {'train/loss': 0.08116970956325531}.\n",
      "Epoch 325 | Batch 70/100 | Loss 0.038088\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32571 < 32590; dropping {'train/loss': 0.0067428601905703545}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32572 < 32590; dropping {'train/loss': 0.024542933329939842}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32573 < 32590; dropping {'train/loss': 0.038342781364917755}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32574 < 32590; dropping {'train/loss': 0.01211371086537838}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32575 < 32590; dropping {'train/loss': 0.0035800407640635967}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32576 < 32590; dropping {'train/loss': 0.0892384722828865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32577 < 32590; dropping {'train/loss': 0.037727758288383484}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32578 < 32590; dropping {'train/loss': 0.0011074134381487966}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32579 < 32590; dropping {'train/loss': 0.13598167896270752}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32580 < 32590; dropping {'train/loss': 0.06335673481225967}.\n",
      "Epoch 325 | Batch 80/100 | Loss 0.038486\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32581 < 32590; dropping {'train/loss': 0.013403798453509808}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32582 < 32590; dropping {'train/loss': 0.03699790686368942}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32583 < 32590; dropping {'train/loss': 0.09061305224895477}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32584 < 32590; dropping {'train/loss': 0.027248088270425797}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32585 < 32590; dropping {'train/loss': 0.004419391974806786}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32586 < 32590; dropping {'train/loss': 0.06691627204418182}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32587 < 32590; dropping {'train/loss': 0.027408087626099586}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32588 < 32590; dropping {'train/loss': 0.018700357526540756}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 32589 < 32590; dropping {'train/loss': 0.011440024711191654}.\n",
      "Epoch 325 | Batch 90/100 | Loss 0.038114\n",
      "Epoch 325 | Batch 100/100 | Loss 0.038509\n",
      "Epoch 326 | Batch 10/100 | Loss 0.026887\n",
      "Epoch 326 | Batch 20/100 | Loss 0.030277\n",
      "Epoch 326 | Batch 30/100 | Loss 0.042066\n",
      "Epoch 326 | Batch 40/100 | Loss 0.040780\n",
      "Epoch 326 | Batch 50/100 | Loss 0.044073\n",
      "Epoch 326 | Batch 60/100 | Loss 0.045743\n",
      "Epoch 326 | Batch 70/100 | Loss 0.055349\n",
      "Epoch 326 | Batch 80/100 | Loss 0.056795\n",
      "Epoch 326 | Batch 90/100 | Loss 0.055836\n",
      "Epoch 326 | Batch 100/100 | Loss 0.061369\n",
      "Epoch 327 | Batch 10/100 | Loss 0.025949\n",
      "Epoch 327 | Batch 20/100 | Loss 0.036001\n",
      "Epoch 327 | Batch 30/100 | Loss 0.041627\n",
      "Epoch 327 | Batch 40/100 | Loss 0.043792\n",
      "Epoch 327 | Batch 50/100 | Loss 0.047193\n",
      "Epoch 327 | Batch 60/100 | Loss 0.045452\n",
      "Epoch 327 | Batch 70/100 | Loss 0.043479\n",
      "Epoch 327 | Batch 80/100 | Loss 0.042258\n",
      "Epoch 327 | Batch 90/100 | Loss 0.043386\n",
      "Epoch 327 | Batch 100/100 | Loss 0.040827\n",
      "Epoch 328 | Batch 10/100 | Loss 0.054612\n",
      "Epoch 328 | Batch 20/100 | Loss 0.054488\n",
      "Epoch 328 | Batch 30/100 | Loss 0.047337\n",
      "Epoch 328 | Batch 40/100 | Loss 0.060948\n",
      "Epoch 328 | Batch 50/100 | Loss 0.061996\n",
      "Epoch 328 | Batch 60/100 | Loss 0.061066\n",
      "Epoch 328 | Batch 70/100 | Loss 0.058759\n",
      "Epoch 328 | Batch 80/100 | Loss 0.056616\n",
      "Epoch 328 | Batch 90/100 | Loss 0.059648\n",
      "Epoch 328 | Batch 100/100 | Loss 0.057938\n",
      "Epoch 329 | Batch 10/100 | Loss 0.028957\n",
      "Epoch 329 | Batch 20/100 | Loss 0.035306\n",
      "Epoch 329 | Batch 30/100 | Loss 0.037733\n",
      "Epoch 329 | Batch 40/100 | Loss 0.039558\n",
      "Epoch 329 | Batch 50/100 | Loss 0.039542\n",
      "Epoch 329 | Batch 60/100 | Loss 0.041762\n",
      "Epoch 329 | Batch 70/100 | Loss 0.041000\n",
      "Epoch 329 | Batch 80/100 | Loss 0.041227\n",
      "Epoch 329 | Batch 90/100 | Loss 0.040898\n",
      "Epoch 329 | Batch 100/100 | Loss 0.039630\n",
      "Epoch 330 | Batch 10/100 | Loss 0.062544\n",
      "Epoch 330 | Batch 20/100 | Loss 0.050737\n",
      "Epoch 330 | Batch 30/100 | Loss 0.047016\n",
      "Epoch 330 | Batch 40/100 | Loss 0.046352\n",
      "Epoch 330 | Batch 50/100 | Loss 0.045018\n",
      "Epoch 330 | Batch 60/100 | Loss 0.043495\n",
      "Epoch 330 | Batch 70/100 | Loss 0.040978\n",
      "Epoch 330 | Batch 80/100 | Loss 0.042150\n",
      "Epoch 330 | Batch 90/100 | Loss 0.042344\n",
      "Epoch 330 | Batch 100/100 | Loss 0.041423\n",
      "Epoch 331 | Batch 10/100 | Loss 0.044684\n",
      "Epoch 331 | Batch 20/100 | Loss 0.053430\n",
      "Epoch 331 | Batch 30/100 | Loss 0.052985\n",
      "Epoch 331 | Batch 40/100 | Loss 0.050550\n",
      "Epoch 331 | Batch 50/100 | Loss 0.046563\n",
      "Epoch 331 | Batch 60/100 | Loss 0.044409\n",
      "Epoch 331 | Batch 70/100 | Loss 0.043480\n",
      "Epoch 331 | Batch 80/100 | Loss 0.044529\n",
      "Epoch 331 | Batch 90/100 | Loss 0.047028\n",
      "Epoch 331 | Batch 100/100 | Loss 0.048215\n",
      "Epoch 332 | Batch 10/100 | Loss 0.043793\n",
      "Epoch 332 | Batch 20/100 | Loss 0.037547\n",
      "Epoch 332 | Batch 30/100 | Loss 0.037284\n",
      "Epoch 332 | Batch 40/100 | Loss 0.035338\n",
      "Epoch 332 | Batch 50/100 | Loss 0.038386\n",
      "Epoch 332 | Batch 60/100 | Loss 0.039859\n",
      "Epoch 332 | Batch 70/100 | Loss 0.040610\n",
      "Epoch 332 | Batch 80/100 | Loss 0.042157\n",
      "Epoch 332 | Batch 90/100 | Loss 0.041529\n",
      "Epoch 332 | Batch 100/100 | Loss 0.042191\n",
      "Epoch 333 | Batch 10/100 | Loss 0.045540\n",
      "Epoch 333 | Batch 20/100 | Loss 0.035972\n",
      "Epoch 333 | Batch 30/100 | Loss 0.036956\n",
      "Epoch 333 | Batch 40/100 | Loss 0.037229\n",
      "Epoch 333 | Batch 50/100 | Loss 0.036850\n",
      "Epoch 333 | Batch 60/100 | Loss 0.037010\n",
      "Epoch 333 | Batch 70/100 | Loss 0.041178\n",
      "Epoch 333 | Batch 80/100 | Loss 0.042337\n",
      "Epoch 333 | Batch 90/100 | Loss 0.043256\n",
      "Epoch 333 | Batch 100/100 | Loss 0.042269\n",
      "Epoch 334 | Batch 10/100 | Loss 0.035812\n",
      "Epoch 334 | Batch 20/100 | Loss 0.042756\n",
      "Epoch 334 | Batch 30/100 | Loss 0.044620\n",
      "Epoch 334 | Batch 40/100 | Loss 0.043954\n",
      "Epoch 334 | Batch 50/100 | Loss 0.044262\n",
      "Epoch 334 | Batch 60/100 | Loss 0.040104\n",
      "Epoch 334 | Batch 70/100 | Loss 0.041538\n",
      "Epoch 334 | Batch 80/100 | Loss 0.042079\n",
      "Epoch 334 | Batch 90/100 | Loss 0.042655\n",
      "Epoch 334 | Batch 100/100 | Loss 0.043400\n",
      "Epoch 335 | Batch 10/100 | Loss 0.070015\n",
      "Epoch 335 | Batch 20/100 | Loss 0.058239\n",
      "Epoch 335 | Batch 30/100 | Loss 0.050654\n",
      "Epoch 335 | Batch 40/100 | Loss 0.047806\n",
      "Epoch 335 | Batch 50/100 | Loss 0.050448\n",
      "Epoch 335 | Batch 60/100 | Loss 0.048620\n",
      "Epoch 335 | Batch 70/100 | Loss 0.047465\n",
      "Epoch 335 | Batch 80/100 | Loss 0.047101\n",
      "Epoch 335 | Batch 90/100 | Loss 0.045903\n",
      "Epoch 335 | Batch 100/100 | Loss 0.049829\n",
      "Epoch 336 | Batch 10/100 | Loss 0.042543\n",
      "Epoch 336 | Batch 20/100 | Loss 0.032291\n",
      "Epoch 336 | Batch 30/100 | Loss 0.032815\n",
      "Epoch 336 | Batch 40/100 | Loss 0.030763\n",
      "Epoch 336 | Batch 50/100 | Loss 0.035803\n",
      "Epoch 336 | Batch 60/100 | Loss 0.037307\n",
      "Epoch 336 | Batch 70/100 | Loss 0.036903\n",
      "Epoch 336 | Batch 80/100 | Loss 0.037291\n",
      "Epoch 336 | Batch 90/100 | Loss 0.036808\n",
      "Epoch 336 | Batch 100/100 | Loss 0.035851\n",
      "Epoch 337 | Batch 10/100 | Loss 0.054239\n",
      "Epoch 337 | Batch 20/100 | Loss 0.047974\n",
      "Epoch 337 | Batch 30/100 | Loss 0.048878\n",
      "Epoch 337 | Batch 40/100 | Loss 0.044946\n",
      "Epoch 337 | Batch 50/100 | Loss 0.046045\n",
      "Epoch 337 | Batch 60/100 | Loss 0.045001\n",
      "Epoch 337 | Batch 70/100 | Loss 0.044507\n",
      "Epoch 337 | Batch 80/100 | Loss 0.047065\n",
      "Epoch 337 | Batch 90/100 | Loss 0.046013\n",
      "Epoch 337 | Batch 100/100 | Loss 0.046619\n",
      "Epoch 338 | Batch 10/100 | Loss 0.025621\n",
      "Epoch 338 | Batch 20/100 | Loss 0.036212\n",
      "Epoch 338 | Batch 30/100 | Loss 0.046487\n",
      "Epoch 338 | Batch 40/100 | Loss 0.062909\n",
      "Epoch 338 | Batch 50/100 | Loss 0.056707\n",
      "Epoch 338 | Batch 60/100 | Loss 0.057434\n",
      "Epoch 338 | Batch 70/100 | Loss 0.056147\n",
      "Epoch 338 | Batch 80/100 | Loss 0.055678\n",
      "Epoch 338 | Batch 90/100 | Loss 0.059856\n",
      "Epoch 338 | Batch 100/100 | Loss 0.059361\n",
      "Epoch 339 | Batch 10/100 | Loss 0.057525\n",
      "Epoch 339 | Batch 20/100 | Loss 0.046027\n",
      "Epoch 339 | Batch 30/100 | Loss 0.046306\n",
      "Epoch 339 | Batch 40/100 | Loss 0.043196\n",
      "Epoch 339 | Batch 50/100 | Loss 0.038511\n",
      "Epoch 339 | Batch 60/100 | Loss 0.040476\n",
      "Epoch 339 | Batch 70/100 | Loss 0.038533\n",
      "Epoch 339 | Batch 80/100 | Loss 0.039280\n",
      "Epoch 339 | Batch 90/100 | Loss 0.040753\n",
      "Epoch 339 | Batch 100/100 | Loss 0.042266\n",
      "Epoch 340 | Batch 10/100 | Loss 0.031175\n",
      "Epoch 340 | Batch 20/100 | Loss 0.029819\n",
      "Epoch 340 | Batch 30/100 | Loss 0.032740\n",
      "Epoch 340 | Batch 40/100 | Loss 0.035790\n",
      "Epoch 340 | Batch 50/100 | Loss 0.036563\n",
      "Epoch 340 | Batch 60/100 | Loss 0.039358\n",
      "Epoch 340 | Batch 70/100 | Loss 0.039241\n",
      "Epoch 340 | Batch 80/100 | Loss 0.037798\n",
      "Epoch 340 | Batch 90/100 | Loss 0.037372\n",
      "Epoch 340 | Batch 100/100 | Loss 0.036856\n",
      "Epoch 341 | Batch 10/100 | Loss 0.051142\n",
      "Epoch 341 | Batch 20/100 | Loss 0.034169\n",
      "Epoch 341 | Batch 30/100 | Loss 0.040351\n",
      "Epoch 341 | Batch 40/100 | Loss 0.044418\n",
      "Epoch 341 | Batch 50/100 | Loss 0.044217\n",
      "Epoch 341 | Batch 60/100 | Loss 0.044685\n",
      "Epoch 341 | Batch 70/100 | Loss 0.044990\n",
      "Epoch 341 | Batch 80/100 | Loss 0.046089\n",
      "Epoch 341 | Batch 90/100 | Loss 0.044843\n",
      "Epoch 341 | Batch 100/100 | Loss 0.044781\n",
      "100 Test Protonet Acc = 91.89% +- 0.96%\n",
      "Epoch 342 | Batch 10/100 | Loss 0.031342\n",
      "Epoch 342 | Batch 20/100 | Loss 0.049741\n",
      "Epoch 342 | Batch 30/100 | Loss 0.048845\n",
      "Epoch 342 | Batch 40/100 | Loss 0.044798\n",
      "Epoch 342 | Batch 50/100 | Loss 0.043198\n",
      "Epoch 342 | Batch 60/100 | Loss 0.045145\n",
      "Epoch 342 | Batch 70/100 | Loss 0.047552\n",
      "Epoch 342 | Batch 80/100 | Loss 0.048712\n",
      "Epoch 342 | Batch 90/100 | Loss 0.048626\n",
      "Epoch 342 | Batch 100/100 | Loss 0.052048\n",
      "Epoch 343 | Batch 10/100 | Loss 0.044698\n",
      "Epoch 343 | Batch 20/100 | Loss 0.039643\n",
      "Epoch 343 | Batch 30/100 | Loss 0.040072\n",
      "Epoch 343 | Batch 40/100 | Loss 0.045371\n",
      "Epoch 343 | Batch 50/100 | Loss 0.040705\n",
      "Epoch 343 | Batch 60/100 | Loss 0.042091\n",
      "Epoch 343 | Batch 70/100 | Loss 0.048019\n",
      "Epoch 343 | Batch 80/100 | Loss 0.048326\n",
      "Epoch 343 | Batch 90/100 | Loss 0.048080\n",
      "Epoch 343 | Batch 100/100 | Loss 0.047458\n",
      "Epoch 344 | Batch 10/100 | Loss 0.054227\n",
      "Epoch 344 | Batch 20/100 | Loss 0.048456\n",
      "Epoch 344 | Batch 30/100 | Loss 0.048126\n",
      "Epoch 344 | Batch 40/100 | Loss 0.045358\n",
      "Epoch 344 | Batch 50/100 | Loss 0.043884\n",
      "Epoch 344 | Batch 60/100 | Loss 0.043833\n",
      "Epoch 344 | Batch 70/100 | Loss 0.044406\n",
      "Epoch 344 | Batch 80/100 | Loss 0.044574\n",
      "Epoch 344 | Batch 90/100 | Loss 0.046779\n",
      "Epoch 344 | Batch 100/100 | Loss 0.048723\n",
      "Epoch 345 | Batch 10/100 | Loss 0.047414\n",
      "Epoch 345 | Batch 20/100 | Loss 0.054055\n",
      "Epoch 345 | Batch 30/100 | Loss 0.048359\n",
      "Epoch 345 | Batch 40/100 | Loss 0.048889\n",
      "Epoch 345 | Batch 50/100 | Loss 0.049557\n",
      "Epoch 345 | Batch 60/100 | Loss 0.055609\n",
      "Epoch 345 | Batch 70/100 | Loss 0.054780\n",
      "Epoch 345 | Batch 80/100 | Loss 0.051228\n",
      "Epoch 345 | Batch 90/100 | Loss 0.048765\n",
      "Epoch 345 | Batch 100/100 | Loss 0.048867\n",
      "Epoch 346 | Batch 10/100 | Loss 0.027325\n",
      "Epoch 346 | Batch 20/100 | Loss 0.034098\n",
      "Epoch 346 | Batch 30/100 | Loss 0.039181\n",
      "Epoch 346 | Batch 40/100 | Loss 0.037261\n",
      "Epoch 346 | Batch 50/100 | Loss 0.040775\n",
      "Epoch 346 | Batch 60/100 | Loss 0.037384\n",
      "Epoch 346 | Batch 70/100 | Loss 0.042239\n",
      "Epoch 346 | Batch 80/100 | Loss 0.040979\n",
      "Epoch 346 | Batch 90/100 | Loss 0.041118\n",
      "Epoch 346 | Batch 100/100 | Loss 0.041417\n",
      "Epoch 347 | Batch 10/100 | Loss 0.037636\n",
      "Epoch 347 | Batch 20/100 | Loss 0.036284\n",
      "Epoch 347 | Batch 30/100 | Loss 0.038497\n",
      "Epoch 347 | Batch 40/100 | Loss 0.041243\n",
      "Epoch 347 | Batch 50/100 | Loss 0.040845\n",
      "Epoch 347 | Batch 60/100 | Loss 0.043931\n",
      "Epoch 347 | Batch 70/100 | Loss 0.048656\n",
      "Epoch 347 | Batch 80/100 | Loss 0.050077\n",
      "Epoch 347 | Batch 90/100 | Loss 0.051595\n",
      "Epoch 347 | Batch 100/100 | Loss 0.053468\n",
      "Epoch 348 | Batch 10/100 | Loss 0.029967\n",
      "Epoch 348 | Batch 20/100 | Loss 0.031748\n",
      "Epoch 348 | Batch 30/100 | Loss 0.035115\n",
      "Epoch 348 | Batch 40/100 | Loss 0.034897\n",
      "Epoch 348 | Batch 50/100 | Loss 0.035109\n",
      "Epoch 348 | Batch 60/100 | Loss 0.034046\n",
      "Epoch 348 | Batch 70/100 | Loss 0.034994\n",
      "Epoch 348 | Batch 80/100 | Loss 0.033409\n",
      "Epoch 348 | Batch 90/100 | Loss 0.033204\n",
      "Epoch 348 | Batch 100/100 | Loss 0.033167\n",
      "Epoch 349 | Batch 10/100 | Loss 0.032714\n",
      "Epoch 349 | Batch 20/100 | Loss 0.033346\n",
      "Epoch 349 | Batch 30/100 | Loss 0.034658\n",
      "Epoch 349 | Batch 40/100 | Loss 0.042789\n",
      "Epoch 349 | Batch 50/100 | Loss 0.040864\n",
      "Epoch 349 | Batch 60/100 | Loss 0.043360\n",
      "Epoch 349 | Batch 70/100 | Loss 0.044859\n",
      "Epoch 349 | Batch 80/100 | Loss 0.044879\n",
      "Epoch 349 | Batch 90/100 | Loss 0.044987\n",
      "Epoch 349 | Batch 100/100 | Loss 0.043932\n",
      "Epoch 350 | Batch 10/100 | Loss 0.061012\n",
      "Epoch 350 | Batch 20/100 | Loss 0.058849\n",
      "Epoch 350 | Batch 30/100 | Loss 0.046678\n",
      "Epoch 350 | Batch 40/100 | Loss 0.050637\n",
      "Epoch 350 | Batch 50/100 | Loss 0.048060\n",
      "Epoch 350 | Batch 60/100 | Loss 0.047275\n",
      "Epoch 350 | Batch 70/100 | Loss 0.047616\n",
      "Epoch 350 | Batch 80/100 | Loss 0.046763\n",
      "Epoch 350 | Batch 90/100 | Loss 0.046674\n",
      "Epoch 350 | Batch 100/100 | Loss 0.046419\n",
      "Epoch 351 | Batch 10/100 | Loss 0.048861\n",
      "Epoch 351 | Batch 20/100 | Loss 0.040173\n",
      "Epoch 351 | Batch 30/100 | Loss 0.038852\n",
      "Epoch 351 | Batch 40/100 | Loss 0.040123\n",
      "Epoch 351 | Batch 50/100 | Loss 0.039160\n",
      "Epoch 351 | Batch 60/100 | Loss 0.041548\n",
      "Epoch 351 | Batch 70/100 | Loss 0.038690\n",
      "Epoch 351 | Batch 80/100 | Loss 0.035638\n",
      "Epoch 351 | Batch 90/100 | Loss 0.034694\n",
      "Epoch 351 | Batch 100/100 | Loss 0.034421\n",
      "Epoch 352 | Batch 10/100 | Loss 0.047034\n",
      "Epoch 352 | Batch 20/100 | Loss 0.050324\n",
      "Epoch 352 | Batch 30/100 | Loss 0.050742\n",
      "Epoch 352 | Batch 40/100 | Loss 0.047685\n",
      "Epoch 352 | Batch 50/100 | Loss 0.045594\n",
      "Epoch 352 | Batch 60/100 | Loss 0.045305\n",
      "Epoch 352 | Batch 70/100 | Loss 0.043011\n",
      "Epoch 352 | Batch 80/100 | Loss 0.042854\n",
      "Epoch 352 | Batch 90/100 | Loss 0.043591\n",
      "Epoch 352 | Batch 100/100 | Loss 0.044776\n",
      "Epoch 353 | Batch 10/100 | Loss 0.029570\n",
      "Epoch 353 | Batch 20/100 | Loss 0.027384\n",
      "Epoch 353 | Batch 30/100 | Loss 0.032620\n",
      "Epoch 353 | Batch 40/100 | Loss 0.039527\n",
      "Epoch 353 | Batch 50/100 | Loss 0.038237\n",
      "Epoch 353 | Batch 60/100 | Loss 0.038289\n",
      "Epoch 353 | Batch 70/100 | Loss 0.038318\n",
      "Epoch 353 | Batch 80/100 | Loss 0.040842\n",
      "Epoch 353 | Batch 90/100 | Loss 0.039880\n",
      "Epoch 353 | Batch 100/100 | Loss 0.039882\n",
      "Epoch 354 | Batch 10/100 | Loss 0.029557\n",
      "Epoch 354 | Batch 20/100 | Loss 0.026633\n",
      "Epoch 354 | Batch 30/100 | Loss 0.039386\n",
      "Epoch 354 | Batch 40/100 | Loss 0.041058\n",
      "Epoch 354 | Batch 50/100 | Loss 0.038828\n",
      "Epoch 354 | Batch 60/100 | Loss 0.040280\n",
      "Epoch 354 | Batch 70/100 | Loss 0.039894\n",
      "Epoch 354 | Batch 80/100 | Loss 0.038818\n",
      "Epoch 354 | Batch 90/100 | Loss 0.040036\n",
      "Epoch 354 | Batch 100/100 | Loss 0.041182\n",
      "Epoch 355 | Batch 10/100 | Loss 0.038078\n",
      "Epoch 355 | Batch 20/100 | Loss 0.045357\n",
      "Epoch 355 | Batch 30/100 | Loss 0.045902\n",
      "Epoch 355 | Batch 40/100 | Loss 0.043979\n",
      "Epoch 355 | Batch 50/100 | Loss 0.041379\n",
      "Epoch 355 | Batch 60/100 | Loss 0.040374\n",
      "Epoch 355 | Batch 70/100 | Loss 0.037931\n",
      "Epoch 355 | Batch 80/100 | Loss 0.039420\n",
      "Epoch 355 | Batch 90/100 | Loss 0.041256\n",
      "Epoch 355 | Batch 100/100 | Loss 0.041579\n",
      "Epoch 356 | Batch 10/100 | Loss 0.029124\n",
      "Epoch 356 | Batch 20/100 | Loss 0.029243\n",
      "Epoch 356 | Batch 30/100 | Loss 0.034355\n",
      "Epoch 356 | Batch 40/100 | Loss 0.032191\n",
      "Epoch 356 | Batch 50/100 | Loss 0.033681\n",
      "Epoch 356 | Batch 60/100 | Loss 0.035073\n",
      "Epoch 356 | Batch 70/100 | Loss 0.039391\n",
      "Epoch 356 | Batch 80/100 | Loss 0.041083\n",
      "Epoch 356 | Batch 90/100 | Loss 0.041737\n",
      "Epoch 356 | Batch 100/100 | Loss 0.042515\n",
      "Epoch 357 | Batch 10/100 | Loss 0.072215\n",
      "Epoch 357 | Batch 20/100 | Loss 0.072085\n",
      "Epoch 357 | Batch 30/100 | Loss 0.070801\n",
      "Epoch 357 | Batch 40/100 | Loss 0.059962\n",
      "Epoch 357 | Batch 50/100 | Loss 0.057521\n",
      "Epoch 357 | Batch 60/100 | Loss 0.059096\n",
      "Epoch 357 | Batch 70/100 | Loss 0.057645\n",
      "Epoch 357 | Batch 80/100 | Loss 0.054058\n",
      "Epoch 357 | Batch 90/100 | Loss 0.053104\n",
      "Epoch 357 | Batch 100/100 | Loss 0.050525\n",
      "Epoch 358 | Batch 10/100 | Loss 0.037286\n",
      "Epoch 358 | Batch 20/100 | Loss 0.039911\n",
      "Epoch 358 | Batch 30/100 | Loss 0.042212\n",
      "Epoch 358 | Batch 40/100 | Loss 0.038384\n",
      "Epoch 358 | Batch 50/100 | Loss 0.048549\n",
      "Epoch 358 | Batch 60/100 | Loss 0.044512\n",
      "Epoch 358 | Batch 70/100 | Loss 0.045220\n",
      "Epoch 358 | Batch 80/100 | Loss 0.044533\n",
      "Epoch 358 | Batch 90/100 | Loss 0.044590\n",
      "Epoch 358 | Batch 100/100 | Loss 0.044111\n",
      "Epoch 359 | Batch 10/100 | Loss 0.050927\n",
      "Epoch 359 | Batch 20/100 | Loss 0.039153\n",
      "Epoch 359 | Batch 30/100 | Loss 0.045158\n",
      "Epoch 359 | Batch 40/100 | Loss 0.044190\n",
      "Epoch 359 | Batch 50/100 | Loss 0.048317\n",
      "Epoch 359 | Batch 60/100 | Loss 0.050560\n",
      "Epoch 359 | Batch 70/100 | Loss 0.048789\n",
      "Epoch 359 | Batch 80/100 | Loss 0.050094\n",
      "Epoch 359 | Batch 90/100 | Loss 0.049066\n",
      "Epoch 359 | Batch 100/100 | Loss 0.049640\n",
      "Epoch 360 | Batch 10/100 | Loss 0.048308\n",
      "Epoch 360 | Batch 20/100 | Loss 0.041053\n",
      "Epoch 360 | Batch 30/100 | Loss 0.041460\n",
      "Epoch 360 | Batch 40/100 | Loss 0.038078\n",
      "Epoch 360 | Batch 50/100 | Loss 0.038486\n",
      "Epoch 360 | Batch 60/100 | Loss 0.039119\n",
      "Epoch 360 | Batch 70/100 | Loss 0.037030\n",
      "Epoch 360 | Batch 80/100 | Loss 0.035613\n",
      "Epoch 360 | Batch 90/100 | Loss 0.036079\n",
      "Epoch 360 | Batch 100/100 | Loss 0.034930\n",
      "Epoch 361 | Batch 10/100 | Loss 0.068303\n",
      "Epoch 361 | Batch 20/100 | Loss 0.045365\n",
      "Epoch 361 | Batch 30/100 | Loss 0.044328\n",
      "Epoch 361 | Batch 40/100 | Loss 0.041707\n",
      "Epoch 361 | Batch 50/100 | Loss 0.047363\n",
      "Epoch 361 | Batch 60/100 | Loss 0.042773\n",
      "Epoch 361 | Batch 70/100 | Loss 0.040433\n",
      "Epoch 361 | Batch 80/100 | Loss 0.038921\n",
      "Epoch 361 | Batch 90/100 | Loss 0.039052\n",
      "Epoch 361 | Batch 100/100 | Loss 0.041095\n",
      "100 Test Protonet Acc = 92.39% +- 0.91%\n",
      "best model! save...\n",
      "Epoch 362 | Batch 10/100 | Loss 0.054605\n",
      "Epoch 362 | Batch 20/100 | Loss 0.054408\n",
      "Epoch 362 | Batch 30/100 | Loss 0.054754\n",
      "Epoch 362 | Batch 40/100 | Loss 0.051301\n",
      "Epoch 362 | Batch 50/100 | Loss 0.048678\n",
      "Epoch 362 | Batch 60/100 | Loss 0.045655\n",
      "Epoch 362 | Batch 70/100 | Loss 0.045139\n",
      "Epoch 362 | Batch 80/100 | Loss 0.043292\n",
      "Epoch 362 | Batch 90/100 | Loss 0.042085\n",
      "Epoch 362 | Batch 100/100 | Loss 0.040478\n",
      "Epoch 363 | Batch 10/100 | Loss 0.028243\n",
      "Epoch 363 | Batch 20/100 | Loss 0.030927\n",
      "Epoch 363 | Batch 30/100 | Loss 0.035179\n",
      "Epoch 363 | Batch 40/100 | Loss 0.040808\n",
      "Epoch 363 | Batch 50/100 | Loss 0.041555\n",
      "Epoch 363 | Batch 60/100 | Loss 0.041647\n",
      "Epoch 363 | Batch 70/100 | Loss 0.039852\n",
      "Epoch 363 | Batch 80/100 | Loss 0.039777\n",
      "Epoch 363 | Batch 90/100 | Loss 0.040892\n",
      "Epoch 363 | Batch 100/100 | Loss 0.041201\n",
      "Epoch 364 | Batch 10/100 | Loss 0.032488\n",
      "Epoch 364 | Batch 20/100 | Loss 0.040743\n",
      "Epoch 364 | Batch 30/100 | Loss 0.042419\n",
      "Epoch 364 | Batch 40/100 | Loss 0.045632\n",
      "Epoch 364 | Batch 50/100 | Loss 0.043286\n",
      "Epoch 364 | Batch 60/100 | Loss 0.047139\n",
      "Epoch 364 | Batch 70/100 | Loss 0.045445\n",
      "Epoch 364 | Batch 80/100 | Loss 0.044878\n",
      "Epoch 364 | Batch 90/100 | Loss 0.041951\n",
      "Epoch 364 | Batch 100/100 | Loss 0.041983\n",
      "Epoch 365 | Batch 10/100 | Loss 0.036429\n",
      "Epoch 365 | Batch 20/100 | Loss 0.041376\n",
      "Epoch 365 | Batch 30/100 | Loss 0.042324\n",
      "Epoch 365 | Batch 40/100 | Loss 0.041341\n",
      "Epoch 365 | Batch 50/100 | Loss 0.038792\n",
      "Epoch 365 | Batch 60/100 | Loss 0.037225\n",
      "Epoch 365 | Batch 70/100 | Loss 0.038001\n",
      "Epoch 365 | Batch 80/100 | Loss 0.037154\n",
      "Epoch 365 | Batch 90/100 | Loss 0.036814\n",
      "Epoch 365 | Batch 100/100 | Loss 0.037538\n",
      "Epoch 366 | Batch 10/100 | Loss 0.025913\n",
      "Epoch 366 | Batch 20/100 | Loss 0.038548\n",
      "Epoch 366 | Batch 30/100 | Loss 0.035930\n",
      "Epoch 366 | Batch 40/100 | Loss 0.036921\n",
      "Epoch 366 | Batch 50/100 | Loss 0.032459\n",
      "Epoch 366 | Batch 60/100 | Loss 0.033048\n",
      "Epoch 366 | Batch 70/100 | Loss 0.033084\n",
      "Epoch 366 | Batch 80/100 | Loss 0.034274\n",
      "Epoch 366 | Batch 90/100 | Loss 0.034659\n",
      "Epoch 366 | Batch 100/100 | Loss 0.036873\n",
      "Epoch 367 | Batch 10/100 | Loss 0.039878\n",
      "Epoch 367 | Batch 20/100 | Loss 0.040138\n",
      "Epoch 367 | Batch 30/100 | Loss 0.046932\n",
      "Epoch 367 | Batch 40/100 | Loss 0.044062\n",
      "Epoch 367 | Batch 50/100 | Loss 0.041649\n",
      "Epoch 367 | Batch 60/100 | Loss 0.040942\n",
      "Epoch 367 | Batch 70/100 | Loss 0.039550\n",
      "Epoch 367 | Batch 80/100 | Loss 0.043104\n",
      "Epoch 367 | Batch 90/100 | Loss 0.042409\n",
      "Epoch 367 | Batch 100/100 | Loss 0.041894\n",
      "Epoch 368 | Batch 10/100 | Loss 0.042361\n",
      "Epoch 368 | Batch 20/100 | Loss 0.044319\n",
      "Epoch 368 | Batch 30/100 | Loss 0.040659\n",
      "Epoch 368 | Batch 40/100 | Loss 0.037211\n",
      "Epoch 368 | Batch 50/100 | Loss 0.037028\n",
      "Epoch 368 | Batch 60/100 | Loss 0.034382\n",
      "Epoch 368 | Batch 70/100 | Loss 0.037833\n",
      "Epoch 368 | Batch 80/100 | Loss 0.041097\n",
      "Epoch 368 | Batch 90/100 | Loss 0.039667\n",
      "Epoch 368 | Batch 100/100 | Loss 0.042002\n",
      "Epoch 369 | Batch 10/100 | Loss 0.032179\n",
      "Epoch 369 | Batch 20/100 | Loss 0.030414\n",
      "Epoch 369 | Batch 30/100 | Loss 0.037827\n",
      "Epoch 369 | Batch 40/100 | Loss 0.042644\n",
      "Epoch 369 | Batch 50/100 | Loss 0.042276\n",
      "Epoch 369 | Batch 60/100 | Loss 0.043480\n",
      "Epoch 369 | Batch 70/100 | Loss 0.041221\n",
      "Epoch 369 | Batch 80/100 | Loss 0.040861\n",
      "Epoch 369 | Batch 90/100 | Loss 0.040078\n",
      "Epoch 369 | Batch 100/100 | Loss 0.038576\n",
      "Epoch 370 | Batch 10/100 | Loss 0.028588\n",
      "Epoch 370 | Batch 20/100 | Loss 0.027129\n",
      "Epoch 370 | Batch 30/100 | Loss 0.024871\n",
      "Epoch 370 | Batch 40/100 | Loss 0.031777\n",
      "Epoch 370 | Batch 50/100 | Loss 0.036231\n",
      "Epoch 370 | Batch 60/100 | Loss 0.042803\n",
      "Epoch 370 | Batch 70/100 | Loss 0.042349\n",
      "Epoch 370 | Batch 80/100 | Loss 0.043893\n",
      "Epoch 370 | Batch 90/100 | Loss 0.044101\n",
      "Epoch 370 | Batch 100/100 | Loss 0.044948\n",
      "Epoch 371 | Batch 10/100 | Loss 0.043466\n",
      "Epoch 371 | Batch 20/100 | Loss 0.046312\n",
      "Epoch 371 | Batch 30/100 | Loss 0.042979\n",
      "Epoch 371 | Batch 40/100 | Loss 0.040778\n",
      "Epoch 371 | Batch 50/100 | Loss 0.041242\n",
      "Epoch 371 | Batch 60/100 | Loss 0.040621\n",
      "Epoch 371 | Batch 70/100 | Loss 0.039236\n",
      "Epoch 371 | Batch 80/100 | Loss 0.040425\n",
      "Epoch 371 | Batch 90/100 | Loss 0.041315\n",
      "Epoch 371 | Batch 100/100 | Loss 0.041451\n",
      "Epoch 372 | Batch 10/100 | Loss 0.019694\n",
      "Epoch 372 | Batch 20/100 | Loss 0.023925\n",
      "Epoch 372 | Batch 30/100 | Loss 0.028914\n",
      "Epoch 372 | Batch 40/100 | Loss 0.029014\n",
      "Epoch 372 | Batch 50/100 | Loss 0.032302\n",
      "Epoch 372 | Batch 60/100 | Loss 0.031915\n",
      "Epoch 372 | Batch 70/100 | Loss 0.032150\n",
      "Epoch 372 | Batch 80/100 | Loss 0.030479\n",
      "Epoch 372 | Batch 90/100 | Loss 0.031996\n",
      "Epoch 372 | Batch 100/100 | Loss 0.031165\n",
      "Epoch 373 | Batch 10/100 | Loss 0.031517\n",
      "Epoch 373 | Batch 20/100 | Loss 0.025616\n",
      "Epoch 373 | Batch 30/100 | Loss 0.028497\n",
      "Epoch 373 | Batch 40/100 | Loss 0.040497\n",
      "Epoch 373 | Batch 50/100 | Loss 0.045294\n",
      "Epoch 373 | Batch 60/100 | Loss 0.048049\n",
      "Epoch 373 | Batch 70/100 | Loss 0.049349\n",
      "Epoch 373 | Batch 80/100 | Loss 0.046226\n",
      "Epoch 373 | Batch 90/100 | Loss 0.047539\n",
      "Epoch 373 | Batch 100/100 | Loss 0.047352\n",
      "Epoch 374 | Batch 10/100 | Loss 0.044191\n",
      "Epoch 374 | Batch 20/100 | Loss 0.048344\n",
      "Epoch 374 | Batch 30/100 | Loss 0.047801\n",
      "Epoch 374 | Batch 40/100 | Loss 0.044197\n",
      "Epoch 374 | Batch 50/100 | Loss 0.044705\n",
      "Epoch 374 | Batch 60/100 | Loss 0.044961\n",
      "Epoch 374 | Batch 70/100 | Loss 0.042901\n",
      "Epoch 374 | Batch 80/100 | Loss 0.042532\n",
      "Epoch 374 | Batch 90/100 | Loss 0.044763\n",
      "Epoch 374 | Batch 100/100 | Loss 0.044122\n",
      "Epoch 375 | Batch 10/100 | Loss 0.014939\n",
      "Epoch 375 | Batch 20/100 | Loss 0.023742\n",
      "Epoch 375 | Batch 30/100 | Loss 0.028828\n",
      "Epoch 375 | Batch 40/100 | Loss 0.031502\n",
      "Epoch 375 | Batch 50/100 | Loss 0.031908\n",
      "Epoch 375 | Batch 60/100 | Loss 0.033411\n",
      "Epoch 375 | Batch 70/100 | Loss 0.032612\n",
      "Epoch 375 | Batch 80/100 | Loss 0.032834\n",
      "Epoch 375 | Batch 90/100 | Loss 0.035566\n",
      "Epoch 375 | Batch 100/100 | Loss 0.037321\n",
      "Epoch 376 | Batch 10/100 | Loss 0.050669\n",
      "Epoch 376 | Batch 20/100 | Loss 0.039804\n",
      "Epoch 376 | Batch 30/100 | Loss 0.042887\n",
      "Epoch 376 | Batch 40/100 | Loss 0.044024\n",
      "Epoch 376 | Batch 50/100 | Loss 0.043204\n",
      "Epoch 376 | Batch 60/100 | Loss 0.043794\n",
      "Epoch 376 | Batch 70/100 | Loss 0.041852\n",
      "Epoch 376 | Batch 80/100 | Loss 0.045409\n",
      "Epoch 376 | Batch 90/100 | Loss 0.045894\n",
      "Epoch 376 | Batch 100/100 | Loss 0.044264\n",
      "Epoch 377 | Batch 10/100 | Loss 0.024110\n",
      "Epoch 377 | Batch 20/100 | Loss 0.031806\n",
      "Epoch 377 | Batch 30/100 | Loss 0.026287\n",
      "Epoch 377 | Batch 40/100 | Loss 0.030137\n",
      "Epoch 377 | Batch 50/100 | Loss 0.032621\n",
      "Epoch 377 | Batch 60/100 | Loss 0.031392\n",
      "Epoch 377 | Batch 70/100 | Loss 0.033372\n",
      "Epoch 377 | Batch 80/100 | Loss 0.036514\n",
      "Epoch 377 | Batch 90/100 | Loss 0.040043\n",
      "Epoch 377 | Batch 100/100 | Loss 0.040606\n",
      "Epoch 378 | Batch 10/100 | Loss 0.040244\n",
      "Epoch 378 | Batch 20/100 | Loss 0.035651\n",
      "Epoch 378 | Batch 30/100 | Loss 0.040621\n",
      "Epoch 378 | Batch 40/100 | Loss 0.040202\n",
      "Epoch 378 | Batch 50/100 | Loss 0.038638\n",
      "Epoch 378 | Batch 60/100 | Loss 0.039599\n",
      "Epoch 378 | Batch 70/100 | Loss 0.040899\n",
      "Epoch 378 | Batch 80/100 | Loss 0.041484\n",
      "Epoch 378 | Batch 90/100 | Loss 0.041556\n",
      "Epoch 378 | Batch 100/100 | Loss 0.042982\n",
      "Epoch 379 | Batch 10/100 | Loss 0.044838\n",
      "Epoch 379 | Batch 20/100 | Loss 0.044308\n",
      "Epoch 379 | Batch 30/100 | Loss 0.047219\n",
      "Epoch 379 | Batch 40/100 | Loss 0.047533\n",
      "Epoch 379 | Batch 50/100 | Loss 0.043890\n",
      "Epoch 379 | Batch 60/100 | Loss 0.043216\n",
      "Epoch 379 | Batch 70/100 | Loss 0.044650\n",
      "Epoch 379 | Batch 80/100 | Loss 0.045377\n",
      "Epoch 379 | Batch 90/100 | Loss 0.045633\n",
      "Epoch 379 | Batch 100/100 | Loss 0.044635\n",
      "Epoch 380 | Batch 10/100 | Loss 0.037134\n",
      "Epoch 380 | Batch 20/100 | Loss 0.033304\n",
      "Epoch 380 | Batch 30/100 | Loss 0.038215\n",
      "Epoch 380 | Batch 40/100 | Loss 0.037152\n",
      "Epoch 380 | Batch 50/100 | Loss 0.039435\n",
      "Epoch 380 | Batch 60/100 | Loss 0.040263\n",
      "Epoch 380 | Batch 70/100 | Loss 0.039644\n",
      "Epoch 380 | Batch 80/100 | Loss 0.039656\n",
      "Epoch 380 | Batch 90/100 | Loss 0.039569\n",
      "Epoch 380 | Batch 100/100 | Loss 0.039620\n",
      "Epoch 381 | Batch 10/100 | Loss 0.031875\n",
      "Epoch 381 | Batch 20/100 | Loss 0.042391\n",
      "Epoch 381 | Batch 30/100 | Loss 0.037248\n",
      "Epoch 381 | Batch 40/100 | Loss 0.037973\n",
      "Epoch 381 | Batch 50/100 | Loss 0.039306\n",
      "Epoch 381 | Batch 60/100 | Loss 0.036585\n",
      "Epoch 381 | Batch 70/100 | Loss 0.037749\n",
      "Epoch 381 | Batch 80/100 | Loss 0.039435\n",
      "Epoch 381 | Batch 90/100 | Loss 0.039112\n",
      "Epoch 381 | Batch 100/100 | Loss 0.038725\n",
      "100 Test Protonet Acc = 92.19% +- 1.17%\n",
      "Epoch 382 | Batch 10/100 | Loss 0.030872\n",
      "Epoch 382 | Batch 20/100 | Loss 0.031986\n",
      "Epoch 382 | Batch 30/100 | Loss 0.030633\n",
      "Epoch 382 | Batch 40/100 | Loss 0.032538\n",
      "Epoch 382 | Batch 50/100 | Loss 0.031588\n",
      "Epoch 382 | Batch 60/100 | Loss 0.034300\n",
      "Epoch 382 | Batch 70/100 | Loss 0.035411\n",
      "Epoch 382 | Batch 80/100 | Loss 0.036839\n",
      "Epoch 382 | Batch 90/100 | Loss 0.035106\n",
      "Epoch 382 | Batch 100/100 | Loss 0.036298\n",
      "Epoch 383 | Batch 10/100 | Loss 0.053542\n",
      "Epoch 383 | Batch 20/100 | Loss 0.037742\n",
      "Epoch 383 | Batch 30/100 | Loss 0.035759\n",
      "Epoch 383 | Batch 40/100 | Loss 0.033215\n",
      "Epoch 383 | Batch 50/100 | Loss 0.039457\n",
      "Epoch 383 | Batch 60/100 | Loss 0.038334\n",
      "Epoch 383 | Batch 70/100 | Loss 0.036498\n",
      "Epoch 383 | Batch 80/100 | Loss 0.035933\n",
      "Epoch 383 | Batch 90/100 | Loss 0.036040\n",
      "Epoch 383 | Batch 100/100 | Loss 0.034891\n",
      "Epoch 384 | Batch 10/100 | Loss 0.038552\n",
      "Epoch 384 | Batch 20/100 | Loss 0.041294\n",
      "Epoch 384 | Batch 30/100 | Loss 0.040931\n",
      "Epoch 384 | Batch 40/100 | Loss 0.042774\n",
      "Epoch 384 | Batch 50/100 | Loss 0.040612\n",
      "Epoch 384 | Batch 60/100 | Loss 0.041174\n",
      "Epoch 384 | Batch 70/100 | Loss 0.041619\n",
      "Epoch 384 | Batch 80/100 | Loss 0.040833\n",
      "Epoch 384 | Batch 90/100 | Loss 0.040605\n",
      "Epoch 384 | Batch 100/100 | Loss 0.041932\n",
      "Epoch 385 | Batch 10/100 | Loss 0.036713\n",
      "Epoch 385 | Batch 20/100 | Loss 0.038748\n",
      "Epoch 385 | Batch 30/100 | Loss 0.045512\n",
      "Epoch 385 | Batch 40/100 | Loss 0.040643\n",
      "Epoch 385 | Batch 50/100 | Loss 0.037999\n",
      "Epoch 385 | Batch 60/100 | Loss 0.038429\n",
      "Epoch 385 | Batch 70/100 | Loss 0.038180\n",
      "Epoch 385 | Batch 80/100 | Loss 0.041415\n",
      "Epoch 385 | Batch 90/100 | Loss 0.040098\n",
      "Epoch 385 | Batch 100/100 | Loss 0.039557\n",
      "Epoch 386 | Batch 10/100 | Loss 0.034558\n",
      "Epoch 386 | Batch 20/100 | Loss 0.053677\n",
      "Epoch 386 | Batch 30/100 | Loss 0.054423\n",
      "Epoch 386 | Batch 40/100 | Loss 0.047949\n",
      "Epoch 386 | Batch 50/100 | Loss 0.042713\n",
      "Epoch 386 | Batch 60/100 | Loss 0.039865\n",
      "Epoch 386 | Batch 70/100 | Loss 0.039377\n",
      "Epoch 386 | Batch 80/100 | Loss 0.040494\n",
      "Epoch 386 | Batch 90/100 | Loss 0.044164\n",
      "Epoch 386 | Batch 100/100 | Loss 0.045030\n",
      "Epoch 387 | Batch 10/100 | Loss 0.029407\n",
      "Epoch 387 | Batch 20/100 | Loss 0.054857\n",
      "Epoch 387 | Batch 30/100 | Loss 0.052434\n",
      "Epoch 387 | Batch 40/100 | Loss 0.045346\n",
      "Epoch 387 | Batch 50/100 | Loss 0.043217\n",
      "Epoch 387 | Batch 60/100 | Loss 0.042662\n",
      "Epoch 387 | Batch 70/100 | Loss 0.046000\n",
      "Epoch 387 | Batch 80/100 | Loss 0.043658\n",
      "Epoch 387 | Batch 90/100 | Loss 0.043545\n",
      "Epoch 387 | Batch 100/100 | Loss 0.043348\n",
      "Epoch 388 | Batch 10/100 | Loss 0.038279\n",
      "Epoch 388 | Batch 20/100 | Loss 0.034239\n",
      "Epoch 388 | Batch 30/100 | Loss 0.036823\n",
      "Epoch 388 | Batch 40/100 | Loss 0.040097\n",
      "Epoch 388 | Batch 50/100 | Loss 0.042216\n",
      "Epoch 388 | Batch 60/100 | Loss 0.040443\n",
      "Epoch 388 | Batch 70/100 | Loss 0.039859\n",
      "Epoch 388 | Batch 80/100 | Loss 0.039935\n",
      "Epoch 388 | Batch 90/100 | Loss 0.038598\n",
      "Epoch 388 | Batch 100/100 | Loss 0.036573\n",
      "Epoch 389 | Batch 10/100 | Loss 0.034904\n",
      "Epoch 389 | Batch 20/100 | Loss 0.038333\n",
      "Epoch 389 | Batch 30/100 | Loss 0.036706\n",
      "Epoch 389 | Batch 40/100 | Loss 0.038746\n",
      "Epoch 389 | Batch 50/100 | Loss 0.041470\n",
      "Epoch 389 | Batch 60/100 | Loss 0.041934\n",
      "Epoch 389 | Batch 70/100 | Loss 0.042923\n",
      "Epoch 389 | Batch 80/100 | Loss 0.043966\n",
      "Epoch 389 | Batch 90/100 | Loss 0.044728\n",
      "Epoch 389 | Batch 100/100 | Loss 0.045480\n",
      "Epoch 390 | Batch 10/100 | Loss 0.035794\n",
      "Epoch 390 | Batch 20/100 | Loss 0.033014\n",
      "Epoch 390 | Batch 30/100 | Loss 0.036873\n",
      "Epoch 390 | Batch 40/100 | Loss 0.035523\n",
      "Epoch 390 | Batch 50/100 | Loss 0.036709\n",
      "Epoch 390 | Batch 60/100 | Loss 0.043952\n",
      "Epoch 390 | Batch 70/100 | Loss 0.040138\n",
      "Epoch 390 | Batch 80/100 | Loss 0.040165\n",
      "Epoch 390 | Batch 90/100 | Loss 0.037818\n",
      "Epoch 390 | Batch 100/100 | Loss 0.036817\n",
      "Epoch 391 | Batch 10/100 | Loss 0.014077\n",
      "Epoch 391 | Batch 20/100 | Loss 0.021806\n",
      "Epoch 391 | Batch 30/100 | Loss 0.021975\n",
      "Epoch 391 | Batch 40/100 | Loss 0.022299\n",
      "Epoch 391 | Batch 50/100 | Loss 0.024377\n",
      "Epoch 391 | Batch 60/100 | Loss 0.027488\n",
      "Epoch 391 | Batch 70/100 | Loss 0.028413\n",
      "Epoch 391 | Batch 80/100 | Loss 0.026429\n",
      "Epoch 391 | Batch 90/100 | Loss 0.027915\n",
      "Epoch 391 | Batch 100/100 | Loss 0.031139\n",
      "Epoch 392 | Batch 10/100 | Loss 0.053182\n",
      "Epoch 392 | Batch 20/100 | Loss 0.046478\n",
      "Epoch 392 | Batch 30/100 | Loss 0.039179\n",
      "Epoch 392 | Batch 40/100 | Loss 0.040200\n",
      "Epoch 392 | Batch 50/100 | Loss 0.039993\n",
      "Epoch 392 | Batch 60/100 | Loss 0.037569\n",
      "Epoch 392 | Batch 70/100 | Loss 0.038790\n",
      "Epoch 392 | Batch 80/100 | Loss 0.038078\n",
      "Epoch 392 | Batch 90/100 | Loss 0.037500\n",
      "Epoch 392 | Batch 100/100 | Loss 0.041187\n",
      "Epoch 393 | Batch 10/100 | Loss 0.044882\n",
      "Epoch 393 | Batch 20/100 | Loss 0.034147\n",
      "Epoch 393 | Batch 30/100 | Loss 0.041068\n",
      "Epoch 393 | Batch 40/100 | Loss 0.047763\n",
      "Epoch 393 | Batch 50/100 | Loss 0.045651\n",
      "Epoch 393 | Batch 60/100 | Loss 0.043613\n",
      "Epoch 393 | Batch 70/100 | Loss 0.047843\n",
      "Epoch 393 | Batch 80/100 | Loss 0.047524\n",
      "Epoch 393 | Batch 90/100 | Loss 0.048207\n",
      "Epoch 393 | Batch 100/100 | Loss 0.046784\n",
      "Epoch 394 | Batch 10/100 | Loss 0.045423\n",
      "Epoch 394 | Batch 20/100 | Loss 0.040801\n",
      "Epoch 394 | Batch 30/100 | Loss 0.046287\n",
      "Epoch 394 | Batch 40/100 | Loss 0.041234\n",
      "Epoch 394 | Batch 50/100 | Loss 0.041207\n",
      "Epoch 394 | Batch 60/100 | Loss 0.046689\n",
      "Epoch 394 | Batch 70/100 | Loss 0.044845\n",
      "Epoch 394 | Batch 80/100 | Loss 0.044535\n",
      "Epoch 394 | Batch 90/100 | Loss 0.044176\n",
      "Epoch 394 | Batch 100/100 | Loss 0.044808\n",
      "Epoch 395 | Batch 10/100 | Loss 0.026426\n",
      "Epoch 395 | Batch 20/100 | Loss 0.039983\n",
      "Epoch 395 | Batch 30/100 | Loss 0.038219\n",
      "Epoch 395 | Batch 40/100 | Loss 0.040413\n",
      "Epoch 395 | Batch 50/100 | Loss 0.037884\n",
      "Epoch 395 | Batch 60/100 | Loss 0.035477\n",
      "Epoch 395 | Batch 70/100 | Loss 0.034452\n",
      "Epoch 395 | Batch 80/100 | Loss 0.034154\n",
      "Epoch 395 | Batch 90/100 | Loss 0.034339\n",
      "Epoch 395 | Batch 100/100 | Loss 0.037034\n",
      "Epoch 396 | Batch 10/100 | Loss 0.034125\n",
      "Epoch 396 | Batch 20/100 | Loss 0.039570\n",
      "Epoch 396 | Batch 30/100 | Loss 0.035415\n",
      "Epoch 396 | Batch 40/100 | Loss 0.035338\n",
      "Epoch 396 | Batch 50/100 | Loss 0.034585\n",
      "Epoch 396 | Batch 60/100 | Loss 0.036478\n",
      "Epoch 396 | Batch 70/100 | Loss 0.037419\n",
      "Epoch 396 | Batch 80/100 | Loss 0.040872\n",
      "Epoch 396 | Batch 90/100 | Loss 0.038899\n",
      "Epoch 396 | Batch 100/100 | Loss 0.039902\n",
      "Epoch 397 | Batch 10/100 | Loss 0.030424\n",
      "Epoch 397 | Batch 20/100 | Loss 0.034765\n",
      "Epoch 397 | Batch 30/100 | Loss 0.041698\n",
      "Epoch 397 | Batch 40/100 | Loss 0.039121\n",
      "Epoch 397 | Batch 50/100 | Loss 0.038774\n",
      "Epoch 397 | Batch 60/100 | Loss 0.040934\n",
      "Epoch 397 | Batch 70/100 | Loss 0.041708\n",
      "Epoch 397 | Batch 80/100 | Loss 0.041802\n",
      "Epoch 397 | Batch 90/100 | Loss 0.042563\n",
      "Epoch 397 | Batch 100/100 | Loss 0.041474\n",
      "Epoch 398 | Batch 10/100 | Loss 0.038890\n",
      "Epoch 398 | Batch 20/100 | Loss 0.037273\n",
      "Epoch 398 | Batch 30/100 | Loss 0.030003\n",
      "Epoch 398 | Batch 40/100 | Loss 0.033700\n",
      "Epoch 398 | Batch 50/100 | Loss 0.035788\n",
      "Epoch 398 | Batch 60/100 | Loss 0.037197\n",
      "Epoch 398 | Batch 70/100 | Loss 0.034939\n",
      "Epoch 398 | Batch 80/100 | Loss 0.036427\n",
      "Epoch 398 | Batch 90/100 | Loss 0.035842\n",
      "Epoch 398 | Batch 100/100 | Loss 0.035900\n",
      "Epoch 399 | Batch 10/100 | Loss 0.028592\n",
      "Epoch 399 | Batch 20/100 | Loss 0.038239\n",
      "Epoch 399 | Batch 30/100 | Loss 0.034485\n",
      "Epoch 399 | Batch 40/100 | Loss 0.032069\n",
      "Epoch 399 | Batch 50/100 | Loss 0.034382\n",
      "Epoch 399 | Batch 60/100 | Loss 0.037276\n",
      "Epoch 399 | Batch 70/100 | Loss 0.035109\n",
      "Epoch 399 | Batch 80/100 | Loss 0.038300\n",
      "Epoch 399 | Batch 90/100 | Loss 0.040762\n",
      "Epoch 399 | Batch 100/100 | Loss 0.042174\n",
      "100 Test Protonet Acc = 92.61% +- 0.87%\n",
      "best model! save...\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20210608_051327-14zlsjib/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20210608_051327-14zlsjib/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _step 40000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val/acc 92.6125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime 37484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp 1623136720\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/loss 0.15576079487800598\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/loss ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val/acc ‚ñÅ‚ñÜ‚ñÑ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mflowers protonet\u001b[0m: \u001b[34mhttps://wandb.ai/multi-input/fsl_ssl/runs/14zlsjib\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset flowers --train_aug --method protonet --committed  --stop_epoch 400 --resume --resume_wandb_id 14zlsjib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20210608_051230-14zlsjib\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mflowers protonet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl/runs/14zlsjib\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Checkpoint restored at  ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/last_model.tar\n",
      "The model's epoch is  319\n",
      "Please rename it to continue training\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 837\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20210608_051230-14zlsjib/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20210608_051230-14zlsjib/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _step 32589\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val/acc 90.4125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime 29963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp 1623121580\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/loss 0.05676136165857315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mflowers protonet\u001b[0m: \u001b[34mhttps://wandb.ai/multi-input/fsl_ssl/runs/14zlsjib\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python wandb_restore.py --id 14zlsjib --path ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/last_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit after saving features and testing.\n",
      "checkpoint_dir: ckpts/dogs/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010\n",
      "USE BN: True\n",
      "outfile is features/dogs/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/novel.hdf5\n",
      "0/80\n",
      "10/80\n",
      "20/80\n",
      "30/80\n",
      "40/80\n",
      "50/80\n",
      "60/80\n",
      "70/80\n"
     ]
    }
   ],
   "source": [
    "# Do again with 200 dimensions\n",
    "# Do again with 399.tar\n",
    "\n",
    "!python save_features.py --dataset dogs --train_aug --method protonet --committed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing involves fine-tuning. Commit after testing.\n",
      "novel_file features/dogs/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/novel.hdf5\n",
      "600 Test Acc = 83.52% +- 0.54%\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataset dogs --train_aug --method protonet --committed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
