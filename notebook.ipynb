{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ./filelists/CUB/\n",
    "# !ln -s  ../../../CUB_200_2011/images ./images\n",
    "# %cd ../../\n",
    "\n",
    "!ln -s /kaggle/input/caltech-birds-2011-dataset/CUB_200_2011/images ./filelists/CUB/images\n",
    "\n",
    "# !ln -s /kaggle/input/miniimagenet/miniImageNet/images ./filelists/miniImagenet/images\n",
    "# !ln -s /kaggle/input/d/arjun2000ashok/tieredimagenet/tiered_imagenet/ ./filelists/tieredImagenet/images\n",
    "# !ln -s /kaggle/input/stanford-dogs-dataset/images/Images ./filelists/dogs/Images\n",
    "# !ln -s /kaggle/input/cars-dataset/images ./filelists/cars/images \n",
    "# !ln -s /kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images ./filelists/aircrafts/images\n",
    "# !ln -s /kaggle/input/vggflowers/images ./filelists/flowers/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1 windows (created Sun Jun 13 03:08:54 2021) [137x32] (attached)\n",
      "1: 1 windows (created Sun Jun 13 04:00:24 2021) [80x23]\n"
     ]
    }
   ],
   "source": [
    "!tmux ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: python: command not found\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset cars --train_aug --method protonet --committed --stop_epoch 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20210609_045359-vba5sgld\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33maircrafts protonet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl/runs/vba5sgld\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Checkpoint restored at  ckpts/aircrafts/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/best_model.tar\n",
      "The model's epoch is  399\n",
      "Please rename it to continue training\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 440\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20210609_045359-vba5sgld/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20210609_045359-vba5sgld/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _step 40000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val/acc 90.6625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val/loss_avg 0.2686292827129364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     val/loss_proto 0.2686292827129364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train/acc_proto 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/loss_proto 0.03371390700340271\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           _runtime 78198\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         _timestamp 1623189826\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/loss 0.03371390700340271\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33maircrafts protonet\u001b[0m: \u001b[34mhttps://wandb.ai/multi-input/fsl_ssl/runs/vba5sgld\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python wandb_restore.py --id vba5sgld --path ckpts/aircrafts/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/best_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit after saving features and testing.\n",
      "checkpoint_dir: ckpts/aircrafts/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010\n",
      "USE BN: True\n",
      "outfile is features/aircrafts/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/novel.hdf5\n",
      "0/40\n",
      "10/40\n",
      "20/40\n",
      "30/40\n"
     ]
    }
   ],
   "source": [
    "# Do again with 200 dimensions\n",
    "# Do again with 399.tar\n",
    "\n",
    "!python save_features.py --dataset aircrafts --train_aug --method protonet --committed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing involves fine-tuning. Commit after testing.\n",
      "novel_file features/aircrafts/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/novel.hdf5\n",
      "600 Test Acc = 91.90% +- 0.35%\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataset aircrafts --train_aug --method protonet --committed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking in baseline train: True\n",
      "USE pre-trained model: False\n",
      "Checkpoint path: ckpts/aircrafts/_resnet18_baseline_aug_tracking_jigsaw_lbda0.50Adam_lr0.0010\n",
      "Fresh wandb run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20210609_053152-2j3ciima\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhonest-plant-71\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl/runs/2j3ciima\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "About to start training. Last model and best model will be saved in wandb at every model save. \n",
      " Run save_features.py and test.py after the training completes, with the same arguments\n",
      "Epoch 1 | Batch 50/313 | Loss 3.908358 | Loss Cls 4.318188 | Loss Jigsaw 3.498529 | Acc Cls 0.017500 | Acc Jigsaw 0.095000\n",
      "Epoch 1 | Batch 100/313 | Loss 3.843621 | Loss Cls 4.165154 | Loss Jigsaw 3.522088 | Acc Cls 0.023750 | Acc Jigsaw 0.083125\n",
      "Epoch 1 | Batch 150/313 | Loss 3.822317 | Loss Cls 4.115179 | Loss Jigsaw 3.529454 | Acc Cls 0.022500 | Acc Jigsaw 0.074167\n",
      "Epoch 1 | Batch 200/313 | Loss 3.808955 | Loss Cls 4.082221 | Loss Jigsaw 3.535689 | Acc Cls 0.024375 | Acc Jigsaw 0.063750\n",
      "Epoch 1 | Batch 250/313 | Loss 3.798203 | Loss Cls 4.057579 | Loss Jigsaw 3.538826 | Acc Cls 0.023750 | Acc Jigsaw 0.054500\n",
      "Epoch 1 | Batch 300/313 | Loss 3.791967 | Loss Cls 4.036258 | Loss Jigsaw 3.547680 | Acc Cls 0.024583 | Acc Jigsaw 0.049375\n",
      "Epoch 2 | Batch 50/313 | Loss 3.731069 | Loss Cls 3.904525 | Loss Jigsaw 3.557613 | Acc Cls 0.032500 | Acc Jigsaw 0.056250\n",
      "Epoch 2 | Batch 100/313 | Loss 3.728743 | Loss Cls 3.902634 | Loss Jigsaw 3.554849 | Acc Cls 0.034375 | Acc Jigsaw 0.050000\n",
      "Epoch 2 | Batch 150/313 | Loss 3.726727 | Loss Cls 3.903623 | Loss Jigsaw 3.549831 | Acc Cls 0.037500 | Acc Jigsaw 0.047083\n",
      "Epoch 2 | Batch 200/313 | Loss 3.720500 | Loss Cls 3.910901 | Loss Jigsaw 3.530097 | Acc Cls 0.035625 | Acc Jigsaw 0.052812\n",
      "Epoch 2 | Batch 250/313 | Loss 3.722607 | Loss Cls 3.914757 | Loss Jigsaw 3.530457 | Acc Cls 0.035500 | Acc Jigsaw 0.049250\n",
      "Epoch 2 | Batch 300/313 | Loss 3.723102 | Loss Cls 3.912611 | Loss Jigsaw 3.533591 | Acc Cls 0.036250 | Acc Jigsaw 0.043958\n",
      "Epoch 3 | Batch 50/313 | Loss 3.682442 | Loss Cls 3.852431 | Loss Jigsaw 3.512453 | Acc Cls 0.042500 | Acc Jigsaw 0.068750\n",
      "Epoch 3 | Batch 100/313 | Loss 3.699393 | Loss Cls 3.866964 | Loss Jigsaw 3.531821 | Acc Cls 0.040000 | Acc Jigsaw 0.071250\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset aircrafts --train_aug --method baseline --jigsaw --lbda 0.5 --committed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking in baseline train: True\n",
      "USE pre-trained model: False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0cd09855391f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_unused_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, module, device_ids, output_device, dim, broadcast_buffers, process_group, bucket_cap_mb, find_unused_parameters, check_reduction)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocess_group\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m     \"\"\"\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         raise RuntimeError(\"Default process group has not been initialized, \"\n\u001b[0m\u001b[1;32m    286\u001b[0m                            \"please make sure to call init_process_group.\")\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "from methods.baselinetrain import BaselineTrain\n",
    "from io_utils import model_dict\n",
    "import torch.nn as nn\n",
    "\n",
    "gpu = 0\n",
    "\n",
    "model = BaselineTrain(model_dict[\"resnet18\"], 10)\n",
    "# if more than 1 gpu, syncbatchnorm\n",
    "\n",
    "model = model.cuda(gpu)\n",
    "\n",
    "model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu], find_unused_parameters=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking in baseline train: True\n",
      "USE pre-trained model: False\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'DistributedDataParallel' object has no attribute 'feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-327272ae641c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# mp.spawn(train, nprocs=1, args=())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-327272ae641c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(gpu)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#     print(dir(model))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_unused_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MASTER_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'127.0.0.1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 772\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'DistributedDataParallel' object has no attribute 'feature'"
     ]
    }
   ],
   "source": [
    "from methods.baselinetrain import BaselineTrain\n",
    "from io_utils import model_dict\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "rank = 0\n",
    "# dist.init_process_group(backend='nccl', init_method='env://', world_size=1, rank=rank)\n",
    "\n",
    "def train(gpu=0):\n",
    "    \n",
    "    \n",
    "    model = BaselineTrain(model_dict[\"resnet18\"], 10)\n",
    "    # if more than 1 gpu, syncbatchnorm here\n",
    "    model = model.cuda(gpu)\n",
    "    print(dir(model))\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu], find_unused_parameters=True) \n",
    "    print(model.feature())\n",
    "\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '29534'\n",
    "# world_size = 1 * 1\n",
    "# mp.spawn(train, nprocs=1, args=())\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "0. BaselineTrain - test with parallel and without parallel. Test with 2 GPUs also.\n",
    "1. Prototypical networks - check train.py and protonet.py, integrate DDP, integrate val_loss\n",
    "2. MAML - same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the distributed loader, model, logging etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params.parallel =  True\n",
      "params.gpus =  1\n",
      "params.devices =  0\n",
      "One process runs on gpu:  0\n",
      "Tracking in baseline train: True\n",
      "Use pre-trained model: False\n",
      "Checkpoint path: ckpts/CUB/_resnet18_baseline_aug_tracking_lr0.0010\n",
      "Fresh wandb run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2021-06-13 19:05:59.543682: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-13 19:05:59.546129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmorning-donkey-123\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl/runs/3qupe6wb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /kaggle/working/fsl_ssl/wandb/run-20210613_190557-3qupe6wb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Evaluation will be done every 2 epochs\n",
      "\n",
      "\n",
      "Created iterator!\n",
      "Epoch 0 | Batch 50/368 | Loss 5.218284 | Acc Cls 0.006250\n",
      "Epoch 0 | Batch 100/368 | Loss 4.996124 | Acc Cls 0.008750\n",
      "Epoch 0 | Batch 150/368 | Loss 4.903812 | Acc Cls 0.010000\n",
      "Epoch 0 | Batch 200/368 | Loss 4.842611 | Acc Cls 0.009687\n",
      "Epoch 0 | Batch 250/368 | Loss 4.794559 | Acc Cls 0.012500\n",
      "Epoch 0 | Batch 300/368 | Loss 4.756430 | Acc Cls 0.013542\n",
      "Epoch 0 | Batch 350/368 | Loss 4.727171 | Acc Cls 0.013929\n",
      "Epoch 0 complete; eta: 10:09:36\n",
      "Created iterator!\n",
      "Epoch 1 | Batch 50/368 | Loss 4.524387 | Acc Cls 0.021250\n",
      "Epoch 1 | Batch 100/368 | Loss 4.541503 | Acc Cls 0.015625\n",
      "Epoch 1 | Batch 150/368 | Loss 4.537857 | Acc Cls 0.017500\n",
      "Epoch 1 | Batch 200/368 | Loss 4.524635 | Acc Cls 0.017812\n",
      "Epoch 1 | Batch 250/368 | Loss 4.515265 | Acc Cls 0.019500\n",
      "Epoch 1 | Batch 300/368 | Loss 4.510932 | Acc Cls 0.020000\n",
      "Epoch 1 | Batch 350/368 | Loss 4.500734 | Acc Cls 0.019107\n",
      "Epoch 1 complete; eta: 10:06:40\n",
      "Evaluating...\n",
      "Created iterator!\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 316, in <module>\n",
      "    mp.spawn(main, nprocs=params.gpus, args=(params,))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 199, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 157, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 77, in join\n",
      "    timeout=timeout,\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset CUB --train_aug --method baseline --committed --stop_epoch 400 --eval_interval 2 --parallel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
