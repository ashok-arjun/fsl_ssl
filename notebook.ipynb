{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link './filelists/CUB/images/images': Read-only file system\n",
      "ln: failed to create symbolic link './filelists/miniImagenet/images/images': Read-only file system\n",
      "ln: failed to create symbolic link './filelists/tieredImagenet/images/tiered_imagenet': Read-only file system\n",
      "ln: failed to create symbolic link './filelists/dogs/images/Images': Read-only file system\n",
      "ln: failed to create symbolic link './filelists/aircrafts/images/images': Read-only file system\n"
     ]
    }
   ],
   "source": [
    "!ln -s /kaggle/input/caltech-birds-2011-dataset/CUB_200_2011/images ./filelists/CUB/images\n",
    "!ln -s /kaggle/input/miniimagenet/miniImageNet/images ./filelists/miniImagenet/images\n",
    "!ln -s /kaggle/input/d/arjun2000ashok/tieredimagenet/tiered_imagenet/ ./filelists/tieredImagenet/images\n",
    "!ln -s /kaggle/input/stanford-dogs-dataset/images/Images ./filelists/dogs/Images\n",
    "!ln -s /kaggle/input/stanford-cars-dataset/cars_train/cars_train ./filelists/cars/images \n",
    "!ln -s /kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images ./filelists/aircrafts/images\n",
    "!ln -s /kaggle/input/vggflowers/images ./filelists/flowers/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: ckpts/dogs/_resnet18_protonet_aug_5way_5shot_16query_lr0.0010\n",
      "Resuming from wandb ID:  11vqc3r9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 11vqc3r9 but id 11vqc3r9 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20210529_072837-11vqc3r9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mdry-frost-7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meta-learners/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meta-learners/fsl_ssl/runs/11vqc3r9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26001 < 29024; dropping {'train/loss': 0.5334404706954956}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26002 < 29024; dropping {'train/loss': 0.4051235616207123}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26003 < 29024; dropping {'train/loss': 0.417832612991333}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26004 < 29024; dropping {'train/loss': 0.37183427810668945}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26005 < 29024; dropping {'train/loss': 0.4160655438899994}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26006 < 29024; dropping {'train/loss': 0.2431834489107132}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26007 < 29024; dropping {'train/loss': 0.3473961651325226}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26008 < 29024; dropping {'train/loss': 0.23503167927265167}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26009 < 29024; dropping {'train/loss': 0.34083691239356995}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26010 < 29024; dropping {'train/loss': 0.5087172985076904}.\n",
      "Epoch 260 | Batch 10/100 | Loss 0.381946\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26011 < 29024; dropping {'train/loss': 0.6639684438705444}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26012 < 29024; dropping {'train/loss': 0.24973466992378235}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26013 < 29024; dropping {'train/loss': 0.37760886549949646}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26014 < 29024; dropping {'train/loss': 0.48374319076538086}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26015 < 29024; dropping {'train/loss': 0.4604216516017914}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26016 < 29024; dropping {'train/loss': 0.30958086252212524}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26017 < 29024; dropping {'train/loss': 0.10722191631793976}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26018 < 29024; dropping {'train/loss': 0.25328347086906433}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26019 < 29024; dropping {'train/loss': 0.5172199606895447}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26020 < 29024; dropping {'train/loss': 0.30106493830680847}.\n",
      "Epoch 260 | Batch 20/100 | Loss 0.377165\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26021 < 29024; dropping {'train/loss': 0.41683268547058105}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26022 < 29024; dropping {'train/loss': 0.6110031008720398}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26023 < 29024; dropping {'train/loss': 0.2833022475242615}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26024 < 29024; dropping {'train/loss': 0.24109454452991486}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26025 < 29024; dropping {'train/loss': 0.5692940354347229}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26026 < 29024; dropping {'train/loss': 0.5054501891136169}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26027 < 29024; dropping {'train/loss': 0.48388952016830444}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26028 < 29024; dropping {'train/loss': 0.1971723437309265}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26029 < 29024; dropping {'train/loss': 0.6273892521858215}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26030 < 29024; dropping {'train/loss': 0.3748062252998352}.\n",
      "Epoch 260 | Batch 30/100 | Loss 0.395118\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26031 < 29024; dropping {'train/loss': 0.7338029146194458}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26032 < 29024; dropping {'train/loss': 0.44871410727500916}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26033 < 29024; dropping {'train/loss': 0.42654967308044434}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26034 < 29024; dropping {'train/loss': 0.35770756006240845}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26035 < 29024; dropping {'train/loss': 0.8441664576530457}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26036 < 29024; dropping {'train/loss': 0.3099921643733978}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26037 < 29024; dropping {'train/loss': 0.32269102334976196}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26038 < 29024; dropping {'train/loss': 0.3503918945789337}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26039 < 29024; dropping {'train/loss': 0.5163642168045044}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26040 < 29024; dropping {'train/loss': 0.4192410111427307}.\n",
      "Epoch 260 | Batch 40/100 | Loss 0.414579\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26041 < 29024; dropping {'train/loss': 0.4151436388492584}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26042 < 29024; dropping {'train/loss': 0.5005406141281128}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26043 < 29024; dropping {'train/loss': 0.22692498564720154}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26044 < 29024; dropping {'train/loss': 0.34943774342536926}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26045 < 29024; dropping {'train/loss': 0.24217119812965393}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26046 < 29024; dropping {'train/loss': 0.4798312187194824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26047 < 29024; dropping {'train/loss': 0.2303611785173416}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26048 < 29024; dropping {'train/loss': 0.4458491802215576}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26049 < 29024; dropping {'train/loss': 0.5044786334037781}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26050 < 29024; dropping {'train/loss': 0.6646875739097595}.\n",
      "Epoch 260 | Batch 50/100 | Loss 0.412852\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26051 < 29024; dropping {'train/loss': 0.6510679125785828}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26052 < 29024; dropping {'train/loss': 0.37635210156440735}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26053 < 29024; dropping {'train/loss': 0.5142530798912048}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26054 < 29024; dropping {'train/loss': 0.4066367745399475}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26055 < 29024; dropping {'train/loss': 0.40958061814308167}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26056 < 29024; dropping {'train/loss': 0.2934219241142273}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26057 < 29024; dropping {'train/loss': 0.2761598229408264}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26058 < 29024; dropping {'train/loss': 0.5104753971099854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26059 < 29024; dropping {'train/loss': 0.5279830694198608}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26060 < 29024; dropping {'train/loss': 0.24885304272174835}.\n",
      "Epoch 260 | Batch 60/100 | Loss 0.414290\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26061 < 29024; dropping {'train/loss': 0.5722009539604187}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26062 < 29024; dropping {'train/loss': 0.5377805233001709}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26063 < 29024; dropping {'train/loss': 0.2108794003725052}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26064 < 29024; dropping {'train/loss': 0.2855522036552429}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26065 < 29024; dropping {'train/loss': 0.3123754858970642}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26066 < 29024; dropping {'train/loss': 0.5324501395225525}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26067 < 29024; dropping {'train/loss': 0.30446693301200867}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26068 < 29024; dropping {'train/loss': 0.4178318381309509}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26069 < 29024; dropping {'train/loss': 0.29223448038101196}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26070 < 29024; dropping {'train/loss': 0.5959807634353638}.\n",
      "Epoch 260 | Batch 70/100 | Loss 0.413130\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26071 < 29024; dropping {'train/loss': 0.30376872420310974}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26072 < 29024; dropping {'train/loss': 0.21722984313964844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26073 < 29024; dropping {'train/loss': 0.37387681007385254}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26074 < 29024; dropping {'train/loss': 0.3747454285621643}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26075 < 29024; dropping {'train/loss': 0.3320273160934448}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26076 < 29024; dropping {'train/loss': 0.48147016763687134}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26077 < 29024; dropping {'train/loss': 0.5066107511520386}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26078 < 29024; dropping {'train/loss': 0.40868115425109863}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26079 < 29024; dropping {'train/loss': 0.5492923855781555}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26080 < 29024; dropping {'train/loss': 0.47092145681381226}.\n",
      "Epoch 260 | Batch 80/100 | Loss 0.411722\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26081 < 29024; dropping {'train/loss': 0.3257492482662201}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26082 < 29024; dropping {'train/loss': 0.2584547698497772}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26083 < 29024; dropping {'train/loss': 0.638405442237854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26084 < 29024; dropping {'train/loss': 0.41413813829421997}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26085 < 29024; dropping {'train/loss': 0.24305549263954163}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26086 < 29024; dropping {'train/loss': 0.37838077545166016}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26087 < 29024; dropping {'train/loss': 0.6205263137817383}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26088 < 29024; dropping {'train/loss': 0.33507731556892395}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26089 < 29024; dropping {'train/loss': 0.3851568102836609}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26090 < 29024; dropping {'train/loss': 0.2886284589767456}.\n",
      "Epoch 260 | Batch 90/100 | Loss 0.409170\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26091 < 29024; dropping {'train/loss': 0.3348758816719055}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26092 < 29024; dropping {'train/loss': 0.38618341088294983}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26093 < 29024; dropping {'train/loss': 0.29578274488449097}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26094 < 29024; dropping {'train/loss': 0.1825558841228485}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26095 < 29024; dropping {'train/loss': 0.4272770881652832}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26096 < 29024; dropping {'train/loss': 0.4049701690673828}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26097 < 29024; dropping {'train/loss': 0.5550509691238403}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26098 < 29024; dropping {'train/loss': 0.382846474647522}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26099 < 29024; dropping {'train/loss': 0.5508803725242615}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26100 < 29024; dropping {'train/loss': 0.3540481626987457}.\n",
      "Epoch 260 | Batch 100/100 | Loss 0.406998\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26101 < 29024; dropping {'train/loss': 0.35189390182495117}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26102 < 29024; dropping {'train/loss': 0.18356624245643616}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26103 < 29024; dropping {'train/loss': 0.37053579092025757}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26104 < 29024; dropping {'train/loss': 0.3930790424346924}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26105 < 29024; dropping {'train/loss': 0.4730769991874695}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26106 < 29024; dropping {'train/loss': 0.4115855097770691}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26107 < 29024; dropping {'train/loss': 0.4068545401096344}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26108 < 29024; dropping {'train/loss': 0.4082789421081543}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26109 < 29024; dropping {'train/loss': 0.2437479943037033}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26110 < 29024; dropping {'train/loss': 0.467939555644989}.\n",
      "Epoch 261 | Batch 10/100 | Loss 0.371056\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26111 < 29024; dropping {'train/loss': 0.5080216526985168}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26112 < 29024; dropping {'train/loss': 0.5171293020248413}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26113 < 29024; dropping {'train/loss': 0.35608142614364624}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26114 < 29024; dropping {'train/loss': 0.2820209562778473}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26115 < 29024; dropping {'train/loss': 0.42552536725997925}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26116 < 29024; dropping {'train/loss': 0.20802263915538788}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26117 < 29024; dropping {'train/loss': 0.3223350942134857}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26118 < 29024; dropping {'train/loss': 0.18366070091724396}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26119 < 29024; dropping {'train/loss': 0.25670430064201355}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26120 < 29024; dropping {'train/loss': 0.3484695255756378}.\n",
      "Epoch 261 | Batch 20/100 | Loss 0.355926\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26121 < 29024; dropping {'train/loss': 0.44063490629196167}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26122 < 29024; dropping {'train/loss': 0.3485073149204254}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26123 < 29024; dropping {'train/loss': 0.5420897006988525}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26124 < 29024; dropping {'train/loss': 0.4180595874786377}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26125 < 29024; dropping {'train/loss': 0.5213432908058167}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26126 < 29024; dropping {'train/loss': 0.26826292276382446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26127 < 29024; dropping {'train/loss': 0.33189573884010315}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26128 < 29024; dropping {'train/loss': 0.4113098978996277}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26129 < 29024; dropping {'train/loss': 0.833846926689148}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26130 < 29024; dropping {'train/loss': 0.33510690927505493}.\n",
      "Epoch 261 | Batch 30/100 | Loss 0.385653\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26131 < 29024; dropping {'train/loss': 0.4196518361568451}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26132 < 29024; dropping {'train/loss': 0.3990684747695923}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26133 < 29024; dropping {'train/loss': 0.2932950258255005}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26134 < 29024; dropping {'train/loss': 0.3574565052986145}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26135 < 29024; dropping {'train/loss': 0.2276730239391327}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26136 < 29024; dropping {'train/loss': 0.49798113107681274}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26137 < 29024; dropping {'train/loss': 0.6207406520843506}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26138 < 29024; dropping {'train/loss': 0.3440348207950592}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26139 < 29024; dropping {'train/loss': 0.5637441277503967}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26140 < 29024; dropping {'train/loss': 0.4595237374305725}.\n",
      "Epoch 261 | Batch 40/100 | Loss 0.393819\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26141 < 29024; dropping {'train/loss': 0.4034031927585602}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26142 < 29024; dropping {'train/loss': 0.28533902764320374}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26143 < 29024; dropping {'train/loss': 0.16674160957336426}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26144 < 29024; dropping {'train/loss': 0.5913161039352417}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26145 < 29024; dropping {'train/loss': 0.42850035429000854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26146 < 29024; dropping {'train/loss': 0.38797903060913086}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26147 < 29024; dropping {'train/loss': 0.5197745561599731}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26148 < 29024; dropping {'train/loss': 0.4861488342285156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26149 < 29024; dropping {'train/loss': 0.3632057011127472}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26150 < 29024; dropping {'train/loss': 0.16678382456302643}.\n",
      "Epoch 261 | Batch 50/100 | Loss 0.391039\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26151 < 29024; dropping {'train/loss': 0.40745335817337036}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26152 < 29024; dropping {'train/loss': 0.6313823461532593}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26153 < 29024; dropping {'train/loss': 0.3605467677116394}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26154 < 29024; dropping {'train/loss': 0.2381150722503662}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26155 < 29024; dropping {'train/loss': 0.4345305562019348}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26156 < 29024; dropping {'train/loss': 0.591437578201294}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26157 < 29024; dropping {'train/loss': 0.34427767992019653}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26158 < 29024; dropping {'train/loss': 0.5662093758583069}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26159 < 29024; dropping {'train/loss': 0.2546355724334717}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26160 < 29024; dropping {'train/loss': 0.40768927335739136}.\n",
      "Epoch 261 | Batch 60/100 | Loss 0.396470\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26161 < 29024; dropping {'train/loss': 0.3831534683704376}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26162 < 29024; dropping {'train/loss': 0.3009566068649292}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26163 < 29024; dropping {'train/loss': 0.5785748958587646}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26164 < 29024; dropping {'train/loss': 0.3446885347366333}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26165 < 29024; dropping {'train/loss': 0.26933300495147705}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26166 < 29024; dropping {'train/loss': 0.43949753046035767}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26167 < 29024; dropping {'train/loss': 0.3358091413974762}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26168 < 29024; dropping {'train/loss': 0.3930298089981079}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26169 < 29024; dropping {'train/loss': 0.5195729732513428}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26170 < 29024; dropping {'train/loss': 0.5609859228134155}.\n",
      "Epoch 261 | Batch 70/100 | Loss 0.398769\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26171 < 29024; dropping {'train/loss': 0.18658187985420227}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26172 < 29024; dropping {'train/loss': 0.36194509267807007}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26173 < 29024; dropping {'train/loss': 0.718587338924408}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26174 < 29024; dropping {'train/loss': 0.20306149125099182}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26175 < 29024; dropping {'train/loss': 0.625893235206604}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26176 < 29024; dropping {'train/loss': 0.31663233041763306}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26177 < 29024; dropping {'train/loss': 0.1962985098361969}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26178 < 29024; dropping {'train/loss': 0.3924328684806824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26179 < 29024; dropping {'train/loss': 0.23172898590564728}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26180 < 29024; dropping {'train/loss': 0.43501192331314087}.\n",
      "Epoch 261 | Batch 80/100 | Loss 0.394775\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26181 < 29024; dropping {'train/loss': 0.333296537399292}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26182 < 29024; dropping {'train/loss': 0.4423268735408783}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26183 < 29024; dropping {'train/loss': 0.3097066879272461}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26184 < 29024; dropping {'train/loss': 0.5083313584327698}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26185 < 29024; dropping {'train/loss': 0.4833286702632904}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26186 < 29024; dropping {'train/loss': 0.06566323339939117}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26187 < 29024; dropping {'train/loss': 0.4334676265716553}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26188 < 29024; dropping {'train/loss': 0.31556859612464905}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26189 < 29024; dropping {'train/loss': 0.29472118616104126}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26190 < 29024; dropping {'train/loss': 0.8111968040466309}.\n",
      "Epoch 261 | Batch 90/100 | Loss 0.395329\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26191 < 29024; dropping {'train/loss': 0.34808430075645447}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26192 < 29024; dropping {'train/loss': 0.724323034286499}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26193 < 29024; dropping {'train/loss': 0.371804416179657}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26194 < 29024; dropping {'train/loss': 0.27963289618492126}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26195 < 29024; dropping {'train/loss': 0.4559177756309509}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26196 < 29024; dropping {'train/loss': 0.4510177969932556}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26197 < 29024; dropping {'train/loss': 0.4019877016544342}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26198 < 29024; dropping {'train/loss': 0.5073394775390625}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26199 < 29024; dropping {'train/loss': 0.4610827565193176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26200 < 29024; dropping {'train/loss': 0.5393938422203064}.\n",
      "Epoch 261 | Batch 100/100 | Loss 0.401202\n",
      "100 Test Protonet Acc = 80.10% +- 1.40%\n",
      "best model! save...\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26201 < 29024; dropping {'train/loss': 0.4276370406150818}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26202 < 29024; dropping {'train/loss': 0.2242966890335083}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26203 < 29024; dropping {'train/loss': 0.331625759601593}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26204 < 29024; dropping {'train/loss': 0.31181806325912476}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26205 < 29024; dropping {'train/loss': 0.5548087954521179}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26206 < 29024; dropping {'train/loss': 0.44544535875320435}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26207 < 29024; dropping {'train/loss': 0.40515390038490295}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26208 < 29024; dropping {'train/loss': 0.5378249287605286}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26209 < 29024; dropping {'train/loss': 0.2681768536567688}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26210 < 29024; dropping {'train/loss': 0.5339999794960022}.\n",
      "Epoch 262 | Batch 10/100 | Loss 0.404079\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26211 < 29024; dropping {'train/loss': 0.2587333917617798}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26212 < 29024; dropping {'train/loss': 0.4958342909812927}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26213 < 29024; dropping {'train/loss': 0.2931618392467499}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26214 < 29024; dropping {'train/loss': 0.19737140834331512}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26215 < 29024; dropping {'train/loss': 0.3605163097381592}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26216 < 29024; dropping {'train/loss': 0.6749565005302429}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26217 < 29024; dropping {'train/loss': 0.39393219351768494}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26218 < 29024; dropping {'train/loss': 0.3571619391441345}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26219 < 29024; dropping {'train/loss': 0.18793989717960358}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26220 < 29024; dropping {'train/loss': 0.49310413002967834}.\n",
      "Epoch 262 | Batch 20/100 | Loss 0.387675\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26221 < 29024; dropping {'train/loss': 0.3012915849685669}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26222 < 29024; dropping {'train/loss': 0.1523473560810089}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26223 < 29024; dropping {'train/loss': 0.21767310798168182}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26224 < 29024; dropping {'train/loss': 0.2787773907184601}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26225 < 29024; dropping {'train/loss': 0.34265902638435364}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26226 < 29024; dropping {'train/loss': 0.18211881816387177}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26227 < 29024; dropping {'train/loss': 0.2672639787197113}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26228 < 29024; dropping {'train/loss': 0.15527236461639404}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26229 < 29024; dropping {'train/loss': 0.3888484835624695}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26230 < 29024; dropping {'train/loss': 0.2547655701637268}.\n",
      "Epoch 262 | Batch 30/100 | Loss 0.343151\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26231 < 29024; dropping {'train/loss': 0.3551894724369049}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26232 < 29024; dropping {'train/loss': 0.24407926201820374}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26233 < 29024; dropping {'train/loss': 0.42707186937332153}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26234 < 29024; dropping {'train/loss': 0.5736474394798279}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26235 < 29024; dropping {'train/loss': 0.4247855544090271}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26236 < 29024; dropping {'train/loss': 0.3742257058620453}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26237 < 29024; dropping {'train/loss': 0.2374478131532669}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26238 < 29024; dropping {'train/loss': 0.36604923009872437}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26239 < 29024; dropping {'train/loss': 0.3099181354045868}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26240 < 29024; dropping {'train/loss': 0.1793297678232193}.\n",
      "Epoch 262 | Batch 40/100 | Loss 0.344657\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26241 < 29024; dropping {'train/loss': 0.5656542778015137}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26242 < 29024; dropping {'train/loss': 0.6753965616226196}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26243 < 29024; dropping {'train/loss': 0.47464531660079956}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26244 < 29024; dropping {'train/loss': 0.39155492186546326}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26245 < 29024; dropping {'train/loss': 0.28563278913497925}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26246 < 29024; dropping {'train/loss': 0.5096100568771362}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26247 < 29024; dropping {'train/loss': 0.630435585975647}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26248 < 29024; dropping {'train/loss': 0.3761865198612213}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26249 < 29024; dropping {'train/loss': 0.3199128210544586}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26250 < 29024; dropping {'train/loss': 0.7342739105224609}.\n",
      "Epoch 262 | Batch 50/100 | Loss 0.374991\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26251 < 29024; dropping {'train/loss': 0.28509631752967834}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26252 < 29024; dropping {'train/loss': 0.26281774044036865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26253 < 29024; dropping {'train/loss': 0.37790656089782715}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26254 < 29024; dropping {'train/loss': 0.2566617429256439}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26255 < 29024; dropping {'train/loss': 0.5821470618247986}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26256 < 29024; dropping {'train/loss': 0.24634066224098206}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26257 < 29024; dropping {'train/loss': 0.20887097716331482}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26258 < 29024; dropping {'train/loss': 0.19638162851333618}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26259 < 29024; dropping {'train/loss': 0.5143413543701172}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26260 < 29024; dropping {'train/loss': 0.2719785273075104}.\n",
      "Epoch 262 | Batch 60/100 | Loss 0.365868\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26261 < 29024; dropping {'train/loss': 0.4476361870765686}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26262 < 29024; dropping {'train/loss': 0.33517786860466003}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26263 < 29024; dropping {'train/loss': 0.39233922958374023}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26264 < 29024; dropping {'train/loss': 0.4659244120121002}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26265 < 29024; dropping {'train/loss': 0.4439094662666321}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26266 < 29024; dropping {'train/loss': 0.18135440349578857}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26267 < 29024; dropping {'train/loss': 0.6345311403274536}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26268 < 29024; dropping {'train/loss': 0.3650580048561096}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26269 < 29024; dropping {'train/loss': 0.6073716878890991}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26270 < 29024; dropping {'train/loss': 0.3144703507423401}.\n",
      "Epoch 262 | Batch 70/100 | Loss 0.373427\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26271 < 29024; dropping {'train/loss': 0.36189162731170654}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26272 < 29024; dropping {'train/loss': 0.4031701982021332}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26273 < 29024; dropping {'train/loss': 0.43605655431747437}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26274 < 29024; dropping {'train/loss': 0.44636979699134827}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26275 < 29024; dropping {'train/loss': 0.5301687121391296}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26276 < 29024; dropping {'train/loss': 0.3428710401058197}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26277 < 29024; dropping {'train/loss': 0.5831669569015503}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26278 < 29024; dropping {'train/loss': 0.26805219054222107}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26279 < 29024; dropping {'train/loss': 0.37421441078186035}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26280 < 29024; dropping {'train/loss': 0.5688852071762085}.\n",
      "Epoch 262 | Batch 80/100 | Loss 0.380684\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26281 < 29024; dropping {'train/loss': 0.21146568655967712}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26282 < 29024; dropping {'train/loss': 0.43514198064804077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26283 < 29024; dropping {'train/loss': 0.38073402643203735}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26284 < 29024; dropping {'train/loss': 0.3586285710334778}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26285 < 29024; dropping {'train/loss': 0.2208520919084549}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26286 < 29024; dropping {'train/loss': 0.2086702138185501}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26287 < 29024; dropping {'train/loss': 0.4259580671787262}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26288 < 29024; dropping {'train/loss': 0.3771783113479614}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26289 < 29024; dropping {'train/loss': 0.5280441045761108}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26290 < 29024; dropping {'train/loss': 0.17309851944446564}.\n",
      "Epoch 262 | Batch 90/100 | Loss 0.375272\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26291 < 29024; dropping {'train/loss': 0.4610133171081543}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26292 < 29024; dropping {'train/loss': 0.7054296731948853}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26293 < 29024; dropping {'train/loss': 0.6265774965286255}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26294 < 29024; dropping {'train/loss': 0.32235363125801086}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26295 < 29024; dropping {'train/loss': 0.4462430477142334}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26296 < 29024; dropping {'train/loss': 0.5121601819992065}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26297 < 29024; dropping {'train/loss': 0.13764998316764832}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26298 < 29024; dropping {'train/loss': 0.4629489779472351}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26299 < 29024; dropping {'train/loss': 0.2781934440135956}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26300 < 29024; dropping {'train/loss': 0.3790993094444275}.\n",
      "Epoch 262 | Batch 100/100 | Loss 0.381062\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26301 < 29024; dropping {'train/loss': 0.3837539255619049}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26302 < 29024; dropping {'train/loss': 0.3952149748802185}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26303 < 29024; dropping {'train/loss': 0.2105025053024292}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26304 < 29024; dropping {'train/loss': 0.5869986414909363}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26305 < 29024; dropping {'train/loss': 0.29476454854011536}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26306 < 29024; dropping {'train/loss': 0.4601004123687744}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26307 < 29024; dropping {'train/loss': 0.5106127858161926}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26308 < 29024; dropping {'train/loss': 0.28662434220314026}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26309 < 29024; dropping {'train/loss': 0.5555029511451721}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26310 < 29024; dropping {'train/loss': 0.3495728075504303}.\n",
      "Epoch 263 | Batch 10/100 | Loss 0.403365\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26311 < 29024; dropping {'train/loss': 0.37782642245292664}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26312 < 29024; dropping {'train/loss': 0.3042994737625122}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26313 < 29024; dropping {'train/loss': 0.3423171043395996}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26314 < 29024; dropping {'train/loss': 0.26827627420425415}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26315 < 29024; dropping {'train/loss': 0.520485520362854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26316 < 29024; dropping {'train/loss': 0.5011062622070312}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26317 < 29024; dropping {'train/loss': 0.3717929720878601}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26318 < 29024; dropping {'train/loss': 0.2907956540584564}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26319 < 29024; dropping {'train/loss': 0.48597532510757446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26320 < 29024; dropping {'train/loss': 0.47317737340927124}.\n",
      "Epoch 263 | Batch 20/100 | Loss 0.398485\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26321 < 29024; dropping {'train/loss': 0.20151181519031525}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26322 < 29024; dropping {'train/loss': 0.37191861867904663}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26323 < 29024; dropping {'train/loss': 0.17764194309711456}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26324 < 29024; dropping {'train/loss': 0.23002485930919647}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26325 < 29024; dropping {'train/loss': 0.41261228919029236}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26326 < 29024; dropping {'train/loss': 0.5523062944412231}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26327 < 29024; dropping {'train/loss': 0.4780677258968353}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26328 < 29024; dropping {'train/loss': 0.33111757040023804}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26329 < 29024; dropping {'train/loss': 0.20192715525627136}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26330 < 29024; dropping {'train/loss': 0.29493001103401184}.\n",
      "Epoch 263 | Batch 30/100 | Loss 0.374059\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26331 < 29024; dropping {'train/loss': 0.1940251886844635}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26332 < 29024; dropping {'train/loss': 0.2683568000793457}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26333 < 29024; dropping {'train/loss': 0.4728594720363617}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26334 < 29024; dropping {'train/loss': 0.6795365214347839}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26335 < 29024; dropping {'train/loss': 0.4133651852607727}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26336 < 29024; dropping {'train/loss': 0.35609668493270874}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26337 < 29024; dropping {'train/loss': 0.3806819021701813}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26338 < 29024; dropping {'train/loss': 0.20612995326519012}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26339 < 29024; dropping {'train/loss': 0.4314398765563965}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26340 < 29024; dropping {'train/loss': 0.40629443526268005}.\n",
      "Epoch 263 | Batch 40/100 | Loss 0.375764\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26341 < 29024; dropping {'train/loss': 0.21893449127674103}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26342 < 29024; dropping {'train/loss': 0.13412822782993317}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26343 < 29024; dropping {'train/loss': 0.24791879951953888}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26344 < 29024; dropping {'train/loss': 0.2229248583316803}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26345 < 29024; dropping {'train/loss': 0.3390088677406311}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26346 < 29024; dropping {'train/loss': 0.45793503522872925}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26347 < 29024; dropping {'train/loss': 0.3056996464729309}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26348 < 29024; dropping {'train/loss': 0.38651859760284424}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26349 < 29024; dropping {'train/loss': 0.563003420829773}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26350 < 29024; dropping {'train/loss': 0.7884657382965088}.\n",
      "Epoch 263 | Batch 50/100 | Loss 0.373902\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26351 < 29024; dropping {'train/loss': 0.24345210194587708}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26352 < 29024; dropping {'train/loss': 0.34484702348709106}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26353 < 29024; dropping {'train/loss': 0.3483434319496155}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26354 < 29024; dropping {'train/loss': 0.36522018909454346}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26355 < 29024; dropping {'train/loss': 0.48505425453186035}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26356 < 29024; dropping {'train/loss': 0.41767454147338867}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26357 < 29024; dropping {'train/loss': 0.4909586012363434}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26358 < 29024; dropping {'train/loss': 0.20757444202899933}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26359 < 29024; dropping {'train/loss': 0.5208008885383606}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26360 < 29024; dropping {'train/loss': 0.1605769693851471}.\n",
      "Epoch 263 | Batch 60/100 | Loss 0.371326\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26361 < 29024; dropping {'train/loss': 0.5900818109512329}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26362 < 29024; dropping {'train/loss': 0.3321783244609833}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26363 < 29024; dropping {'train/loss': 0.44284528493881226}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26364 < 29024; dropping {'train/loss': 0.627310574054718}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26365 < 29024; dropping {'train/loss': 0.5194025635719299}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26366 < 29024; dropping {'train/loss': 0.46039456129074097}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26367 < 29024; dropping {'train/loss': 0.34672075510025024}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26368 < 29024; dropping {'train/loss': 0.5718976259231567}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26369 < 29024; dropping {'train/loss': 0.343995600938797}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26370 < 29024; dropping {'train/loss': 0.43419843912124634}.\n",
      "Epoch 263 | Batch 70/100 | Loss 0.384980\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26371 < 29024; dropping {'train/loss': 0.2191830426454544}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26372 < 29024; dropping {'train/loss': 0.34743547439575195}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26373 < 29024; dropping {'train/loss': 0.41486063599586487}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26374 < 29024; dropping {'train/loss': 0.30769646167755127}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26375 < 29024; dropping {'train/loss': 0.23102083802223206}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26376 < 29024; dropping {'train/loss': 0.49297261238098145}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26377 < 29024; dropping {'train/loss': 0.6565799713134766}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26378 < 29024; dropping {'train/loss': 0.37376144528388977}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26379 < 29024; dropping {'train/loss': 0.30032479763031006}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26380 < 29024; dropping {'train/loss': 0.469503253698349}.\n",
      "Epoch 263 | Batch 80/100 | Loss 0.384524\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26381 < 29024; dropping {'train/loss': 0.271280974149704}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26382 < 29024; dropping {'train/loss': 0.33771222829818726}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26383 < 29024; dropping {'train/loss': 0.32370802760124207}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26384 < 29024; dropping {'train/loss': 0.5384751558303833}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26385 < 29024; dropping {'train/loss': 0.49324655532836914}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26386 < 29024; dropping {'train/loss': 0.14997513592243195}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26387 < 29024; dropping {'train/loss': 0.31428733468055725}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26388 < 29024; dropping {'train/loss': 0.24036331474781036}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26389 < 29024; dropping {'train/loss': 0.5373984575271606}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26390 < 29024; dropping {'train/loss': 0.3015134930610657}.\n",
      "Epoch 263 | Batch 90/100 | Loss 0.380777\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26391 < 29024; dropping {'train/loss': 0.4905349314212799}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26392 < 29024; dropping {'train/loss': 0.40102845430374146}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26393 < 29024; dropping {'train/loss': 0.5684178471565247}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26394 < 29024; dropping {'train/loss': 0.47176122665405273}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26395 < 29024; dropping {'train/loss': 0.454639732837677}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26396 < 29024; dropping {'train/loss': 0.4228280484676361}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26397 < 29024; dropping {'train/loss': 0.5147215127944946}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26398 < 29024; dropping {'train/loss': 0.32858389616012573}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26399 < 29024; dropping {'train/loss': 0.28353723883628845}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26400 < 29024; dropping {'train/loss': 0.11150544881820679}.\n",
      "Epoch 263 | Batch 100/100 | Loss 0.383175\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26401 < 29024; dropping {'train/loss': 0.2196960151195526}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26402 < 29024; dropping {'train/loss': 0.49271219968795776}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26403 < 29024; dropping {'train/loss': 0.46862897276878357}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26404 < 29024; dropping {'train/loss': 0.3576585054397583}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26405 < 29024; dropping {'train/loss': 0.19411422312259674}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26406 < 29024; dropping {'train/loss': 0.34707921743392944}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26407 < 29024; dropping {'train/loss': 0.41371116042137146}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26408 < 29024; dropping {'train/loss': 0.3015635311603546}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26409 < 29024; dropping {'train/loss': 0.6727630496025085}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26410 < 29024; dropping {'train/loss': 0.6038802266120911}.\n",
      "Epoch 264 | Batch 10/100 | Loss 0.407181\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26411 < 29024; dropping {'train/loss': 0.31654471158981323}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26412 < 29024; dropping {'train/loss': 0.2875726521015167}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26413 < 29024; dropping {'train/loss': 0.4344615042209625}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26414 < 29024; dropping {'train/loss': 0.35073840618133545}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26415 < 29024; dropping {'train/loss': 0.48615893721580505}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26416 < 29024; dropping {'train/loss': 0.3550789952278137}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26417 < 29024; dropping {'train/loss': 0.25094181299209595}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26418 < 29024; dropping {'train/loss': 0.21774575114250183}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26419 < 29024; dropping {'train/loss': 0.36713922023773193}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26420 < 29024; dropping {'train/loss': 0.3957939147949219}.\n",
      "Epoch 264 | Batch 20/100 | Loss 0.376699\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26421 < 29024; dropping {'train/loss': 0.22946448624134064}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26422 < 29024; dropping {'train/loss': 0.5347797870635986}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26423 < 29024; dropping {'train/loss': 0.4441327452659607}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26424 < 29024; dropping {'train/loss': 0.3622722029685974}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26425 < 29024; dropping {'train/loss': 0.5147144794464111}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26426 < 29024; dropping {'train/loss': 0.40916872024536133}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26427 < 29024; dropping {'train/loss': 0.31580671668052673}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26428 < 29024; dropping {'train/loss': 0.320727676153183}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26429 < 29024; dropping {'train/loss': 0.4931131899356842}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26430 < 29024; dropping {'train/loss': 0.32959550619125366}.\n",
      "Epoch 264 | Batch 30/100 | Loss 0.382925\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26431 < 29024; dropping {'train/loss': 0.2351180613040924}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26432 < 29024; dropping {'train/loss': 0.33736902475357056}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26433 < 29024; dropping {'train/loss': 0.8118893504142761}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26434 < 29024; dropping {'train/loss': 0.30732864141464233}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26435 < 29024; dropping {'train/loss': 0.5159083008766174}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26436 < 29024; dropping {'train/loss': 0.19164636731147766}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26437 < 29024; dropping {'train/loss': 0.43614310026168823}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26438 < 29024; dropping {'train/loss': 0.13968612253665924}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26439 < 29024; dropping {'train/loss': 0.49035245180130005}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26440 < 29024; dropping {'train/loss': 0.40376177430152893}.\n",
      "Epoch 264 | Batch 40/100 | Loss 0.383924\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26441 < 29024; dropping {'train/loss': 0.42399439215660095}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26442 < 29024; dropping {'train/loss': 0.3915376663208008}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26443 < 29024; dropping {'train/loss': 0.36145150661468506}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26444 < 29024; dropping {'train/loss': 0.21848449110984802}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26445 < 29024; dropping {'train/loss': 0.43389683961868286}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26446 < 29024; dropping {'train/loss': 0.41521620750427246}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26447 < 29024; dropping {'train/loss': 0.5902295112609863}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26448 < 29024; dropping {'train/loss': 0.23360423743724823}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26449 < 29024; dropping {'train/loss': 0.3030748665332794}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26450 < 29024; dropping {'train/loss': 0.5151947736740112}.\n",
      "Epoch 264 | Batch 50/100 | Loss 0.384873\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26451 < 29024; dropping {'train/loss': 0.24966172873973846}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26452 < 29024; dropping {'train/loss': 0.36369454860687256}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26453 < 29024; dropping {'train/loss': 0.4738444685935974}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26454 < 29024; dropping {'train/loss': 0.3380886912345886}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26455 < 29024; dropping {'train/loss': 0.18606393039226532}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26456 < 29024; dropping {'train/loss': 0.2003728151321411}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26457 < 29024; dropping {'train/loss': 0.33172255754470825}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26458 < 29024; dropping {'train/loss': 0.3829039931297302}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26459 < 29024; dropping {'train/loss': 0.3148801326751709}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26460 < 29024; dropping {'train/loss': 0.4170633852481842}.\n",
      "Epoch 264 | Batch 60/100 | Loss 0.375032\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26461 < 29024; dropping {'train/loss': 0.4095745086669922}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26462 < 29024; dropping {'train/loss': 0.2569332718849182}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26463 < 29024; dropping {'train/loss': 0.47027117013931274}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26464 < 29024; dropping {'train/loss': 0.4246833920478821}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26465 < 29024; dropping {'train/loss': 0.3745005130767822}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26466 < 29024; dropping {'train/loss': 0.364351361989975}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26467 < 29024; dropping {'train/loss': 0.17639261484146118}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26468 < 29024; dropping {'train/loss': 0.4676094055175781}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26469 < 29024; dropping {'train/loss': 0.3333825469017029}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26470 < 29024; dropping {'train/loss': 0.36800119280815125}.\n",
      "Epoch 264 | Batch 70/100 | Loss 0.373538\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26471 < 29024; dropping {'train/loss': 0.47352486848831177}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26472 < 29024; dropping {'train/loss': 0.46091896295547485}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26473 < 29024; dropping {'train/loss': 0.2422047108411789}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26474 < 29024; dropping {'train/loss': 0.26707419753074646}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26475 < 29024; dropping {'train/loss': 0.339593768119812}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26476 < 29024; dropping {'train/loss': 0.2622707188129425}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26477 < 29024; dropping {'train/loss': 0.47952571511268616}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26478 < 29024; dropping {'train/loss': 0.47090083360671997}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26479 < 29024; dropping {'train/loss': 0.3388625979423523}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26480 < 29024; dropping {'train/loss': 0.3877388834953308}.\n",
      "Epoch 264 | Batch 80/100 | Loss 0.373378\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26481 < 29024; dropping {'train/loss': 0.25917476415634155}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26482 < 29024; dropping {'train/loss': 0.36123421788215637}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26483 < 29024; dropping {'train/loss': 0.4616197943687439}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26484 < 29024; dropping {'train/loss': 0.5610718727111816}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26485 < 29024; dropping {'train/loss': 0.3717140257358551}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26486 < 29024; dropping {'train/loss': 0.3513376712799072}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26487 < 29024; dropping {'train/loss': 0.32069113850593567}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26488 < 29024; dropping {'train/loss': 0.37037038803100586}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26489 < 29024; dropping {'train/loss': 0.41624727845191956}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26490 < 29024; dropping {'train/loss': 0.22397597134113312}.\n",
      "Epoch 264 | Batch 90/100 | Loss 0.372974\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26491 < 29024; dropping {'train/loss': 0.44610410928726196}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26492 < 29024; dropping {'train/loss': 0.45012784004211426}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26493 < 29024; dropping {'train/loss': 0.4268713891506195}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26494 < 29024; dropping {'train/loss': 0.3686261773109436}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26495 < 29024; dropping {'train/loss': 0.1376565396785736}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26496 < 29024; dropping {'train/loss': 0.35232487320899963}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26497 < 29024; dropping {'train/loss': 0.252309113740921}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26498 < 29024; dropping {'train/loss': 0.22862279415130615}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26499 < 29024; dropping {'train/loss': 0.6074802875518799}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26500 < 29024; dropping {'train/loss': 0.325740784406662}.\n",
      "Epoch 264 | Batch 100/100 | Loss 0.371636\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26501 < 29024; dropping {'train/loss': 0.37457093596458435}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26502 < 29024; dropping {'train/loss': 0.3292156457901001}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26503 < 29024; dropping {'train/loss': 0.3052874207496643}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26504 < 29024; dropping {'train/loss': 0.42670974135398865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26505 < 29024; dropping {'train/loss': 0.4601052403450012}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26506 < 29024; dropping {'train/loss': 0.263592392206192}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26507 < 29024; dropping {'train/loss': 0.32490405440330505}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26508 < 29024; dropping {'train/loss': 0.27778589725494385}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26509 < 29024; dropping {'train/loss': 0.3761119544506073}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26510 < 29024; dropping {'train/loss': 0.4055400788784027}.\n",
      "Epoch 265 | Batch 10/100 | Loss 0.354382\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26511 < 29024; dropping {'train/loss': 0.5738000869750977}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26512 < 29024; dropping {'train/loss': 0.3742862343788147}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26513 < 29024; dropping {'train/loss': 0.23386554419994354}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26514 < 29024; dropping {'train/loss': 0.5183595418930054}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26515 < 29024; dropping {'train/loss': 0.3142455518245697}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26516 < 29024; dropping {'train/loss': 0.3720879554748535}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26517 < 29024; dropping {'train/loss': 0.396750271320343}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26518 < 29024; dropping {'train/loss': 0.14957444369792938}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26519 < 29024; dropping {'train/loss': 0.10061778128147125}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26520 < 29024; dropping {'train/loss': 0.6182552576065063}.\n",
      "Epoch 265 | Batch 20/100 | Loss 0.359783\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26521 < 29024; dropping {'train/loss': 0.31248739361763}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26522 < 29024; dropping {'train/loss': 0.24807271361351013}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26523 < 29024; dropping {'train/loss': 0.2456456869840622}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26524 < 29024; dropping {'train/loss': 0.44736355543136597}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26525 < 29024; dropping {'train/loss': 0.18672491610050201}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26526 < 29024; dropping {'train/loss': 0.22630631923675537}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26527 < 29024; dropping {'train/loss': 0.47407108545303345}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26528 < 29024; dropping {'train/loss': 0.3191598653793335}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26529 < 29024; dropping {'train/loss': 0.3947114646434784}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26530 < 29024; dropping {'train/loss': 0.3910009562969208}.\n",
      "Epoch 265 | Batch 30/100 | Loss 0.348040\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26531 < 29024; dropping {'train/loss': 0.22842144966125488}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26532 < 29024; dropping {'train/loss': 0.1537545621395111}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26533 < 29024; dropping {'train/loss': 0.5496118664741516}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26534 < 29024; dropping {'train/loss': 0.4062005579471588}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26535 < 29024; dropping {'train/loss': 0.3858887553215027}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26536 < 29024; dropping {'train/loss': 0.2815844416618347}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26537 < 29024; dropping {'train/loss': 0.5115863084793091}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26538 < 29024; dropping {'train/loss': 0.12952251732349396}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26539 < 29024; dropping {'train/loss': 0.9446600675582886}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26540 < 29024; dropping {'train/loss': 0.5187930464744568}.\n",
      "Epoch 265 | Batch 40/100 | Loss 0.363781\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26541 < 29024; dropping {'train/loss': 0.1502154916524887}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26542 < 29024; dropping {'train/loss': 0.2666226327419281}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26543 < 29024; dropping {'train/loss': 0.4688292443752289}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26544 < 29024; dropping {'train/loss': 0.30604326725006104}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26545 < 29024; dropping {'train/loss': 0.28843119740486145}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26546 < 29024; dropping {'train/loss': 0.200055330991745}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26547 < 29024; dropping {'train/loss': 0.21803276240825653}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26548 < 29024; dropping {'train/loss': 0.6599236726760864}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26549 < 29024; dropping {'train/loss': 0.47387900948524475}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26550 < 29024; dropping {'train/loss': 0.3424546718597412}.\n",
      "Epoch 265 | Batch 50/100 | Loss 0.358514\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26551 < 29024; dropping {'train/loss': 0.5053566694259644}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26552 < 29024; dropping {'train/loss': 0.4304880201816559}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26553 < 29024; dropping {'train/loss': 0.48647546768188477}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26554 < 29024; dropping {'train/loss': 0.43534451723098755}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26555 < 29024; dropping {'train/loss': 0.16210490465164185}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26556 < 29024; dropping {'train/loss': 0.4039389491081238}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26557 < 29024; dropping {'train/loss': 0.13457195460796356}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26558 < 29024; dropping {'train/loss': 0.4287237226963043}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26559 < 29024; dropping {'train/loss': 0.339646577835083}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26560 < 29024; dropping {'train/loss': 0.5788278579711914}.\n",
      "Epoch 265 | Batch 60/100 | Loss 0.363853\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26561 < 29024; dropping {'train/loss': 0.11927653849124908}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26562 < 29024; dropping {'train/loss': 0.5657309293746948}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26563 < 29024; dropping {'train/loss': 0.38277050852775574}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26564 < 29024; dropping {'train/loss': 0.4555209279060364}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26565 < 29024; dropping {'train/loss': 0.19523663818836212}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26566 < 29024; dropping {'train/loss': 0.33175045251846313}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26567 < 29024; dropping {'train/loss': 0.4707478880882263}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26568 < 29024; dropping {'train/loss': 0.286318838596344}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26569 < 29024; dropping {'train/loss': 0.2892116606235504}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26570 < 29024; dropping {'train/loss': 0.31249067187309265}.\n",
      "Epoch 265 | Batch 70/100 | Loss 0.360575\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26571 < 29024; dropping {'train/loss': 0.6064671277999878}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26572 < 29024; dropping {'train/loss': 0.4399120807647705}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26573 < 29024; dropping {'train/loss': 0.16750551760196686}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26574 < 29024; dropping {'train/loss': 0.511423647403717}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26575 < 29024; dropping {'train/loss': 0.31753042340278625}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26576 < 29024; dropping {'train/loss': 0.5416319370269775}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26577 < 29024; dropping {'train/loss': 0.3498130440711975}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26578 < 29024; dropping {'train/loss': 0.18776360154151917}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26579 < 29024; dropping {'train/loss': 0.3308621942996979}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26580 < 29024; dropping {'train/loss': 0.7777072191238403}.\n",
      "Epoch 265 | Batch 80/100 | Loss 0.368386\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26581 < 29024; dropping {'train/loss': 0.30051353573799133}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26582 < 29024; dropping {'train/loss': 0.35489481687545776}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26583 < 29024; dropping {'train/loss': 0.36685866117477417}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26584 < 29024; dropping {'train/loss': 0.3123137354850769}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26585 < 29024; dropping {'train/loss': 0.3948758840560913}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26586 < 29024; dropping {'train/loss': 0.18808740377426147}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26587 < 29024; dropping {'train/loss': 0.4625174105167389}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26588 < 29024; dropping {'train/loss': 0.19992606341838837}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26589 < 29024; dropping {'train/loss': 0.5959196090698242}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26590 < 29024; dropping {'train/loss': 0.19316616654396057}.\n",
      "Epoch 265 | Batch 90/100 | Loss 0.364888\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26591 < 29024; dropping {'train/loss': 0.24606528878211975}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26592 < 29024; dropping {'train/loss': 0.38329368829727173}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26593 < 29024; dropping {'train/loss': 0.2913523316383362}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26594 < 29024; dropping {'train/loss': 0.39537495374679565}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26595 < 29024; dropping {'train/loss': 0.39511623978614807}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26596 < 29024; dropping {'train/loss': 0.28423255681991577}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26597 < 29024; dropping {'train/loss': 0.331387460231781}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26598 < 29024; dropping {'train/loss': 0.27956703305244446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26599 < 29024; dropping {'train/loss': 0.23551850020885468}.\n",
      "Epoch 265 | Batch 100/100 | Loss 0.359979\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26600 < 29024; dropping {'train/loss': 0.3160487115383148}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26601 < 29024; dropping {'train/loss': 0.2858509421348572}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26602 < 29024; dropping {'train/loss': 0.3542518615722656}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26603 < 29024; dropping {'train/loss': 0.19369688630104065}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26604 < 29024; dropping {'train/loss': 0.31176185607910156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26605 < 29024; dropping {'train/loss': 0.35593804717063904}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26606 < 29024; dropping {'train/loss': 0.30455636978149414}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26607 < 29024; dropping {'train/loss': 0.1752803772687912}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26608 < 29024; dropping {'train/loss': 0.11155377328395844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26609 < 29024; dropping {'train/loss': 0.2925242483615875}.\n",
      "Epoch 266 | Batch 10/100 | Loss 0.272963wandb: WARNING Step must only increase in log calls.  Step 26610 < 29024; dropping {'train/loss': 0.3442175090312958}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26611 < 29024; dropping {'train/loss': 0.16626900434494019}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26612 < 29024; dropping {'train/loss': 0.444200336933136}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26613 < 29024; dropping {'train/loss': 0.546120822429657}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26614 < 29024; dropping {'train/loss': 0.2663039565086365}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26615 < 29024; dropping {'train/loss': 0.24394389986991882}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26616 < 29024; dropping {'train/loss': 0.4004507064819336}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26617 < 29024; dropping {'train/loss': 0.3905467391014099}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26618 < 29024; dropping {'train/loss': 0.24949944019317627}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26619 < 29024; dropping {'train/loss': 0.7477916479110718}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26620 < 29024; dropping {'train/loss': 0.34729427099227905}.\n",
      "Epoch 266 | Batch 20/100 | Loss 0.326603\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26621 < 29024; dropping {'train/loss': 0.3093908429145813}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26622 < 29024; dropping {'train/loss': 0.3690364360809326}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26623 < 29024; dropping {'train/loss': 0.47920042276382446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26624 < 29024; dropping {'train/loss': 0.3735444247722626}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26625 < 29024; dropping {'train/loss': 0.2042803317308426}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26626 < 29024; dropping {'train/loss': 0.28555041551589966}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26627 < 29024; dropping {'train/loss': 0.40637245774269104}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26628 < 29024; dropping {'train/loss': 0.14378127455711365}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26629 < 29024; dropping {'train/loss': 0.30937856435775757}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26630 < 29024; dropping {'train/loss': 0.5083670616149902}.\n",
      "Epoch 266 | Batch 30/100 | Loss 0.330698\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26631 < 29024; dropping {'train/loss': 0.4400614798069}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26632 < 29024; dropping {'train/loss': 0.3455870747566223}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26633 < 29024; dropping {'train/loss': 0.2646077573299408}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26634 < 29024; dropping {'train/loss': 0.24478629231452942}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26635 < 29024; dropping {'train/loss': 0.47091054916381836}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26636 < 29024; dropping {'train/loss': 0.45784005522727966}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26637 < 29024; dropping {'train/loss': 0.269491046667099}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26638 < 29024; dropping {'train/loss': 0.31143128871917725}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26639 < 29024; dropping {'train/loss': 0.4669235646724701}.\n",
      "Epoch 266 | Batch 40/100 | Loss 0.339910wandb: WARNING Step must only increase in log calls.  Step 26640 < 29024; dropping {'train/loss': 0.4038143754005432}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26641 < 29024; dropping {'train/loss': 0.17584392428398132}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26642 < 29024; dropping {'train/loss': 0.6874929666519165}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26643 < 29024; dropping {'train/loss': 0.29052096605300903}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26644 < 29024; dropping {'train/loss': 0.3179484009742737}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26645 < 29024; dropping {'train/loss': 0.5303223133087158}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26646 < 29024; dropping {'train/loss': 0.34044843912124634}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26647 < 29024; dropping {'train/loss': 0.5921472907066345}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26648 < 29024; dropping {'train/loss': 0.33378225564956665}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26649 < 29024; dropping {'train/loss': 0.4957437515258789}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26650 < 29024; dropping {'train/loss': 0.1587565839290619}.\n",
      "Epoch 266 | Batch 50/100 | Loss 0.350388\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26651 < 29024; dropping {'train/loss': 0.29719221591949463}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26652 < 29024; dropping {'train/loss': 0.2785221338272095}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26653 < 29024; dropping {'train/loss': 0.457631915807724}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26654 < 29024; dropping {'train/loss': 0.429169237613678}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26655 < 29024; dropping {'train/loss': 0.49876436591148376}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26656 < 29024; dropping {'train/loss': 0.26796358823776245}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26657 < 29024; dropping {'train/loss': 0.2701696455478668}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26658 < 29024; dropping {'train/loss': 0.37290501594543457}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26659 < 29024; dropping {'train/loss': 0.28355538845062256}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26660 < 29024; dropping {'train/loss': 0.26156920194625854}.\n",
      "Epoch 266 | Batch 60/100 | Loss 0.348948\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26661 < 29024; dropping {'train/loss': 0.5588347315788269}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26662 < 29024; dropping {'train/loss': 0.26155149936676025}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26663 < 29024; dropping {'train/loss': 0.3991772532463074}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26664 < 29024; dropping {'train/loss': 0.18273606896400452}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26665 < 29024; dropping {'train/loss': 0.30238234996795654}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26666 < 29024; dropping {'train/loss': 0.5380920767784119}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26667 < 29024; dropping {'train/loss': 0.24799302220344543}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26668 < 29024; dropping {'train/loss': 0.2534576654434204}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26669 < 29024; dropping {'train/loss': 0.2223091870546341}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26670 < 29024; dropping {'train/loss': 0.36155444383621216}.\n",
      "Epoch 266 | Batch 70/100 | Loss 0.346642\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26671 < 29024; dropping {'train/loss': 0.35792940855026245}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26672 < 29024; dropping {'train/loss': 0.7420125603675842}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26673 < 29024; dropping {'train/loss': 0.35771286487579346}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26674 < 29024; dropping {'train/loss': 0.440886914730072}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26675 < 29024; dropping {'train/loss': 0.4078649580478668}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26676 < 29024; dropping {'train/loss': 0.33388882875442505}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26677 < 29024; dropping {'train/loss': 0.20998768508434296}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26678 < 29024; dropping {'train/loss': 0.5024954080581665}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26679 < 29024; dropping {'train/loss': 0.43849706649780273}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26680 < 29024; dropping {'train/loss': 0.5413490533828735}.\n",
      "Epoch 266 | Batch 80/100 | Loss 0.357470\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26681 < 29024; dropping {'train/loss': 0.28219303488731384}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26682 < 29024; dropping {'train/loss': 0.3456128239631653}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26683 < 29024; dropping {'train/loss': 0.27245837450027466}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26684 < 29024; dropping {'train/loss': 0.44980388879776}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26685 < 29024; dropping {'train/loss': 0.31369131803512573}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26686 < 29024; dropping {'train/loss': 0.4255070686340332}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26687 < 29024; dropping {'train/loss': 0.6160939335823059}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26688 < 29024; dropping {'train/loss': 0.5745751261711121}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26689 < 29024; dropping {'train/loss': 0.3418407738208771}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26690 < 29024; dropping {'train/loss': 0.3711780905723572}.\n",
      "Epoch 266 | Batch 90/100 | Loss 0.362117\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26691 < 29024; dropping {'train/loss': 0.38725343346595764}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26692 < 29024; dropping {'train/loss': 0.4102019667625427}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26693 < 29024; dropping {'train/loss': 0.1837644726037979}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26694 < 29024; dropping {'train/loss': 0.4080243706703186}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26695 < 29024; dropping {'train/loss': 0.1939363181591034}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26696 < 29024; dropping {'train/loss': 0.25052982568740845}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26697 < 29024; dropping {'train/loss': 0.4777084290981293}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26698 < 29024; dropping {'train/loss': 0.3463497459888458}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26699 < 29024; dropping {'train/loss': 0.3640066385269165}.\n",
      "Epoch 266 | Batch 100/100 | Loss 0.358214wandb: WARNING Step must only increase in log calls.  Step 26700 < 29024; dropping {'train/loss': 0.20912961661815643}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26701 < 29024; dropping {'train/loss': 0.4740517735481262}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26702 < 29024; dropping {'train/loss': 0.2664388418197632}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26703 < 29024; dropping {'train/loss': 0.4740025997161865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26704 < 29024; dropping {'train/loss': 0.5383847951889038}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26705 < 29024; dropping {'train/loss': 0.31694987416267395}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26706 < 29024; dropping {'train/loss': 0.4737947881221771}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26707 < 29024; dropping {'train/loss': 0.4125754237174988}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26708 < 29024; dropping {'train/loss': 0.3463585078716278}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26709 < 29024; dropping {'train/loss': 0.30357351899147034}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26710 < 29024; dropping {'train/loss': 0.539636492729187}.\n",
      "Epoch 267 | Batch 10/100 | Loss 0.414577\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26711 < 29024; dropping {'train/loss': 0.5051215887069702}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26712 < 29024; dropping {'train/loss': 0.4421406686306}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26713 < 29024; dropping {'train/loss': 0.15018269419670105}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26714 < 29024; dropping {'train/loss': 0.2466125786304474}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26715 < 29024; dropping {'train/loss': 0.2503446638584137}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26716 < 29024; dropping {'train/loss': 0.40845784544944763}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26717 < 29024; dropping {'train/loss': 0.5709553956985474}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26718 < 29024; dropping {'train/loss': 0.4125870168209076}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26719 < 29024; dropping {'train/loss': 0.30208510160446167}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26720 < 29024; dropping {'train/loss': 0.4434688985347748}.\n",
      "Epoch 267 | Batch 20/100 | Loss 0.393886\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26721 < 29024; dropping {'train/loss': 0.1472669392824173}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26722 < 29024; dropping {'train/loss': 0.21553118526935577}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26723 < 29024; dropping {'train/loss': 0.3448403775691986}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26724 < 29024; dropping {'train/loss': 0.5047146677970886}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26725 < 29024; dropping {'train/loss': 0.3200964331626892}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26726 < 29024; dropping {'train/loss': 0.38164108991622925}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26727 < 29024; dropping {'train/loss': 0.40424877405166626}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26728 < 29024; dropping {'train/loss': 0.435802161693573}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26729 < 29024; dropping {'train/loss': 0.2229948341846466}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26730 < 29024; dropping {'train/loss': 0.15124137699604034}.\n",
      "Epoch 267 | Batch 30/100 | Loss 0.366870\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26731 < 29024; dropping {'train/loss': 0.5401069521903992}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26732 < 29024; dropping {'train/loss': 0.28511935472488403}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26733 < 29024; dropping {'train/loss': 0.29032713174819946}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26734 < 29024; dropping {'train/loss': 0.16342607140541077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26735 < 29024; dropping {'train/loss': 0.4039517343044281}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26736 < 29024; dropping {'train/loss': 0.47600317001342773}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26737 < 29024; dropping {'train/loss': 0.517584502696991}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26738 < 29024; dropping {'train/loss': 0.252987802028656}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26739 < 29024; dropping {'train/loss': 0.4551006257534027}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26740 < 29024; dropping {'train/loss': 0.29548993706703186}.\n",
      "Epoch 267 | Batch 40/100 | Loss 0.367155\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26741 < 29024; dropping {'train/loss': 0.4825913906097412}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26742 < 29024; dropping {'train/loss': 0.45322829484939575}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26743 < 29024; dropping {'train/loss': 0.6296402215957642}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26744 < 29024; dropping {'train/loss': 0.5600499510765076}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26745 < 29024; dropping {'train/loss': 0.32013577222824097}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26746 < 29024; dropping {'train/loss': 0.4726930260658264}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26747 < 29024; dropping {'train/loss': 0.3584723472595215}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26748 < 29024; dropping {'train/loss': 0.45465287566185}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26749 < 29024; dropping {'train/loss': 0.4698050022125244}.\n",
      "Epoch 267 | Batch 50/100 | Loss 0.384501wandb: WARNING Step must only increase in log calls.  Step 26750 < 29024; dropping {'train/loss': 0.33757284283638}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26751 < 29024; dropping {'train/loss': 0.34721639752388}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26752 < 29024; dropping {'train/loss': 0.317823588848114}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26753 < 29024; dropping {'train/loss': 0.4387247562408447}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26754 < 29024; dropping {'train/loss': 0.45514601469039917}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26755 < 29024; dropping {'train/loss': 0.42808184027671814}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26756 < 29024; dropping {'train/loss': 0.4294322431087494}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26757 < 29024; dropping {'train/loss': 0.2652425467967987}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26758 < 29024; dropping {'train/loss': 0.672060489654541}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26759 < 29024; dropping {'train/loss': 0.4944985508918762}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26760 < 29024; dropping {'train/loss': 0.327892929315567}.\n",
      "Epoch 267 | Batch 60/100 | Loss 0.390019\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26761 < 29024; dropping {'train/loss': 0.35875794291496277}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26762 < 29024; dropping {'train/loss': 0.28205591440200806}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26763 < 29024; dropping {'train/loss': 0.301553875207901}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26764 < 29024; dropping {'train/loss': 0.18859711289405823}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26765 < 29024; dropping {'train/loss': 0.24749641120433807}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26766 < 29024; dropping {'train/loss': 0.4275349974632263}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26767 < 29024; dropping {'train/loss': 0.24020645022392273}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26768 < 29024; dropping {'train/loss': 0.38367533683776855}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26769 < 29024; dropping {'train/loss': 0.5011481046676636}.\n",
      "Epoch 267 | Batch 70/100 | Loss 0.381129wandb: WARNING Step must only increase in log calls.  Step 26770 < 29024; dropping {'train/loss': 0.34682223200798035}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26771 < 29024; dropping {'train/loss': 0.15812453627586365}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26772 < 29024; dropping {'train/loss': 0.40672993659973145}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26773 < 29024; dropping {'train/loss': 0.5281618237495422}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26774 < 29024; dropping {'train/loss': 0.42296165227890015}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26775 < 29024; dropping {'train/loss': 0.34016746282577515}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26776 < 29024; dropping {'train/loss': 0.2993466556072235}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26777 < 29024; dropping {'train/loss': 0.5091997981071472}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26778 < 29024; dropping {'train/loss': 0.26789915561676025}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26779 < 29024; dropping {'train/loss': 0.4086831510066986}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26780 < 29024; dropping {'train/loss': 0.7264931797981262}.\n",
      "Epoch 267 | Batch 80/100 | Loss 0.384335\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26781 < 29024; dropping {'train/loss': 0.5780589580535889}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26782 < 29024; dropping {'train/loss': 0.35140740871429443}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26783 < 29024; dropping {'train/loss': 0.546425461769104}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26784 < 29024; dropping {'train/loss': 0.26850569248199463}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26785 < 29024; dropping {'train/loss': 0.5677021145820618}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26786 < 29024; dropping {'train/loss': 0.48043689131736755}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26787 < 29024; dropping {'train/loss': 0.17429031431674957}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26788 < 29024; dropping {'train/loss': 0.353763610124588}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26789 < 29024; dropping {'train/loss': 0.5644198656082153}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26790 < 29024; dropping {'train/loss': 0.5359248518943787}.\n",
      "Epoch 267 | Batch 90/100 | Loss 0.390752\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26791 < 29024; dropping {'train/loss': 0.44424599409103394}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26792 < 29024; dropping {'train/loss': 0.4644099175930023}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26793 < 29024; dropping {'train/loss': 0.2588961720466614}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26794 < 29024; dropping {'train/loss': 0.22702892124652863}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26795 < 29024; dropping {'train/loss': 0.06071951985359192}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26796 < 29024; dropping {'train/loss': 0.364957720041275}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26797 < 29024; dropping {'train/loss': 0.5088496804237366}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26798 < 29024; dropping {'train/loss': 0.6160565614700317}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26799 < 29024; dropping {'train/loss': 0.3260076940059662}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26800 < 29024; dropping {'train/loss': 0.5294946432113647}.\n",
      "Epoch 267 | Batch 100/100 | Loss 0.389684\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26801 < 29024; dropping {'train/loss': 0.12331581115722656}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26802 < 29024; dropping {'train/loss': 0.3160347640514374}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26803 < 29024; dropping {'train/loss': 0.4948630928993225}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26804 < 29024; dropping {'train/loss': 0.4463079571723938}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26805 < 29024; dropping {'train/loss': 0.4988359808921814}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26806 < 29024; dropping {'train/loss': 0.19683119654655457}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26807 < 29024; dropping {'train/loss': 0.3852245509624481}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26808 < 29024; dropping {'train/loss': 0.6876348257064819}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26809 < 29024; dropping {'train/loss': 0.45676374435424805}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26810 < 29024; dropping {'train/loss': 0.19601845741271973}.\n",
      "Epoch 268 | Batch 10/100 | Loss 0.380183\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26811 < 29024; dropping {'train/loss': 0.23284950852394104}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26812 < 29024; dropping {'train/loss': 0.6494619846343994}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26813 < 29024; dropping {'train/loss': 0.31448858976364136}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26814 < 29024; dropping {'train/loss': 0.3615063726902008}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26815 < 29024; dropping {'train/loss': 0.36011311411857605}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26816 < 29024; dropping {'train/loss': 0.47359949350357056}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26817 < 29024; dropping {'train/loss': 0.3504624664783478}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26818 < 29024; dropping {'train/loss': 0.16990642249584198}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26819 < 29024; dropping {'train/loss': 0.33773812651634216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26820 < 29024; dropping {'train/loss': 0.5309889912605286}.\n",
      "Epoch 268 | Batch 20/100 | Loss 0.379147\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26821 < 29024; dropping {'train/loss': 0.5571701526641846}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26822 < 29024; dropping {'train/loss': 0.5328230857849121}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26823 < 29024; dropping {'train/loss': 0.4873196482658386}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26824 < 29024; dropping {'train/loss': 0.4379151463508606}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26825 < 29024; dropping {'train/loss': 0.6094015836715698}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26826 < 29024; dropping {'train/loss': 0.49052390456199646}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26827 < 29024; dropping {'train/loss': 0.4306839108467102}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26828 < 29024; dropping {'train/loss': 0.34064555168151855}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26829 < 29024; dropping {'train/loss': 0.3464144766330719}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26830 < 29024; dropping {'train/loss': 0.45915454626083374}.\n",
      "Epoch 268 | Batch 30/100 | Loss 0.409167\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26831 < 29024; dropping {'train/loss': 0.5307235717773438}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26832 < 29024; dropping {'train/loss': 0.5140305161476135}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26833 < 29024; dropping {'train/loss': 0.37857508659362793}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26834 < 29024; dropping {'train/loss': 0.2679000496864319}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26835 < 29024; dropping {'train/loss': 0.35400527715682983}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26836 < 29024; dropping {'train/loss': 0.3149894177913666}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26837 < 29024; dropping {'train/loss': 0.22877201437950134}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26838 < 29024; dropping {'train/loss': 0.48137807846069336}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26839 < 29024; dropping {'train/loss': 0.46184617280960083}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26840 < 29024; dropping {'train/loss': 0.25021570920944214}.\n",
      "Epoch 268 | Batch 40/100 | Loss 0.401436\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26841 < 29024; dropping {'train/loss': 0.31265780329704285}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26842 < 29024; dropping {'train/loss': 0.3549959659576416}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26843 < 29024; dropping {'train/loss': 0.273086816072464}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26844 < 29024; dropping {'train/loss': 0.3369824290275574}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26845 < 29024; dropping {'train/loss': 0.3659423589706421}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26846 < 29024; dropping {'train/loss': 0.5219844579696655}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26847 < 29024; dropping {'train/loss': 0.32031959295272827}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26848 < 29024; dropping {'train/loss': 0.6064668297767639}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26849 < 29024; dropping {'train/loss': 0.694515585899353}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26850 < 29024; dropping {'train/loss': 0.42939653992652893}.\n",
      "Epoch 268 | Batch 50/100 | Loss 0.405476\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26851 < 29024; dropping {'train/loss': 0.3412918746471405}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26852 < 29024; dropping {'train/loss': 0.32437723875045776}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26853 < 29024; dropping {'train/loss': 0.3313172161579132}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26854 < 29024; dropping {'train/loss': 0.47209101915359497}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26855 < 29024; dropping {'train/loss': 0.24380925297737122}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26856 < 29024; dropping {'train/loss': 0.4601626396179199}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26857 < 29024; dropping {'train/loss': 0.35560956597328186}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26858 < 29024; dropping {'train/loss': 0.24422402679920197}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26859 < 29024; dropping {'train/loss': 0.5494813323020935}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26860 < 29024; dropping {'train/loss': 0.19945046305656433}.\n",
      "Epoch 268 | Batch 60/100 | Loss 0.396593\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26861 < 29024; dropping {'train/loss': 0.22503098845481873}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26862 < 29024; dropping {'train/loss': 0.24005413055419922}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26863 < 29024; dropping {'train/loss': 0.46763595938682556}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26864 < 29024; dropping {'train/loss': 0.12724271416664124}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26865 < 29024; dropping {'train/loss': 0.24368011951446533}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26866 < 29024; dropping {'train/loss': 0.4352894723415375}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26867 < 29024; dropping {'train/loss': 0.4750756621360779}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26868 < 29024; dropping {'train/loss': 0.25545182824134827}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26869 < 29024; dropping {'train/loss': 0.39063969254493713}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26870 < 29024; dropping {'train/loss': 0.1239573135972023}.\n",
      "Epoch 268 | Batch 70/100 | Loss 0.382566\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26871 < 29024; dropping {'train/loss': 0.18139664828777313}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26872 < 29024; dropping {'train/loss': 0.3049713373184204}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26873 < 29024; dropping {'train/loss': 0.3201553225517273}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26874 < 29024; dropping {'train/loss': 0.34147965908050537}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26875 < 29024; dropping {'train/loss': 0.3740367293357849}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26876 < 29024; dropping {'train/loss': 0.28399041295051575}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26877 < 29024; dropping {'train/loss': 0.370769739151001}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26878 < 29024; dropping {'train/loss': 0.5112127661705017}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26879 < 29024; dropping {'train/loss': 0.19983914494514465}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26880 < 29024; dropping {'train/loss': 0.5423487424850464}.\n",
      "Epoch 268 | Batch 80/100 | Loss 0.377623\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26881 < 29024; dropping {'train/loss': 0.3800790309906006}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26882 < 29024; dropping {'train/loss': 0.515008807182312}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26883 < 29024; dropping {'train/loss': 0.5605341196060181}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26884 < 29024; dropping {'train/loss': 0.4200403094291687}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26885 < 29024; dropping {'train/loss': 0.26800045371055603}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26886 < 29024; dropping {'train/loss': 0.4451534152030945}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26887 < 29024; dropping {'train/loss': 0.28601372241973877}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26888 < 29024; dropping {'train/loss': 0.3138977885246277}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26889 < 29024; dropping {'train/loss': 0.19285514950752258}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26890 < 29024; dropping {'train/loss': 0.7703739404678345}.\n",
      "Epoch 268 | Batch 90/100 | Loss 0.381798\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26891 < 29024; dropping {'train/loss': 0.5806509256362915}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26892 < 29024; dropping {'train/loss': 0.5358575582504272}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26893 < 29024; dropping {'train/loss': 0.2837589383125305}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26894 < 29024; dropping {'train/loss': 0.2508932948112488}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26895 < 29024; dropping {'train/loss': 0.4164653420448303}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26896 < 29024; dropping {'train/loss': 0.4892069399356842}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26897 < 29024; dropping {'train/loss': 0.5304943919181824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26898 < 29024; dropping {'train/loss': 0.30419662594795227}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26899 < 29024; dropping {'train/loss': 0.1409183293581009}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26900 < 29024; dropping {'train/loss': 0.42056339979171753}.\n",
      "Epoch 268 | Batch 100/100 | Loss 0.383148\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26901 < 29024; dropping {'train/loss': 0.2599775195121765}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26902 < 29024; dropping {'train/loss': 0.5311201810836792}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26903 < 29024; dropping {'train/loss': 0.3722063899040222}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26904 < 29024; dropping {'train/loss': 0.2872719168663025}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26905 < 29024; dropping {'train/loss': 0.3590337634086609}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26906 < 29024; dropping {'train/loss': 0.7340748310089111}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26907 < 29024; dropping {'train/loss': 0.3051418364048004}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26908 < 29024; dropping {'train/loss': 0.2844313979148865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26909 < 29024; dropping {'train/loss': 0.3958415389060974}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26910 < 29024; dropping {'train/loss': 0.3966082036495209}.\n",
      "Epoch 269 | Batch 10/100 | Loss 0.392571\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26911 < 29024; dropping {'train/loss': 0.19721761345863342}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26912 < 29024; dropping {'train/loss': 0.6592302322387695}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26913 < 29024; dropping {'train/loss': 0.6683136224746704}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26914 < 29024; dropping {'train/loss': 0.5814719796180725}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26915 < 29024; dropping {'train/loss': 0.3721948564052582}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26916 < 29024; dropping {'train/loss': 0.2209852933883667}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26917 < 29024; dropping {'train/loss': 0.3791368007659912}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26918 < 29024; dropping {'train/loss': 0.2697582244873047}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26919 < 29024; dropping {'train/loss': 0.3998280167579651}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26920 < 29024; dropping {'train/loss': 0.673413872718811}.\n",
      "Epoch 269 | Batch 20/100 | Loss 0.417363\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26921 < 29024; dropping {'train/loss': 0.35456281900405884}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26922 < 29024; dropping {'train/loss': 0.2984057664871216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26923 < 29024; dropping {'train/loss': 0.32802289724349976}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26924 < 29024; dropping {'train/loss': 0.251830518245697}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26925 < 29024; dropping {'train/loss': 0.6201596260070801}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26926 < 29024; dropping {'train/loss': 0.5131012201309204}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26927 < 29024; dropping {'train/loss': 0.3652208149433136}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26928 < 29024; dropping {'train/loss': 0.4919578433036804}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26929 < 29024; dropping {'train/loss': 0.2289220541715622}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26930 < 29024; dropping {'train/loss': 0.33215591311454773}.\n",
      "Epoch 269 | Batch 30/100 | Loss 0.404387\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26931 < 29024; dropping {'train/loss': 0.4136638641357422}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26932 < 29024; dropping {'train/loss': 0.25304028391838074}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26933 < 29024; dropping {'train/loss': 0.1458374410867691}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26934 < 29024; dropping {'train/loss': 0.28302687406539917}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26935 < 29024; dropping {'train/loss': 0.3371858596801758}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26936 < 29024; dropping {'train/loss': 0.4537806510925293}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26937 < 29024; dropping {'train/loss': 0.5106459856033325}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26938 < 29024; dropping {'train/loss': 0.38557201623916626}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26939 < 29024; dropping {'train/loss': 0.3036440908908844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26940 < 29024; dropping {'train/loss': 0.6183800101280212}.\n",
      "Epoch 269 | Batch 40/100 | Loss 0.395909\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26941 < 29024; dropping {'train/loss': 0.4166225492954254}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26942 < 29024; dropping {'train/loss': 0.5550562739372253}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26943 < 29024; dropping {'train/loss': 0.2315749228000641}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26944 < 29024; dropping {'train/loss': 0.27270811796188354}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26945 < 29024; dropping {'train/loss': 0.38728973269462585}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26946 < 29024; dropping {'train/loss': 0.29604390263557434}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26947 < 29024; dropping {'train/loss': 0.46074071526527405}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26948 < 29024; dropping {'train/loss': 0.376625657081604}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26949 < 29024; dropping {'train/loss': 0.3780563175678253}.\n",
      "Epoch 269 | Batch 50/100 | Loss 0.389818\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26950 < 29024; dropping {'train/loss': 0.2798144221305847}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26951 < 29024; dropping {'train/loss': 0.29560568928718567}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26952 < 29024; dropping {'train/loss': 0.27436599135398865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26953 < 29024; dropping {'train/loss': 0.4466427266597748}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26954 < 29024; dropping {'train/loss': 0.2923370599746704}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26955 < 29024; dropping {'train/loss': 0.3624035716056824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26956 < 29024; dropping {'train/loss': 0.44560670852661133}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26957 < 29024; dropping {'train/loss': 0.3082502782344818}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26958 < 29024; dropping {'train/loss': 0.2507578432559967}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26959 < 29024; dropping {'train/loss': 0.37840795516967773}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26960 < 29024; dropping {'train/loss': 0.3231309950351715}.\n",
      "Epoch 269 | Batch 60/100 | Loss 0.381140\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26961 < 29024; dropping {'train/loss': 0.37383660674095154}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26962 < 29024; dropping {'train/loss': 0.2187637984752655}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26963 < 29024; dropping {'train/loss': 0.32423877716064453}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26964 < 29024; dropping {'train/loss': 0.23175528645515442}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26965 < 29024; dropping {'train/loss': 0.6553986668586731}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26966 < 29024; dropping {'train/loss': 0.44509249925613403}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26967 < 29024; dropping {'train/loss': 0.29292526841163635}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26968 < 29024; dropping {'train/loss': 0.69367516040802}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26969 < 29024; dropping {'train/loss': 0.3608887791633606}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26970 < 29024; dropping {'train/loss': 0.3502795100212097}.\n",
      "Epoch 269 | Batch 70/100 | Loss 0.383075\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26971 < 29024; dropping {'train/loss': 0.36885353922843933}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26972 < 29024; dropping {'train/loss': 0.21920862793922424}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26973 < 29024; dropping {'train/loss': 0.35296517610549927}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26974 < 29024; dropping {'train/loss': 0.536916196346283}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26975 < 29024; dropping {'train/loss': 0.5432993173599243}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26976 < 29024; dropping {'train/loss': 0.20630741119384766}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26977 < 29024; dropping {'train/loss': 0.39045780897140503}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26978 < 29024; dropping {'train/loss': 0.49020394682884216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26979 < 29024; dropping {'train/loss': 0.25402721762657166}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26980 < 29024; dropping {'train/loss': 0.377585232257843}.\n",
      "Epoch 269 | Batch 80/100 | Loss 0.381939\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26981 < 29024; dropping {'train/loss': 0.35292473435401917}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26982 < 29024; dropping {'train/loss': 0.2871030569076538}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26983 < 29024; dropping {'train/loss': 0.3801555037498474}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26984 < 29024; dropping {'train/loss': 0.2431301772594452}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26985 < 29024; dropping {'train/loss': 0.38914206624031067}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26986 < 29024; dropping {'train/loss': 0.3546028435230255}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26987 < 29024; dropping {'train/loss': 0.3249605596065521}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26988 < 29024; dropping {'train/loss': 0.16379103064537048}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26989 < 29024; dropping {'train/loss': 0.5177689790725708}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26990 < 29024; dropping {'train/loss': 0.49041181802749634}.\n",
      "Epoch 269 | Batch 90/100 | Loss 0.378434\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26991 < 29024; dropping {'train/loss': 0.3539567291736603}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26992 < 29024; dropping {'train/loss': 0.17445603013038635}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26993 < 29024; dropping {'train/loss': 0.1852133870124817}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26994 < 29024; dropping {'train/loss': 0.29686883091926575}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26995 < 29024; dropping {'train/loss': 0.32159513235092163}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26996 < 29024; dropping {'train/loss': 0.30970048904418945}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26997 < 29024; dropping {'train/loss': 0.31699785590171814}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26998 < 29024; dropping {'train/loss': 0.6891776323318481}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 26999 < 29024; dropping {'train/loss': 0.5469721555709839}.\n",
      "Epoch 269 | Batch 100/100 | Loss 0.376422wandb: WARNING Step must only increase in log calls.  Step 27000 < 29024; dropping {'train/loss': 0.3881465494632721}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27001 < 29024; dropping {'train/loss': 0.20944495499134064}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27002 < 29024; dropping {'train/loss': 0.4103659987449646}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27003 < 29024; dropping {'train/loss': 0.27304571866989136}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27004 < 29024; dropping {'train/loss': 0.3551023602485657}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27005 < 29024; dropping {'train/loss': 0.3663386404514313}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27006 < 29024; dropping {'train/loss': 0.4625461995601654}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27007 < 29024; dropping {'train/loss': 0.14391706883907318}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27008 < 29024; dropping {'train/loss': 0.302589476108551}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27009 < 29024; dropping {'train/loss': 0.19589713215827942}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27010 < 29024; dropping {'train/loss': 0.5342317819595337}.\n",
      "Epoch 270 | Batch 10/100 | Loss 0.325348\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27011 < 29024; dropping {'train/loss': 0.4406830668449402}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27012 < 29024; dropping {'train/loss': 0.3968947231769562}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27013 < 29024; dropping {'train/loss': 0.27118775248527527}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27014 < 29024; dropping {'train/loss': 0.5700065493583679}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27015 < 29024; dropping {'train/loss': 0.5240421295166016}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27016 < 29024; dropping {'train/loss': 0.4631100296974182}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27017 < 29024; dropping {'train/loss': 0.4475417137145996}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27018 < 29024; dropping {'train/loss': 0.8841923475265503}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27019 < 29024; dropping {'train/loss': 0.4601050913333893}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27020 < 29024; dropping {'train/loss': 0.23477542400360107}.\n",
      "Epoch 270 | Batch 20/100 | Loss 0.397301\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27021 < 29024; dropping {'train/loss': 0.5126977562904358}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27022 < 29024; dropping {'train/loss': 0.6631217002868652}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27023 < 29024; dropping {'train/loss': 0.3320923447608948}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27024 < 29024; dropping {'train/loss': 0.29725712537765503}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27025 < 29024; dropping {'train/loss': 0.35611170530319214}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27026 < 29024; dropping {'train/loss': 0.3580837845802307}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27027 < 29024; dropping {'train/loss': 0.4333225190639496}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27028 < 29024; dropping {'train/loss': 0.41043704748153687}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27029 < 29024; dropping {'train/loss': 0.20653478801250458}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27030 < 29024; dropping {'train/loss': 0.33512431383132935}.\n",
      "Epoch 270 | Batch 30/100 | Loss 0.395027\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27031 < 29024; dropping {'train/loss': 0.4661639332771301}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27032 < 29024; dropping {'train/loss': 0.350002646446228}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27033 < 29024; dropping {'train/loss': 0.4563133120536804}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27034 < 29024; dropping {'train/loss': 0.24925673007965088}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27035 < 29024; dropping {'train/loss': 0.6729158163070679}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27036 < 29024; dropping {'train/loss': 0.2986067831516266}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27037 < 29024; dropping {'train/loss': 0.288910448551178}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27038 < 29024; dropping {'train/loss': 0.2912152111530304}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27039 < 29024; dropping {'train/loss': 0.248521089553833}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27040 < 29024; dropping {'train/loss': 0.47636422514915466}.\n",
      "Epoch 270 | Batch 40/100 | Loss 0.391227\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27041 < 29024; dropping {'train/loss': 0.44964319467544556}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27042 < 29024; dropping {'train/loss': 0.314851313829422}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27043 < 29024; dropping {'train/loss': 0.38268446922302246}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27044 < 29024; dropping {'train/loss': 0.32951587438583374}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27045 < 29024; dropping {'train/loss': 0.7989823818206787}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27046 < 29024; dropping {'train/loss': 0.4277063012123108}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27047 < 29024; dropping {'train/loss': 0.22379271686077118}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27048 < 29024; dropping {'train/loss': 0.18164685368537903}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27049 < 29024; dropping {'train/loss': 0.3590606153011322}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27050 < 29024; dropping {'train/loss': 0.16830788552761078}.\n",
      "Epoch 270 | Batch 50/100 | Loss 0.385705\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27051 < 29024; dropping {'train/loss': 0.3232477307319641}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27052 < 29024; dropping {'train/loss': 0.26826372742652893}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27053 < 29024; dropping {'train/loss': 0.5568537712097168}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27054 < 29024; dropping {'train/loss': 0.48086676001548767}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27055 < 29024; dropping {'train/loss': 0.4501166343688965}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27056 < 29024; dropping {'train/loss': 0.36391139030456543}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27057 < 29024; dropping {'train/loss': 0.2635933756828308}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27058 < 29024; dropping {'train/loss': 0.3232671618461609}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27059 < 29024; dropping {'train/loss': 0.4092872142791748}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27060 < 29024; dropping {'train/loss': 0.3624650239944458}.\n",
      "Epoch 270 | Batch 60/100 | Loss 0.384786\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27061 < 29024; dropping {'train/loss': 0.42286959290504456}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27062 < 29024; dropping {'train/loss': 0.6372724175453186}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27063 < 29024; dropping {'train/loss': 0.5535525679588318}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27064 < 29024; dropping {'train/loss': 0.45124268531799316}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27065 < 29024; dropping {'train/loss': 0.44448980689048767}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27066 < 29024; dropping {'train/loss': 0.6272751688957214}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27067 < 29024; dropping {'train/loss': 0.49405771493911743}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27068 < 29024; dropping {'train/loss': 0.1444418728351593}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27069 < 29024; dropping {'train/loss': 0.40836524963378906}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27070 < 29024; dropping {'train/loss': 0.26239779591560364}.\n",
      "Epoch 270 | Batch 70/100 | Loss 0.393330\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27071 < 29024; dropping {'train/loss': 0.0715809017419815}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27072 < 29024; dropping {'train/loss': 0.41557732224464417}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27073 < 29024; dropping {'train/loss': 0.4470239281654358}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27074 < 29024; dropping {'train/loss': 0.3810722231864929}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27075 < 29024; dropping {'train/loss': 0.30298611521720886}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27076 < 29024; dropping {'train/loss': 0.47934508323669434}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27077 < 29024; dropping {'train/loss': 0.29720163345336914}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27078 < 29024; dropping {'train/loss': 0.5744500160217285}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27079 < 29024; dropping {'train/loss': 0.3400164246559143}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27080 < 29024; dropping {'train/loss': 0.19831202924251556}.\n",
      "Epoch 270 | Batch 80/100 | Loss 0.388008\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27081 < 29024; dropping {'train/loss': 0.5172208547592163}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27082 < 29024; dropping {'train/loss': 0.38199707865715027}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27083 < 29024; dropping {'train/loss': 0.24512150883674622}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27084 < 29024; dropping {'train/loss': 0.5014787316322327}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27085 < 29024; dropping {'train/loss': 0.23710057139396667}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27086 < 29024; dropping {'train/loss': 0.40071386098861694}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27087 < 29024; dropping {'train/loss': 0.5285282731056213}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27088 < 29024; dropping {'train/loss': 0.5102052688598633}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27089 < 29024; dropping {'train/loss': 0.45836037397384644}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27090 < 29024; dropping {'train/loss': 0.38278594613075256}.\n",
      "Epoch 270 | Batch 90/100 | Loss 0.391158\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27091 < 29024; dropping {'train/loss': 0.24353334307670593}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27092 < 29024; dropping {'train/loss': 0.2512096166610718}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27093 < 29024; dropping {'train/loss': 0.3226442039012909}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27094 < 29024; dropping {'train/loss': 0.3958091735839844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27095 < 29024; dropping {'train/loss': 0.4009149670600891}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27096 < 29024; dropping {'train/loss': 0.27299100160598755}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27097 < 29024; dropping {'train/loss': 0.36407381296157837}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27098 < 29024; dropping {'train/loss': 0.1346445083618164}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27099 < 29024; dropping {'train/loss': 0.46451839804649353}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27100 < 29024; dropping {'train/loss': 0.23972666263580322}.\n",
      "Epoch 270 | Batch 100/100 | Loss 0.382942\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27101 < 29024; dropping {'train/loss': 0.20204734802246094}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27102 < 29024; dropping {'train/loss': 0.44540053606033325}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27103 < 29024; dropping {'train/loss': 0.44511526823043823}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27104 < 29024; dropping {'train/loss': 0.49265679717063904}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27105 < 29024; dropping {'train/loss': 0.36918073892593384}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27106 < 29024; dropping {'train/loss': 0.3810293972492218}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27107 < 29024; dropping {'train/loss': 0.6506584882736206}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27108 < 29024; dropping {'train/loss': 0.2883186936378479}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27109 < 29024; dropping {'train/loss': 0.4163273274898529}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27110 < 29024; dropping {'train/loss': 0.33294278383255005}.\n",
      "Epoch 271 | Batch 10/100 | Loss 0.402368\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27111 < 29024; dropping {'train/loss': 0.3606037497520447}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27112 < 29024; dropping {'train/loss': 0.3626306653022766}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27113 < 29024; dropping {'train/loss': 0.435391902923584}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27114 < 29024; dropping {'train/loss': 0.32797104120254517}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27115 < 29024; dropping {'train/loss': 0.32324591279029846}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27116 < 29024; dropping {'train/loss': 0.6399048566818237}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27117 < 29024; dropping {'train/loss': 0.46001043915748596}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27118 < 29024; dropping {'train/loss': 0.4820200800895691}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27119 < 29024; dropping {'train/loss': 0.48477140069007874}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27120 < 29024; dropping {'train/loss': 0.5386146306991577}.\n",
      "Epoch 271 | Batch 20/100 | Loss 0.421942\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27121 < 29024; dropping {'train/loss': 0.12781181931495667}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27122 < 29024; dropping {'train/loss': 0.3726212680339813}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27123 < 29024; dropping {'train/loss': 0.20996356010437012}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27124 < 29024; dropping {'train/loss': 0.38185951113700867}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27125 < 29024; dropping {'train/loss': 0.21629111468791962}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27126 < 29024; dropping {'train/loss': 0.5744327902793884}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27127 < 29024; dropping {'train/loss': 0.6458919048309326}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27128 < 29024; dropping {'train/loss': 0.2600114047527313}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27129 < 29024; dropping {'train/loss': 0.18750473856925964}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27130 < 29024; dropping {'train/loss': 0.39267951250076294}.\n",
      "Epoch 271 | Batch 30/100 | Loss 0.393597\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27131 < 29024; dropping {'train/loss': 0.3955415189266205}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27132 < 29024; dropping {'train/loss': 0.43267393112182617}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27133 < 29024; dropping {'train/loss': 0.24670979380607605}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27134 < 29024; dropping {'train/loss': 0.12969937920570374}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27135 < 29024; dropping {'train/loss': 0.19425469636917114}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27136 < 29024; dropping {'train/loss': 0.11485645920038223}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27137 < 29024; dropping {'train/loss': 0.41058748960494995}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27138 < 29024; dropping {'train/loss': 0.3897463381290436}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27139 < 29024; dropping {'train/loss': 0.16745518147945404}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27140 < 29024; dropping {'train/loss': 0.35862770676612854}.\n",
      "Epoch 271 | Batch 40/100 | Loss 0.366202\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27141 < 29024; dropping {'train/loss': 0.3109428882598877}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27142 < 29024; dropping {'train/loss': 0.45388054847717285}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27143 < 29024; dropping {'train/loss': 0.42532700300216675}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27144 < 29024; dropping {'train/loss': 0.33743447065353394}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27145 < 29024; dropping {'train/loss': 0.39647072553634644}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27146 < 29024; dropping {'train/loss': 0.5789815783500671}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27147 < 29024; dropping {'train/loss': 0.3171320855617523}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27148 < 29024; dropping {'train/loss': 0.22663044929504395}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27149 < 29024; dropping {'train/loss': 0.41055575013160706}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27150 < 29024; dropping {'train/loss': 0.3437703251838684}.\n",
      "Epoch 271 | Batch 50/100 | Loss 0.368984\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27151 < 29024; dropping {'train/loss': 0.44961580634117126}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27152 < 29024; dropping {'train/loss': 0.3449239134788513}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27153 < 29024; dropping {'train/loss': 0.2646220028400421}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27154 < 29024; dropping {'train/loss': 0.20166857540607452}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27155 < 29024; dropping {'train/loss': 0.22967520356178284}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27156 < 29024; dropping {'train/loss': 0.27804940938949585}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27157 < 29024; dropping {'train/loss': 0.5054718852043152}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27158 < 29024; dropping {'train/loss': 0.4333893656730652}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27159 < 29024; dropping {'train/loss': 0.2587229013442993}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27160 < 29024; dropping {'train/loss': 0.5656293034553528}.\n",
      "Epoch 271 | Batch 60/100 | Loss 0.366349\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27161 < 29024; dropping {'train/loss': 0.42477425932884216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27162 < 29024; dropping {'train/loss': 0.3919568359851837}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27163 < 29024; dropping {'train/loss': 0.19136928021907806}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27164 < 29024; dropping {'train/loss': 0.4836169183254242}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27165 < 29024; dropping {'train/loss': 0.20539160072803497}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27166 < 29024; dropping {'train/loss': 0.378719687461853}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27167 < 29024; dropping {'train/loss': 0.38198113441467285}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27168 < 29024; dropping {'train/loss': 0.27778345346450806}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27169 < 29024; dropping {'train/loss': 0.2701733708381653}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27170 < 29024; dropping {'train/loss': 0.2314089834690094}.\n",
      "Epoch 271 | Batch 70/100 | Loss 0.360259\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27171 < 29024; dropping {'train/loss': 0.4603598117828369}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27172 < 29024; dropping {'train/loss': 0.45477357506752014}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27173 < 29024; dropping {'train/loss': 0.4613701403141022}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27174 < 29024; dropping {'train/loss': 0.19941578805446625}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27175 < 29024; dropping {'train/loss': 0.5481556057929993}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27176 < 29024; dropping {'train/loss': 0.12190748751163483}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27177 < 29024; dropping {'train/loss': 0.31109604239463806}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27178 < 29024; dropping {'train/loss': 0.34016790986061096}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27179 < 29024; dropping {'train/loss': 0.36647242307662964}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27180 < 29024; dropping {'train/loss': 0.32729989290237427}.\n",
      "Epoch 271 | Batch 80/100 | Loss 0.360114\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27181 < 29024; dropping {'train/loss': 0.4736531674861908}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27182 < 29024; dropping {'train/loss': 0.4038143754005432}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27183 < 29024; dropping {'train/loss': 0.5006235241889954}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27184 < 29024; dropping {'train/loss': 0.2729952037334442}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27185 < 29024; dropping {'train/loss': 0.28853264451026917}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27186 < 29024; dropping {'train/loss': 0.3141600489616394}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27187 < 29024; dropping {'train/loss': 0.19357839226722717}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27188 < 29024; dropping {'train/loss': 0.504180371761322}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27189 < 29024; dropping {'train/loss': 0.48854175209999084}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27190 < 29024; dropping {'train/loss': 0.36836329102516174}.\n",
      "Epoch 271 | Batch 90/100 | Loss 0.362418\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27191 < 29024; dropping {'train/loss': 0.44219961762428284}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27192 < 29024; dropping {'train/loss': 0.4918425679206848}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27193 < 29024; dropping {'train/loss': 0.25510892271995544}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27194 < 29024; dropping {'train/loss': 0.36021706461906433}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27195 < 29024; dropping {'train/loss': 0.3664051294326782}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27196 < 29024; dropping {'train/loss': 0.2995288074016571}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27197 < 29024; dropping {'train/loss': 0.2091241180896759}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27198 < 29024; dropping {'train/loss': 0.3265440762042999}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27199 < 29024; dropping {'train/loss': 0.3538661003112793}.\n",
      "Epoch 271 | Batch 100/100 | Loss 0.364035wandb: WARNING Step must only increase in log calls.  Step 27200 < 29024; dropping {'train/loss': 0.681102454662323}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27201 < 29024; dropping {'train/loss': 0.555449366569519}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27202 < 29024; dropping {'train/loss': 0.3294130861759186}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27203 < 29024; dropping {'train/loss': 0.5879282355308533}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27204 < 29024; dropping {'train/loss': 0.43143177032470703}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27205 < 29024; dropping {'train/loss': 0.2403099238872528}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27206 < 29024; dropping {'train/loss': 0.3212302327156067}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27207 < 29024; dropping {'train/loss': 0.4851054251194}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27208 < 29024; dropping {'train/loss': 0.4700142741203308}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27209 < 29024; dropping {'train/loss': 0.39360612630844116}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27210 < 29024; dropping {'train/loss': 0.27640393376350403}.\n",
      "Epoch 272 | Batch 10/100 | Loss 0.409089\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27211 < 29024; dropping {'train/loss': 0.23377437889575958}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27212 < 29024; dropping {'train/loss': 0.14752236008644104}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27213 < 29024; dropping {'train/loss': 0.2560439109802246}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27214 < 29024; dropping {'train/loss': 0.3659510016441345}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27215 < 29024; dropping {'train/loss': 0.3305363059043884}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27216 < 29024; dropping {'train/loss': 0.2740243971347809}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27217 < 29024; dropping {'train/loss': 0.4307418763637543}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27218 < 29024; dropping {'train/loss': 0.17108793556690216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27219 < 29024; dropping {'train/loss': 0.2783229351043701}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27220 < 29024; dropping {'train/loss': 0.3537132143974304}.\n",
      "Epoch 272 | Batch 20/100 | Loss 0.346631\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27221 < 29024; dropping {'train/loss': 0.33538031578063965}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27222 < 29024; dropping {'train/loss': 0.2871137857437134}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27223 < 29024; dropping {'train/loss': 0.38382792472839355}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27224 < 29024; dropping {'train/loss': 0.49240532517433167}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27225 < 29024; dropping {'train/loss': 0.20219150185585022}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27226 < 29024; dropping {'train/loss': 0.25254860520362854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27227 < 29024; dropping {'train/loss': 0.4168437123298645}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27228 < 29024; dropping {'train/loss': 0.354441374540329}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27229 < 29024; dropping {'train/loss': 0.4112464487552643}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27230 < 29024; dropping {'train/loss': 0.4193567633628845}.\n",
      "Epoch 272 | Batch 30/100 | Loss 0.349599\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27231 < 29024; dropping {'train/loss': 0.700853705406189}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27232 < 29024; dropping {'train/loss': 0.25640326738357544}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27233 < 29024; dropping {'train/loss': 0.1862124502658844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27234 < 29024; dropping {'train/loss': 0.2858247756958008}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27235 < 29024; dropping {'train/loss': 0.31418102979660034}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27236 < 29024; dropping {'train/loss': 0.5682370066642761}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27237 < 29024; dropping {'train/loss': 0.4813733696937561}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27238 < 29024; dropping {'train/loss': 0.5354670286178589}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27239 < 29024; dropping {'train/loss': 0.39371997117996216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27240 < 29024; dropping {'train/loss': 0.3286333680152893}.\n",
      "Epoch 272 | Batch 40/100 | Loss 0.363472\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27241 < 29024; dropping {'train/loss': 0.27669352293014526}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27242 < 29024; dropping {'train/loss': 0.20738422870635986}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27243 < 29024; dropping {'train/loss': 0.3734126091003418}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27244 < 29024; dropping {'train/loss': 0.31772759556770325}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27245 < 29024; dropping {'train/loss': 0.6332722902297974}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27246 < 29024; dropping {'train/loss': 0.3185496926307678}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27247 < 29024; dropping {'train/loss': 0.25621524453163147}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27248 < 29024; dropping {'train/loss': 0.39426594972610474}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27249 < 29024; dropping {'train/loss': 0.13852839171886444}.\n",
      "Epoch 272 | Batch 50/100 | Loss 0.359823wandb: WARNING Step must only increase in log calls.  Step 27250 < 29024; dropping {'train/loss': 0.5362118482589722}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27251 < 29024; dropping {'train/loss': 0.2747330367565155}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27252 < 29024; dropping {'train/loss': 0.7454642653465271}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27253 < 29024; dropping {'train/loss': 0.25002816319465637}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27254 < 29024; dropping {'train/loss': 0.2928324043750763}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27255 < 29024; dropping {'train/loss': 0.2714976668357849}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27256 < 29024; dropping {'train/loss': 0.5182932019233704}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27257 < 29024; dropping {'train/loss': 0.31510716676712036}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27258 < 29024; dropping {'train/loss': 0.5472301244735718}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27259 < 29024; dropping {'train/loss': 0.4821135997772217}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27260 < 29024; dropping {'train/loss': 0.4581453204154968}.\n",
      "Epoch 272 | Batch 60/100 | Loss 0.369110\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27261 < 29024; dropping {'train/loss': 0.3401631712913513}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27262 < 29024; dropping {'train/loss': 0.7453053593635559}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27263 < 29024; dropping {'train/loss': 0.38569697737693787}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27264 < 29024; dropping {'train/loss': 0.2676095962524414}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27265 < 29024; dropping {'train/loss': 0.5446651577949524}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27266 < 29024; dropping {'train/loss': 0.20772376656532288}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27267 < 29024; dropping {'train/loss': 0.17202317714691162}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27268 < 29024; dropping {'train/loss': 0.3771180212497711}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27269 < 29024; dropping {'train/loss': 0.29927927255630493}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27270 < 29024; dropping {'train/loss': 0.20019030570983887}.\n",
      "Epoch 272 | Batch 70/100 | Loss 0.366948\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27271 < 29024; dropping {'train/loss': 0.24963876605033875}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27272 < 29024; dropping {'train/loss': 0.30759984254837036}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27273 < 29024; dropping {'train/loss': 0.3304198384284973}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27274 < 29024; dropping {'train/loss': 0.19645357131958008}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27275 < 29024; dropping {'train/loss': 0.28773921728134155}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27276 < 29024; dropping {'train/loss': 0.5094023942947388}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27277 < 29024; dropping {'train/loss': 0.2276889532804489}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27278 < 29024; dropping {'train/loss': 0.346691370010376}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27279 < 29024; dropping {'train/loss': 0.3231203258037567}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27280 < 29024; dropping {'train/loss': 0.3933878540992737}.\n",
      "Epoch 272 | Batch 80/100 | Loss 0.360731\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27281 < 29024; dropping {'train/loss': 0.24592694640159607}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27282 < 29024; dropping {'train/loss': 0.4888896346092224}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27283 < 29024; dropping {'train/loss': 0.531607985496521}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27284 < 29024; dropping {'train/loss': 0.37625783681869507}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27285 < 29024; dropping {'train/loss': 0.26260143518447876}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27286 < 29024; dropping {'train/loss': 0.25813019275665283}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27287 < 29024; dropping {'train/loss': 0.5059653520584106}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27288 < 29024; dropping {'train/loss': 0.5734173655509949}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27289 < 29024; dropping {'train/loss': 0.14771413803100586}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27290 < 29024; dropping {'train/loss': 0.23507240414619446}.\n",
      "Epoch 272 | Batch 90/100 | Loss 0.360934\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27291 < 29024; dropping {'train/loss': 0.32657209038734436}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27292 < 29024; dropping {'train/loss': 0.36068665981292725}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27293 < 29024; dropping {'train/loss': 0.23129022121429443}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27294 < 29024; dropping {'train/loss': 0.5667230486869812}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27295 < 29024; dropping {'train/loss': 0.3054584264755249}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27296 < 29024; dropping {'train/loss': 0.12855853140354156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27297 < 29024; dropping {'train/loss': 0.42942509055137634}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27298 < 29024; dropping {'train/loss': 0.33609074354171753}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27299 < 29024; dropping {'train/loss': 0.7990986108779907}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27300 < 29024; dropping {'train/loss': 0.326066792011261}.\n",
      "Epoch 272 | Batch 100/100 | Loss 0.362940\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27301 < 29024; dropping {'train/loss': 0.30685752630233765}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27302 < 29024; dropping {'train/loss': 0.4649692475795746}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27303 < 29024; dropping {'train/loss': 0.2123381346464157}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27304 < 29024; dropping {'train/loss': 0.22380796074867249}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27305 < 29024; dropping {'train/loss': 0.35144323110580444}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27306 < 29024; dropping {'train/loss': 0.24723689258098602}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27307 < 29024; dropping {'train/loss': 0.2307123839855194}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27308 < 29024; dropping {'train/loss': 0.3857658803462982}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27309 < 29024; dropping {'train/loss': 0.3477320075035095}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27310 < 29024; dropping {'train/loss': 0.2517379820346832}.\n",
      "Epoch 273 | Batch 10/100 | Loss 0.302260\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27311 < 29024; dropping {'train/loss': 0.2543354332447052}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27312 < 29024; dropping {'train/loss': 0.49021393060684204}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27313 < 29024; dropping {'train/loss': 0.2669452130794525}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27314 < 29024; dropping {'train/loss': 0.508671224117279}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27315 < 29024; dropping {'train/loss': 0.28837329149246216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27316 < 29024; dropping {'train/loss': 0.31789135932922363}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27317 < 29024; dropping {'train/loss': 0.22456304728984833}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27318 < 29024; dropping {'train/loss': 0.4506004750728607}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27319 < 29024; dropping {'train/loss': 0.13156983256340027}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27320 < 29024; dropping {'train/loss': 0.42008286714553833}.\n",
      "Epoch 273 | Batch 20/100 | Loss 0.318792\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27321 < 29024; dropping {'train/loss': 0.25262120366096497}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27322 < 29024; dropping {'train/loss': 0.21756145358085632}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27323 < 29024; dropping {'train/loss': 0.48734816908836365}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27324 < 29024; dropping {'train/loss': 0.252175509929657}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27325 < 29024; dropping {'train/loss': 0.3438042998313904}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27326 < 29024; dropping {'train/loss': 0.44107526540756226}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27327 < 29024; dropping {'train/loss': 0.4467095732688904}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27328 < 29024; dropping {'train/loss': 0.31901633739471436}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27329 < 29024; dropping {'train/loss': 0.3417236804962158}.\n",
      "Epoch 273 | Batch 30/100 | Loss 0.329145wandb: WARNING Step must only increase in log calls.  Step 27330 < 29024; dropping {'train/loss': 0.39645496010780334}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27331 < 29024; dropping {'train/loss': 0.2175198793411255}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27332 < 29024; dropping {'train/loss': 0.39436909556388855}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27333 < 29024; dropping {'train/loss': 0.3194499611854553}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27334 < 29024; dropping {'train/loss': 0.31969326734542847}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27335 < 29024; dropping {'train/loss': 0.6580678224563599}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27336 < 29024; dropping {'train/loss': 0.36909306049346924}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27337 < 29024; dropping {'train/loss': 0.35869139432907104}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27338 < 29024; dropping {'train/loss': 0.6220837831497192}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27339 < 29024; dropping {'train/loss': 0.3623044192790985}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27340 < 29024; dropping {'train/loss': 0.2634572982788086}.\n",
      "Epoch 273 | Batch 40/100 | Loss 0.343977\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27341 < 29024; dropping {'train/loss': 0.6578259468078613}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27342 < 29024; dropping {'train/loss': 0.3590138554573059}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27343 < 29024; dropping {'train/loss': 0.4156476557254791}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27344 < 29024; dropping {'train/loss': 0.47340336441993713}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27345 < 29024; dropping {'train/loss': 0.46386608481407166}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27346 < 29024; dropping {'train/loss': 0.5525889992713928}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27347 < 29024; dropping {'train/loss': 0.5194203853607178}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27348 < 29024; dropping {'train/loss': 0.48697829246520996}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27349 < 29024; dropping {'train/loss': 0.25397664308547974}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27350 < 29024; dropping {'train/loss': 0.29960504174232483}.\n",
      "Epoch 273 | Batch 50/100 | Loss 0.364828\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27351 < 29024; dropping {'train/loss': 0.4683868885040283}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27352 < 29024; dropping {'train/loss': 0.1989469826221466}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27353 < 29024; dropping {'train/loss': 0.2240583598613739}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27354 < 29024; dropping {'train/loss': 0.3379534184932709}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27355 < 29024; dropping {'train/loss': 0.4031616747379303}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27356 < 29024; dropping {'train/loss': 0.46853798627853394}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27357 < 29024; dropping {'train/loss': 0.3569251596927643}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27358 < 29024; dropping {'train/loss': 0.28593069314956665}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27359 < 29024; dropping {'train/loss': 0.3272019028663635}.\n",
      "Epoch 273 | Batch 60/100 | Loss 0.361493wandb: WARNING Step must only increase in log calls.  Step 27360 < 29024; dropping {'train/loss': 0.3770635724067688}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27361 < 29024; dropping {'train/loss': 0.546039342880249}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27362 < 29024; dropping {'train/loss': 0.24839964509010315}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27363 < 29024; dropping {'train/loss': 0.30136388540267944}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27364 < 29024; dropping {'train/loss': 0.23289485275745392}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27365 < 29024; dropping {'train/loss': 0.3744646906852722}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27366 < 29024; dropping {'train/loss': 0.5245863795280457}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27367 < 29024; dropping {'train/loss': 0.5296907424926758}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27368 < 29024; dropping {'train/loss': 0.27775493264198303}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27369 < 29024; dropping {'train/loss': 0.21213476359844208}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27370 < 29024; dropping {'train/loss': 0.5235228538513184}.\n",
      "Epoch 273 | Batch 70/100 | Loss 0.363720\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27371 < 29024; dropping {'train/loss': 0.4061421751976013}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27372 < 29024; dropping {'train/loss': 0.31289201974868774}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27373 < 29024; dropping {'train/loss': 0.2114095389842987}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27374 < 29024; dropping {'train/loss': 0.4962213635444641}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27375 < 29024; dropping {'train/loss': 0.2302529513835907}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27376 < 29024; dropping {'train/loss': 0.31951043009757996}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27377 < 29024; dropping {'train/loss': 0.45249804854393005}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27378 < 29024; dropping {'train/loss': 0.5190849304199219}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27379 < 29024; dropping {'train/loss': 0.36427539587020874}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27380 < 29024; dropping {'train/loss': 0.5564785599708557}.\n",
      "Epoch 273 | Batch 80/100 | Loss 0.366615\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27381 < 29024; dropping {'train/loss': 0.37007349729537964}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27382 < 29024; dropping {'train/loss': 0.4376453757286072}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27383 < 29024; dropping {'train/loss': 0.6453723907470703}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27384 < 29024; dropping {'train/loss': 0.32053250074386597}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27385 < 29024; dropping {'train/loss': 0.5303860902786255}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27386 < 29024; dropping {'train/loss': 0.23534953594207764}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27387 < 29024; dropping {'train/loss': 0.38711661100387573}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27388 < 29024; dropping {'train/loss': 0.31287088990211487}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27389 < 29024; dropping {'train/loss': 0.8474749326705933}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27390 < 29024; dropping {'train/loss': 0.3236684203147888}.\n",
      "Epoch 273 | Batch 90/100 | Loss 0.374885\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27391 < 29024; dropping {'train/loss': 0.4033361077308655}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27392 < 29024; dropping {'train/loss': 0.21061702072620392}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27393 < 29024; dropping {'train/loss': 0.5201675295829773}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27394 < 29024; dropping {'train/loss': 0.2945672869682312}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27395 < 29024; dropping {'train/loss': 0.5219324231147766}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27396 < 29024; dropping {'train/loss': 0.45928773283958435}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27397 < 29024; dropping {'train/loss': 0.39039191603660583}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27398 < 29024; dropping {'train/loss': 0.3455183207988739}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27399 < 29024; dropping {'train/loss': 0.392014741897583}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27400 < 29024; dropping {'train/loss': 0.504907488822937}.\n",
      "Epoch 273 | Batch 100/100 | Loss 0.377824\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27401 < 29024; dropping {'train/loss': 0.2316843718290329}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27402 < 29024; dropping {'train/loss': 0.2663467526435852}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27403 < 29024; dropping {'train/loss': 0.36770838499069214}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27404 < 29024; dropping {'train/loss': 0.4689003527164459}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27405 < 29024; dropping {'train/loss': 0.4935320019721985}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27406 < 29024; dropping {'train/loss': 0.23397740721702576}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27407 < 29024; dropping {'train/loss': 0.20758108794689178}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27408 < 29024; dropping {'train/loss': 0.17258203029632568}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27409 < 29024; dropping {'train/loss': 0.37542152404785156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27410 < 29024; dropping {'train/loss': 0.24142324924468994}.\n",
      "Epoch 274 | Batch 10/100 | Loss 0.305916\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27411 < 29024; dropping {'train/loss': 0.304389625787735}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27412 < 29024; dropping {'train/loss': 0.4378390312194824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27413 < 29024; dropping {'train/loss': 0.3534042537212372}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27414 < 29024; dropping {'train/loss': 0.6142041087150574}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27415 < 29024; dropping {'train/loss': 0.3639408349990845}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27416 < 29024; dropping {'train/loss': 0.42710357904434204}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27417 < 29024; dropping {'train/loss': 0.3658815026283264}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27418 < 29024; dropping {'train/loss': 0.45113301277160645}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27419 < 29024; dropping {'train/loss': 0.17275671660900116}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27420 < 29024; dropping {'train/loss': 0.4905102252960205}.\n",
      "Epoch 274 | Batch 20/100 | Loss 0.352016\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27421 < 29024; dropping {'train/loss': 0.09213908761739731}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27422 < 29024; dropping {'train/loss': 0.21074314415454865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27423 < 29024; dropping {'train/loss': 0.6948421597480774}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27424 < 29024; dropping {'train/loss': 0.569222092628479}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27425 < 29024; dropping {'train/loss': 0.34222647547721863}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27426 < 29024; dropping {'train/loss': 0.40821346640586853}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27427 < 29024; dropping {'train/loss': 0.309133917093277}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27428 < 29024; dropping {'train/loss': 0.410053014755249}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27429 < 29024; dropping {'train/loss': 0.6082686185836792}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27430 < 29024; dropping {'train/loss': 0.3917464315891266}.\n",
      "Epoch 274 | Batch 30/100 | Loss 0.369230\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27431 < 29024; dropping {'train/loss': 0.19556675851345062}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27432 < 29024; dropping {'train/loss': 0.350832998752594}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27433 < 29024; dropping {'train/loss': 0.5270235538482666}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27434 < 29024; dropping {'train/loss': 0.49453258514404297}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27435 < 29024; dropping {'train/loss': 0.3727544844150543}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27436 < 29024; dropping {'train/loss': 0.20193973183631897}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27437 < 29024; dropping {'train/loss': 0.44982561469078064}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27438 < 29024; dropping {'train/loss': 0.34799233078956604}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27439 < 29024; dropping {'train/loss': 0.15539827942848206}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27440 < 29024; dropping {'train/loss': 0.5903359651565552}.\n",
      "Epoch 274 | Batch 40/100 | Loss 0.369078\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27441 < 29024; dropping {'train/loss': 0.44376134872436523}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27442 < 29024; dropping {'train/loss': 0.34161797165870667}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27443 < 29024; dropping {'train/loss': 0.41615113615989685}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27444 < 29024; dropping {'train/loss': 0.660702645778656}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27445 < 29024; dropping {'train/loss': 0.5284777879714966}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27446 < 29024; dropping {'train/loss': 0.2649911642074585}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27447 < 29024; dropping {'train/loss': 0.4920560419559479}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27448 < 29024; dropping {'train/loss': 0.26923075318336487}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27449 < 29024; dropping {'train/loss': 0.272769570350647}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27450 < 29024; dropping {'train/loss': 0.5966283082962036}.\n",
      "Epoch 274 | Batch 50/100 | Loss 0.380990\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27451 < 29024; dropping {'train/loss': 0.4034547209739685}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27452 < 29024; dropping {'train/loss': 0.23239748179912567}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27453 < 29024; dropping {'train/loss': 0.5492284893989563}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27454 < 29024; dropping {'train/loss': 0.3109905421733856}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27455 < 29024; dropping {'train/loss': 0.2810117304325104}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27456 < 29024; dropping {'train/loss': 0.6096621155738831}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27457 < 29024; dropping {'train/loss': 0.4371740221977234}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27458 < 29024; dropping {'train/loss': 0.4531835913658142}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27459 < 29024; dropping {'train/loss': 0.3767886459827423}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27460 < 29024; dropping {'train/loss': 0.27699339389801025}.\n",
      "Epoch 274 | Batch 60/100 | Loss 0.383006\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27461 < 29024; dropping {'train/loss': 0.3434562683105469}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27462 < 29024; dropping {'train/loss': 0.42538899183273315}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27463 < 29024; dropping {'train/loss': 0.18775717914104462}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27464 < 29024; dropping {'train/loss': 0.5340331792831421}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27465 < 29024; dropping {'train/loss': 0.17459134757518768}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27466 < 29024; dropping {'train/loss': 0.529636025428772}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27467 < 29024; dropping {'train/loss': 0.3401179909706116}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27468 < 29024; dropping {'train/loss': 0.30329495668411255}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27469 < 29024; dropping {'train/loss': 0.2647404074668884}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27470 < 29024; dropping {'train/loss': 0.520725429058075}.\n",
      "Epoch 274 | Batch 70/100 | Loss 0.380059\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27471 < 29024; dropping {'train/loss': 0.45601096749305725}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27472 < 29024; dropping {'train/loss': 0.26923492550849915}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27473 < 29024; dropping {'train/loss': 0.4957956373691559}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27474 < 29024; dropping {'train/loss': 0.37192219495773315}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27475 < 29024; dropping {'train/loss': 0.3612852692604065}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27476 < 29024; dropping {'train/loss': 0.29159966111183167}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27477 < 29024; dropping {'train/loss': 0.3286726474761963}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27478 < 29024; dropping {'train/loss': 0.36261457204818726}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27479 < 29024; dropping {'train/loss': 0.5573955774307251}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27480 < 29024; dropping {'train/loss': 0.24252888560295105}.\n",
      "Epoch 274 | Batch 80/100 | Loss 0.379265\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27481 < 29024; dropping {'train/loss': 0.33049482107162476}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27482 < 29024; dropping {'train/loss': 0.297453910112381}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27483 < 29024; dropping {'train/loss': 0.21970057487487793}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27484 < 29024; dropping {'train/loss': 0.3672233819961548}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27485 < 29024; dropping {'train/loss': 0.2740294337272644}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27486 < 29024; dropping {'train/loss': 0.24732422828674316}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27487 < 29024; dropping {'train/loss': 0.3047555983066559}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27488 < 29024; dropping {'train/loss': 0.0763765424489975}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27489 < 29024; dropping {'train/loss': 0.21414002776145935}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27490 < 29024; dropping {'train/loss': 0.25776416063308716}.\n",
      "Epoch 274 | Batch 90/100 | Loss 0.365894\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27491 < 29024; dropping {'train/loss': 0.325059711933136}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27492 < 29024; dropping {'train/loss': 0.2507559657096863}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27493 < 29024; dropping {'train/loss': 0.13820192217826843}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27494 < 29024; dropping {'train/loss': 0.4454565644264221}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27495 < 29024; dropping {'train/loss': 0.4017857015132904}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27496 < 29024; dropping {'train/loss': 0.08498703688383102}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27497 < 29024; dropping {'train/loss': 0.6012766361236572}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27498 < 29024; dropping {'train/loss': 0.2489347755908966}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27499 < 29024; dropping {'train/loss': 0.2852417528629303}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27500 < 29024; dropping {'train/loss': 0.46630144119262695}.\n",
      "Epoch 274 | Batch 100/100 | Loss 0.361784\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27501 < 29024; dropping {'train/loss': 0.39468979835510254}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27502 < 29024; dropping {'train/loss': 0.2330155372619629}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27503 < 29024; dropping {'train/loss': 0.5877394676208496}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27504 < 29024; dropping {'train/loss': 0.0980362743139267}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27505 < 29024; dropping {'train/loss': 0.12402205169200897}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27506 < 29024; dropping {'train/loss': 0.32872894406318665}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27507 < 29024; dropping {'train/loss': 0.33579450845718384}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27508 < 29024; dropping {'train/loss': 0.7401484251022339}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27509 < 29024; dropping {'train/loss': 0.47345858812332153}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27510 < 29024; dropping {'train/loss': 0.3738245368003845}.\n",
      "Epoch 275 | Batch 10/100 | Loss 0.368946\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27511 < 29024; dropping {'train/loss': 0.6348224878311157}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27512 < 29024; dropping {'train/loss': 0.24276752769947052}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27513 < 29024; dropping {'train/loss': 0.5660304427146912}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27514 < 29024; dropping {'train/loss': 0.402949720621109}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27515 < 29024; dropping {'train/loss': 0.24960096180438995}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27516 < 29024; dropping {'train/loss': 0.40474629402160645}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27517 < 29024; dropping {'train/loss': 0.40463748574256897}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27518 < 29024; dropping {'train/loss': 0.3608517050743103}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27519 < 29024; dropping {'train/loss': 0.29230761528015137}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27520 < 29024; dropping {'train/loss': 0.5142714977264404}.\n",
      "Epoch 275 | Batch 20/100 | Loss 0.388122\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27521 < 29024; dropping {'train/loss': 0.22392387688159943}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27522 < 29024; dropping {'train/loss': 0.2890247702598572}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27523 < 29024; dropping {'train/loss': 0.3768494725227356}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27524 < 29024; dropping {'train/loss': 0.2433696687221527}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27525 < 29024; dropping {'train/loss': 0.3388884663581848}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27526 < 29024; dropping {'train/loss': 0.24523672461509705}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27527 < 29024; dropping {'train/loss': 0.2698090970516205}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27528 < 29024; dropping {'train/loss': 0.3676774501800537}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27529 < 29024; dropping {'train/loss': 0.25377246737480164}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27530 < 29024; dropping {'train/loss': 0.326524555683136}.\n",
      "Epoch 275 | Batch 30/100 | Loss 0.356584\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27531 < 29024; dropping {'train/loss': 0.3114733099937439}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27532 < 29024; dropping {'train/loss': 0.3222859501838684}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27533 < 29024; dropping {'train/loss': 0.23364880681037903}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27534 < 29024; dropping {'train/loss': 0.34222012758255005}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27535 < 29024; dropping {'train/loss': 0.5600203275680542}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27536 < 29024; dropping {'train/loss': 0.5327127575874329}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27537 < 29024; dropping {'train/loss': 0.4047321677207947}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27538 < 29024; dropping {'train/loss': 0.23994676768779755}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27539 < 29024; dropping {'train/loss': 0.3754355013370514}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27540 < 29024; dropping {'train/loss': 0.5618456602096558}.\n",
      "Epoch 275 | Batch 40/100 | Loss 0.364546\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27541 < 29024; dropping {'train/loss': 0.22585070133209229}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27542 < 29024; dropping {'train/loss': 0.4063694477081299}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27543 < 29024; dropping {'train/loss': 0.2556034028530121}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27544 < 29024; dropping {'train/loss': 0.3594798743724823}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27545 < 29024; dropping {'train/loss': 0.2503402829170227}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27546 < 29024; dropping {'train/loss': 0.3246132731437683}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27547 < 29024; dropping {'train/loss': 0.1612693965435028}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27548 < 29024; dropping {'train/loss': 0.17682160437107086}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27549 < 29024; dropping {'train/loss': 0.40668219327926636}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27550 < 29024; dropping {'train/loss': 0.34793588519096375}.\n",
      "Epoch 275 | Batch 50/100 | Loss 0.349936\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27551 < 29024; dropping {'train/loss': 0.7774022817611694}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27552 < 29024; dropping {'train/loss': 0.5258911848068237}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27553 < 29024; dropping {'train/loss': 0.36549997329711914}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27554 < 29024; dropping {'train/loss': 0.4274032711982727}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27555 < 29024; dropping {'train/loss': 0.4360819458961487}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27556 < 29024; dropping {'train/loss': 0.4485880732536316}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27557 < 29024; dropping {'train/loss': 0.4135289192199707}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27558 < 29024; dropping {'train/loss': 0.6484236717224121}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27559 < 29024; dropping {'train/loss': 0.321690171957016}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27560 < 29024; dropping {'train/loss': 0.29976996779441833}.\n",
      "Epoch 275 | Batch 60/100 | Loss 0.369351\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27561 < 29024; dropping {'train/loss': 0.5923014283180237}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27562 < 29024; dropping {'train/loss': 0.6379245519638062}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27563 < 29024; dropping {'train/loss': 0.3245098292827606}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27564 < 29024; dropping {'train/loss': 0.527969479560852}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27565 < 29024; dropping {'train/loss': 0.3756588399410248}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27566 < 29024; dropping {'train/loss': 0.37901514768600464}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27567 < 29024; dropping {'train/loss': 0.5272488594055176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27568 < 29024; dropping {'train/loss': 0.2877545952796936}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27569 < 29024; dropping {'train/loss': 0.46369796991348267}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27570 < 29024; dropping {'train/loss': 0.21158412098884583}.\n",
      "Epoch 275 | Batch 70/100 | Loss 0.378411\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27571 < 29024; dropping {'train/loss': 0.20464321970939636}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27572 < 29024; dropping {'train/loss': 0.2699965834617615}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27573 < 29024; dropping {'train/loss': 0.591896116733551}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27574 < 29024; dropping {'train/loss': 0.13667313754558563}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27575 < 29024; dropping {'train/loss': 0.4214175343513489}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27576 < 29024; dropping {'train/loss': 0.42198753356933594}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27577 < 29024; dropping {'train/loss': 0.44132599234580994}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27578 < 29024; dropping {'train/loss': 0.18029636144638062}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27579 < 29024; dropping {'train/loss': 0.42535656690597534}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27580 < 29024; dropping {'train/loss': 0.1771247684955597}.\n",
      "Epoch 275 | Batch 80/100 | Loss 0.371993\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27581 < 29024; dropping {'train/loss': 0.2290852814912796}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27582 < 29024; dropping {'train/loss': 0.32634299993515015}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27583 < 29024; dropping {'train/loss': 0.12552039325237274}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27584 < 29024; dropping {'train/loss': 0.4240489602088928}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27585 < 29024; dropping {'train/loss': 0.32081830501556396}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27586 < 29024; dropping {'train/loss': 0.3935734033584595}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27587 < 29024; dropping {'train/loss': 0.2601455748081207}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27588 < 29024; dropping {'train/loss': 0.23149657249450684}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27589 < 29024; dropping {'train/loss': 0.36785316467285156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27590 < 29024; dropping {'train/loss': 0.30518248677253723}.\n",
      "Epoch 275 | Batch 90/100 | Loss 0.363817\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27591 < 29024; dropping {'train/loss': 0.25241395831108093}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27592 < 29024; dropping {'train/loss': 0.6280491352081299}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27593 < 29024; dropping {'train/loss': 0.6014240980148315}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27594 < 29024; dropping {'train/loss': 0.2578151822090149}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27595 < 29024; dropping {'train/loss': 0.5554429292678833}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27596 < 29024; dropping {'train/loss': 0.398257315158844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27597 < 29024; dropping {'train/loss': 0.3374008238315582}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27598 < 29024; dropping {'train/loss': 0.34650084376335144}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27599 < 29024; dropping {'train/loss': 0.2291376143693924}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27600 < 29024; dropping {'train/loss': 0.7453407645225525}.\n",
      "Epoch 275 | Batch 100/100 | Loss 0.370953\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27601 < 29024; dropping {'train/loss': 0.5552865266799927}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27602 < 29024; dropping {'train/loss': 0.4718140959739685}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27603 < 29024; dropping {'train/loss': 0.27101677656173706}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27604 < 29024; dropping {'train/loss': 0.6834337711334229}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27605 < 29024; dropping {'train/loss': 0.575200080871582}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27606 < 29024; dropping {'train/loss': 0.13895714282989502}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27607 < 29024; dropping {'train/loss': 0.23855528235435486}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27608 < 29024; dropping {'train/loss': 0.2499430924654007}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27609 < 29024; dropping {'train/loss': 0.42268937826156616}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27610 < 29024; dropping {'train/loss': 0.3081051707267761}.\n",
      "Epoch 276 | Batch 10/100 | Loss 0.391500\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27611 < 29024; dropping {'train/loss': 0.3192199170589447}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27612 < 29024; dropping {'train/loss': 0.35698503255844116}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27613 < 29024; dropping {'train/loss': 0.2406747043132782}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27614 < 29024; dropping {'train/loss': 0.6880740523338318}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27615 < 29024; dropping {'train/loss': 0.295077383518219}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27616 < 29024; dropping {'train/loss': 0.15785372257232666}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27617 < 29024; dropping {'train/loss': 0.3993646502494812}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27618 < 29024; dropping {'train/loss': 0.3513653576374054}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27619 < 29024; dropping {'train/loss': 0.18018387258052826}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27620 < 29024; dropping {'train/loss': 0.37088194489479065}.\n",
      "Epoch 276 | Batch 20/100 | Loss 0.363734\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27621 < 29024; dropping {'train/loss': 0.3415929675102234}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27622 < 29024; dropping {'train/loss': 0.3719601631164551}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27623 < 29024; dropping {'train/loss': 0.3739955425262451}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27624 < 29024; dropping {'train/loss': 0.28890541195869446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27625 < 29024; dropping {'train/loss': 0.3537619709968567}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27626 < 29024; dropping {'train/loss': 0.47111964225769043}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27627 < 29024; dropping {'train/loss': 0.4020335078239441}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27628 < 29024; dropping {'train/loss': 0.32499879598617554}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27629 < 29024; dropping {'train/loss': 0.3072202801704407}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27630 < 29024; dropping {'train/loss': 0.35065189003944397}.\n",
      "Epoch 276 | Batch 30/100 | Loss 0.362031\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27631 < 29024; dropping {'train/loss': 0.4864625036716461}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27632 < 29024; dropping {'train/loss': 0.24651296436786652}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27633 < 29024; dropping {'train/loss': 0.2628571391105652}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27634 < 29024; dropping {'train/loss': 0.4295956492424011}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27635 < 29024; dropping {'train/loss': 0.4209877848625183}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27636 < 29024; dropping {'train/loss': 0.42866796255111694}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27637 < 29024; dropping {'train/loss': 0.32674068212509155}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27638 < 29024; dropping {'train/loss': 0.22411537170410156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27639 < 29024; dropping {'train/loss': 0.3483838140964508}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27640 < 29024; dropping {'train/loss': 0.35729819536209106}.\n",
      "Epoch 276 | Batch 40/100 | Loss 0.359814\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27641 < 29024; dropping {'train/loss': 0.5642074942588806}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27642 < 29024; dropping {'train/loss': 0.6895967125892639}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27643 < 29024; dropping {'train/loss': 0.14710280299186707}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27644 < 29024; dropping {'train/loss': 0.34533020853996277}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27645 < 29024; dropping {'train/loss': 0.40285736322402954}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27646 < 29024; dropping {'train/loss': 0.24842748045921326}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27647 < 29024; dropping {'train/loss': 0.4858705401420593}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27648 < 29024; dropping {'train/loss': 0.2835440933704376}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27649 < 29024; dropping {'train/loss': 0.5148884057998657}.\n",
      "Epoch 276 | Batch 50/100 | Loss 0.365494wandb: WARNING Step must only increase in log calls.  Step 27650 < 29024; dropping {'train/loss': 0.20032937824726105}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27651 < 29024; dropping {'train/loss': 0.6118751764297485}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27652 < 29024; dropping {'train/loss': 0.05986301973462105}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27653 < 29024; dropping {'train/loss': 0.3406718373298645}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27654 < 29024; dropping {'train/loss': 0.5270541906356812}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27655 < 29024; dropping {'train/loss': 0.1851813942193985}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27656 < 29024; dropping {'train/loss': 0.2695305645465851}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27657 < 29024; dropping {'train/loss': 0.40904879570007324}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27658 < 29024; dropping {'train/loss': 0.2964023947715759}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27659 < 29024; dropping {'train/loss': 0.3803003430366516}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27660 < 29024; dropping {'train/loss': 0.4247322082519531}.\n",
      "Epoch 276 | Batch 60/100 | Loss 0.362989\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27661 < 29024; dropping {'train/loss': 0.24640142917633057}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27662 < 29024; dropping {'train/loss': 0.15209709107875824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27663 < 29024; dropping {'train/loss': 0.4493536353111267}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27664 < 29024; dropping {'train/loss': 0.48540201783180237}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27665 < 29024; dropping {'train/loss': 0.22575227916240692}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27666 < 29024; dropping {'train/loss': 0.4357498586177826}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27667 < 29024; dropping {'train/loss': 0.3009341061115265}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27668 < 29024; dropping {'train/loss': 0.2714935839176178}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27669 < 29024; dropping {'train/loss': 0.43627581000328064}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27670 < 29024; dropping {'train/loss': 0.5628452897071838}.\n",
      "Epoch 276 | Batch 70/100 | Loss 0.362081\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27671 < 29024; dropping {'train/loss': 0.2502219080924988}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27672 < 29024; dropping {'train/loss': 0.3939493000507355}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27673 < 29024; dropping {'train/loss': 0.5151224136352539}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27674 < 29024; dropping {'train/loss': 0.5159324407577515}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27675 < 29024; dropping {'train/loss': 0.3679225742816925}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27676 < 29024; dropping {'train/loss': 0.6223171353340149}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27677 < 29024; dropping {'train/loss': 0.3884263336658478}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27678 < 29024; dropping {'train/loss': 0.6655193567276001}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27679 < 29024; dropping {'train/loss': 0.21674159169197083}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27680 < 29024; dropping {'train/loss': 0.49078574776649475}.\n",
      "Epoch 276 | Batch 80/100 | Loss 0.372158\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27681 < 29024; dropping {'train/loss': 0.3736661672592163}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27682 < 29024; dropping {'train/loss': 0.2645770013332367}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27683 < 29024; dropping {'train/loss': 0.6371549367904663}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27684 < 29024; dropping {'train/loss': 0.24556222558021545}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27685 < 29024; dropping {'train/loss': 0.2825253903865814}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27686 < 29024; dropping {'train/loss': 0.30208343267440796}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27687 < 29024; dropping {'train/loss': 0.2382170408964157}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27688 < 29024; dropping {'train/loss': 0.28378087282180786}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27689 < 29024; dropping {'train/loss': 0.1398819386959076}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27690 < 29024; dropping {'train/loss': 0.5434322357177734}.\n",
      "Epoch 276 | Batch 90/100 | Loss 0.367594\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27691 < 29024; dropping {'train/loss': 0.39073091745376587}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27692 < 29024; dropping {'train/loss': 0.2667681574821472}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27693 < 29024; dropping {'train/loss': 0.4101892411708832}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27694 < 29024; dropping {'train/loss': 0.2114119529724121}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27695 < 29024; dropping {'train/loss': 0.4441445767879486}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27696 < 29024; dropping {'train/loss': 0.15008506178855896}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27697 < 29024; dropping {'train/loss': 0.35876399278640747}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27698 < 29024; dropping {'train/loss': 0.5441153645515442}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27699 < 29024; dropping {'train/loss': 0.32701247930526733}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27700 < 29024; dropping {'train/loss': 0.20068852603435516}.\n",
      "Epoch 276 | Batch 100/100 | Loss 0.363874\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27701 < 29024; dropping {'train/loss': 0.2999420762062073}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27702 < 29024; dropping {'train/loss': 0.5386247634887695}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27703 < 29024; dropping {'train/loss': 0.47987794876098633}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27704 < 29024; dropping {'train/loss': 0.758856475353241}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27705 < 29024; dropping {'train/loss': 0.4029507040977478}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27706 < 29024; dropping {'train/loss': 0.44727277755737305}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27707 < 29024; dropping {'train/loss': 0.26034364104270935}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27708 < 29024; dropping {'train/loss': 0.367587149143219}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27709 < 29024; dropping {'train/loss': 0.30076271295547485}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27710 < 29024; dropping {'train/loss': 0.41140061616897583}.\n",
      "Epoch 277 | Batch 10/100 | Loss 0.426762\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27711 < 29024; dropping {'train/loss': 0.42184704542160034}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27712 < 29024; dropping {'train/loss': 0.3721161484718323}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27713 < 29024; dropping {'train/loss': 0.20805542171001434}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27714 < 29024; dropping {'train/loss': 0.18267987668514252}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27715 < 29024; dropping {'train/loss': 0.1806175857782364}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27716 < 29024; dropping {'train/loss': 0.22912606596946716}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27717 < 29024; dropping {'train/loss': 0.49961209297180176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27718 < 29024; dropping {'train/loss': 0.4129626154899597}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27719 < 29024; dropping {'train/loss': 0.5480490922927856}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27720 < 29024; dropping {'train/loss': 0.37382742762565613}.\n",
      "Epoch 277 | Batch 20/100 | Loss 0.384826\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27721 < 29024; dropping {'train/loss': 0.3343643546104431}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27722 < 29024; dropping {'train/loss': 0.5639669895172119}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27723 < 29024; dropping {'train/loss': 0.3208789825439453}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27724 < 29024; dropping {'train/loss': 0.2005370408296585}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27725 < 29024; dropping {'train/loss': 0.25701799988746643}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27726 < 29024; dropping {'train/loss': 0.2729388475418091}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27727 < 29024; dropping {'train/loss': 0.2900429368019104}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27728 < 29024; dropping {'train/loss': 0.40412408113479614}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27729 < 29024; dropping {'train/loss': 0.3881787657737732}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27730 < 29024; dropping {'train/loss': 0.2026558369398117}.\n",
      "Epoch 277 | Batch 30/100 | Loss 0.364374\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27731 < 29024; dropping {'train/loss': 0.29811912775039673}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27732 < 29024; dropping {'train/loss': 0.11059572547674179}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27733 < 29024; dropping {'train/loss': 0.3563213348388672}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27734 < 29024; dropping {'train/loss': 0.18910346925258636}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27735 < 29024; dropping {'train/loss': 0.288759708404541}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27736 < 29024; dropping {'train/loss': 0.5110489130020142}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27737 < 29024; dropping {'train/loss': 0.30749353766441345}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27738 < 29024; dropping {'train/loss': 0.27964872121810913}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27739 < 29024; dropping {'train/loss': 0.16780000925064087}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27740 < 29024; dropping {'train/loss': 0.37804949283599854}.\n",
      "Epoch 277 | Batch 40/100 | Loss 0.345454\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27741 < 29024; dropping {'train/loss': 0.3963828980922699}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27742 < 29024; dropping {'train/loss': 0.3524358868598938}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27743 < 29024; dropping {'train/loss': 0.4339984357357025}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27744 < 29024; dropping {'train/loss': 0.3348453938961029}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27745 < 29024; dropping {'train/loss': 0.588640570640564}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27746 < 29024; dropping {'train/loss': 0.780351459980011}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27747 < 29024; dropping {'train/loss': 0.27665191888809204}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27748 < 29024; dropping {'train/loss': 0.4858708381652832}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27749 < 29024; dropping {'train/loss': 0.29646387696266174}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27750 < 29024; dropping {'train/loss': 0.3457220196723938}.\n",
      "Epoch 277 | Batch 50/100 | Loss 0.362190\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27751 < 29024; dropping {'train/loss': 0.29294127225875854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27752 < 29024; dropping {'train/loss': 0.3362658619880676}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27753 < 29024; dropping {'train/loss': 0.45516452193260193}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27754 < 29024; dropping {'train/loss': 0.6011431813240051}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27755 < 29024; dropping {'train/loss': 0.5020438432693481}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27756 < 29024; dropping {'train/loss': 0.4394534230232239}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27757 < 29024; dropping {'train/loss': 0.3832716643810272}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27758 < 29024; dropping {'train/loss': 0.1560545414686203}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27759 < 29024; dropping {'train/loss': 0.42449110746383667}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27760 < 29024; dropping {'train/loss': 0.25539666414260864}.\n",
      "Epoch 277 | Batch 60/100 | Loss 0.365929\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27761 < 29024; dropping {'train/loss': 0.2065638303756714}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27762 < 29024; dropping {'train/loss': 0.24316684901714325}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27763 < 29024; dropping {'train/loss': 0.4056645929813385}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27764 < 29024; dropping {'train/loss': 0.21056675910949707}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27765 < 29024; dropping {'train/loss': 0.31084662675857544}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27766 < 29024; dropping {'train/loss': 0.3721785247325897}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27767 < 29024; dropping {'train/loss': 0.27022525668144226}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27768 < 29024; dropping {'train/loss': 0.3372906446456909}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27769 < 29024; dropping {'train/loss': 0.6137818098068237}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27770 < 29024; dropping {'train/loss': 0.4284009337425232}.\n",
      "Epoch 277 | Batch 70/100 | Loss 0.362206\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27771 < 29024; dropping {'train/loss': 0.2781810462474823}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27772 < 29024; dropping {'train/loss': 0.3094577491283417}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27773 < 29024; dropping {'train/loss': 0.44565892219543457}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27774 < 29024; dropping {'train/loss': 0.33652204275131226}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27775 < 29024; dropping {'train/loss': 0.3871905207633972}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27776 < 29024; dropping {'train/loss': 0.636669397354126}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27777 < 29024; dropping {'train/loss': 0.3823111653327942}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27778 < 29024; dropping {'train/loss': 0.4426448345184326}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27779 < 29024; dropping {'train/loss': 0.1392214596271515}.\n",
      "Epoch 277 | Batch 80/100 | Loss 0.363704\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27780 < 29024; dropping {'train/loss': 0.3840343952178955}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27781 < 29024; dropping {'train/loss': 0.3031718134880066}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27782 < 29024; dropping {'train/loss': 0.22264926135540009}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27783 < 29024; dropping {'train/loss': 0.4043703079223633}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27784 < 29024; dropping {'train/loss': 0.5041561722755432}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27785 < 29024; dropping {'train/loss': 0.29003581404685974}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27786 < 29024; dropping {'train/loss': 0.41138219833374023}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27787 < 29024; dropping {'train/loss': 0.4152882993221283}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27788 < 29024; dropping {'train/loss': 0.21391288936138153}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27789 < 29024; dropping {'train/loss': 0.3414022624492645}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27790 < 29024; dropping {'train/loss': 0.34730857610702515}.\n",
      "Epoch 277 | Batch 90/100 | Loss 0.361667\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27791 < 29024; dropping {'train/loss': 0.5959632992744446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27792 < 29024; dropping {'train/loss': 0.20184342563152313}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27793 < 29024; dropping {'train/loss': 0.20310084521770477}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27794 < 29024; dropping {'train/loss': 0.6133197546005249}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27795 < 29024; dropping {'train/loss': 0.24941778182983398}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27796 < 29024; dropping {'train/loss': 0.2344069927930832}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27797 < 29024; dropping {'train/loss': 0.21468904614448547}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27798 < 29024; dropping {'train/loss': 0.2413593828678131}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27799 < 29024; dropping {'train/loss': 0.17455114424228668}.\n",
      "Epoch 277 | Batch 100/100 | Loss 0.354498\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27800 < 29024; dropping {'train/loss': 0.1711096614599228}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27801 < 29024; dropping {'train/loss': 0.6160100698471069}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27802 < 29024; dropping {'train/loss': 0.46043404936790466}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27803 < 29024; dropping {'train/loss': 0.5144997835159302}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27804 < 29024; dropping {'train/loss': 0.4011690020561218}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27805 < 29024; dropping {'train/loss': 0.3861227333545685}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27806 < 29024; dropping {'train/loss': 0.20363762974739075}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27807 < 29024; dropping {'train/loss': 0.37631356716156006}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27808 < 29024; dropping {'train/loss': 0.22610625624656677}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27809 < 29024; dropping {'train/loss': 0.2913220524787903}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27810 < 29024; dropping {'train/loss': 0.41068536043167114}.\n",
      "Epoch 278 | Batch 10/100 | Loss 0.388630\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27811 < 29024; dropping {'train/loss': 0.4055287837982178}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27812 < 29024; dropping {'train/loss': 0.3413141667842865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27813 < 29024; dropping {'train/loss': 0.4133993983268738}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27814 < 29024; dropping {'train/loss': 0.35219046473503113}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27815 < 29024; dropping {'train/loss': 0.8405190706253052}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27816 < 29024; dropping {'train/loss': 0.4999217391014099}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27817 < 29024; dropping {'train/loss': 0.2702499032020569}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27818 < 29024; dropping {'train/loss': 0.30871912837028503}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27819 < 29024; dropping {'train/loss': 0.3297182023525238}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27820 < 29024; dropping {'train/loss': 0.512829601764679}.\n",
      "Epoch 278 | Batch 20/100 | Loss 0.408035\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27821 < 29024; dropping {'train/loss': 0.34107276797294617}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27822 < 29024; dropping {'train/loss': 0.6529968976974487}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27823 < 29024; dropping {'train/loss': 0.2768439054489136}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27824 < 29024; dropping {'train/loss': 0.2419181764125824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27825 < 29024; dropping {'train/loss': 0.6450643539428711}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27826 < 29024; dropping {'train/loss': 0.5156111121177673}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27827 < 29024; dropping {'train/loss': 0.39091405272483826}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27828 < 29024; dropping {'train/loss': 0.30894556641578674}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27829 < 29024; dropping {'train/loss': 0.4762652814388275}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27830 < 29024; dropping {'train/loss': 0.3996971547603607}.\n",
      "Epoch 278 | Batch 30/100 | Loss 0.413667\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27831 < 29024; dropping {'train/loss': 0.16730011999607086}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27832 < 29024; dropping {'train/loss': 0.2541801929473877}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27833 < 29024; dropping {'train/loss': 0.3174484670162201}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27834 < 29024; dropping {'train/loss': 0.31275326013565063}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27835 < 29024; dropping {'train/loss': 0.366799920797348}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27836 < 29024; dropping {'train/loss': 0.21479353308677673}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27837 < 29024; dropping {'train/loss': 0.49447959661483765}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27838 < 29024; dropping {'train/loss': 0.2908363938331604}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27839 < 29024; dropping {'train/loss': 0.3885871171951294}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27840 < 29024; dropping {'train/loss': 0.4313850998878479}.\n",
      "Epoch 278 | Batch 40/100 | Loss 0.391215\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27841 < 29024; dropping {'train/loss': 0.1526269018650055}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27842 < 29024; dropping {'train/loss': 0.510995090007782}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27843 < 29024; dropping {'train/loss': 0.34119585156440735}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27844 < 29024; dropping {'train/loss': 0.27024292945861816}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27845 < 29024; dropping {'train/loss': 0.32884466648101807}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27846 < 29024; dropping {'train/loss': 0.32279151678085327}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27847 < 29024; dropping {'train/loss': 0.25752967596054077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27848 < 29024; dropping {'train/loss': 0.3783431351184845}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27849 < 29024; dropping {'train/loss': 0.22432434558868408}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27850 < 29024; dropping {'train/loss': 0.6966941952705383}.\n",
      "Epoch 278 | Batch 50/100 | Loss 0.382643\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27851 < 29024; dropping {'train/loss': 0.32781410217285156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27852 < 29024; dropping {'train/loss': 0.5077918767929077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27853 < 29024; dropping {'train/loss': 0.6945372223854065}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27854 < 29024; dropping {'train/loss': 0.264411985874176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27855 < 29024; dropping {'train/loss': 0.20399701595306396}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27856 < 29024; dropping {'train/loss': 0.7886366248130798}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27857 < 29024; dropping {'train/loss': 0.2972694933414459}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27858 < 29024; dropping {'train/loss': 0.3528265357017517}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27859 < 29024; dropping {'train/loss': 0.27928441762924194}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27860 < 29024; dropping {'train/loss': 0.32823193073272705}.\n",
      "Epoch 278 | Batch 60/100 | Loss 0.386283\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27861 < 29024; dropping {'train/loss': 0.3209726810455322}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27862 < 29024; dropping {'train/loss': 0.18281933665275574}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27863 < 29024; dropping {'train/loss': 0.2105671912431717}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27864 < 29024; dropping {'train/loss': 0.33899062871932983}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27865 < 29024; dropping {'train/loss': 0.2640112042427063}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27866 < 29024; dropping {'train/loss': 0.23117172718048096}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27867 < 29024; dropping {'train/loss': 0.34206217527389526}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27868 < 29024; dropping {'train/loss': 0.4057849049568176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27869 < 29024; dropping {'train/loss': 0.3192310035228729}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27870 < 29024; dropping {'train/loss': 0.4334731996059418}.\n",
      "Epoch 278 | Batch 70/100 | Loss 0.374658\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27871 < 29024; dropping {'train/loss': 0.6059784293174744}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27872 < 29024; dropping {'train/loss': 0.48267197608947754}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27873 < 29024; dropping {'train/loss': 0.42159804701805115}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27874 < 29024; dropping {'train/loss': 0.5002378225326538}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27875 < 29024; dropping {'train/loss': 0.17992272973060608}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27876 < 29024; dropping {'train/loss': 0.3861594498157501}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27877 < 29024; dropping {'train/loss': 0.28018659353256226}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27878 < 29024; dropping {'train/loss': 0.5028080940246582}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27879 < 29024; dropping {'train/loss': 0.5501250624656677}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27880 < 29024; dropping {'train/loss': 0.2749931514263153}.\n",
      "Epoch 278 | Batch 80/100 | Loss 0.380134\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27881 < 29024; dropping {'train/loss': 0.5099114179611206}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27882 < 29024; dropping {'train/loss': 0.5727845430374146}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27883 < 29024; dropping {'train/loss': 0.3660910427570343}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27884 < 29024; dropping {'train/loss': 0.31339770555496216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27885 < 29024; dropping {'train/loss': 0.37043529748916626}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27886 < 29024; dropping {'train/loss': 0.15129148960113525}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27887 < 29024; dropping {'train/loss': 0.40354546904563904}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27888 < 29024; dropping {'train/loss': 0.2286537140607834}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27889 < 29024; dropping {'train/loss': 0.333615243434906}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27890 < 29024; dropping {'train/loss': 0.16681665182113647}.\n",
      "Epoch 278 | Batch 90/100 | Loss 0.375859\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27891 < 29024; dropping {'train/loss': 0.28475716710090637}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27892 < 29024; dropping {'train/loss': 0.29548904299736023}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27893 < 29024; dropping {'train/loss': 0.3506213128566742}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27894 < 29024; dropping {'train/loss': 0.47789010405540466}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27895 < 29024; dropping {'train/loss': 0.40281781554222107}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27896 < 29024; dropping {'train/loss': 0.5118265748023987}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27897 < 29024; dropping {'train/loss': 0.3504663109779358}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27898 < 29024; dropping {'train/loss': 0.39654678106307983}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27899 < 29024; dropping {'train/loss': 0.38175567984580994}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27900 < 29024; dropping {'train/loss': 0.39012616872787476}.\n",
      "Epoch 278 | Batch 100/100 | Loss 0.376696\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27901 < 29024; dropping {'train/loss': 0.7085667848587036}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27902 < 29024; dropping {'train/loss': 0.19118176400661469}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27903 < 29024; dropping {'train/loss': 0.4844961166381836}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27904 < 29024; dropping {'train/loss': 0.31017082929611206}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27905 < 29024; dropping {'train/loss': 0.41420847177505493}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27906 < 29024; dropping {'train/loss': 0.4214175343513489}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27907 < 29024; dropping {'train/loss': 0.3910751938819885}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27908 < 29024; dropping {'train/loss': 0.5074523091316223}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27909 < 29024; dropping {'train/loss': 0.4055178761482239}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27910 < 29024; dropping {'train/loss': 0.3242402970790863}.\n",
      "Epoch 279 | Batch 10/100 | Loss 0.415833\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27911 < 29024; dropping {'train/loss': 0.28292450308799744}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27912 < 29024; dropping {'train/loss': 0.33809274435043335}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27913 < 29024; dropping {'train/loss': 0.36070531606674194}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27914 < 29024; dropping {'train/loss': 0.21802735328674316}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27915 < 29024; dropping {'train/loss': 0.18921694159507751}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27916 < 29024; dropping {'train/loss': 0.2919503152370453}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27917 < 29024; dropping {'train/loss': 0.3727092444896698}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27918 < 29024; dropping {'train/loss': 0.5970416069030762}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27919 < 29024; dropping {'train/loss': 0.5403528213500977}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27920 < 29024; dropping {'train/loss': 0.45866265892982483}.\n",
      "Epoch 279 | Batch 20/100 | Loss 0.390401\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27921 < 29024; dropping {'train/loss': 0.6100727319717407}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27922 < 29024; dropping {'train/loss': 0.15256337821483612}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27923 < 29024; dropping {'train/loss': 0.5298044085502625}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27924 < 29024; dropping {'train/loss': 0.3159407675266266}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27925 < 29024; dropping {'train/loss': 0.43095070123672485}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27926 < 29024; dropping {'train/loss': 0.24760250747203827}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27927 < 29024; dropping {'train/loss': 0.17843091487884521}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27928 < 29024; dropping {'train/loss': 0.5250864028930664}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27929 < 29024; dropping {'train/loss': 0.3709656596183777}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27930 < 29024; dropping {'train/loss': 0.331277459859848}.\n",
      "Epoch 279 | Batch 30/100 | Loss 0.383357\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27931 < 29024; dropping {'train/loss': 0.4781354069709778}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27932 < 29024; dropping {'train/loss': 0.24386492371559143}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27933 < 29024; dropping {'train/loss': 0.5319441556930542}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27934 < 29024; dropping {'train/loss': 0.5373467206954956}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27935 < 29024; dropping {'train/loss': 0.2376377135515213}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27936 < 29024; dropping {'train/loss': 0.27090293169021606}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27937 < 29024; dropping {'train/loss': 0.20407052338123322}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27938 < 29024; dropping {'train/loss': 0.3974613547325134}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27939 < 29024; dropping {'train/loss': 0.22030356526374817}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27940 < 29024; dropping {'train/loss': 0.45924001932144165}.\n",
      "Epoch 279 | Batch 40/100 | Loss 0.377040\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27941 < 29024; dropping {'train/loss': 0.4249725937843323}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27942 < 29024; dropping {'train/loss': 0.5130718946456909}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27943 < 29024; dropping {'train/loss': 0.289014995098114}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27944 < 29024; dropping {'train/loss': 0.35840103030204773}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27945 < 29024; dropping {'train/loss': 0.6563262939453125}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27946 < 29024; dropping {'train/loss': 0.24697160720825195}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27947 < 29024; dropping {'train/loss': 0.4586673676967621}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27948 < 29024; dropping {'train/loss': 0.2882232069969177}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27949 < 29024; dropping {'train/loss': 0.5338909029960632}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27950 < 29024; dropping {'train/loss': 0.28709283471107483}.\n",
      "Epoch 279 | Batch 50/100 | Loss 0.382765\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27951 < 29024; dropping {'train/loss': 0.4201708436012268}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27952 < 29024; dropping {'train/loss': 0.24059061706066132}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27953 < 29024; dropping {'train/loss': 0.5367738604545593}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27954 < 29024; dropping {'train/loss': 0.2713015079498291}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27955 < 29024; dropping {'train/loss': 0.20101550221443176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27956 < 29024; dropping {'train/loss': 0.39490026235580444}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27957 < 29024; dropping {'train/loss': 0.5228501558303833}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27958 < 29024; dropping {'train/loss': 0.30100569128990173}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27959 < 29024; dropping {'train/loss': 0.35772445797920227}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27960 < 29024; dropping {'train/loss': 0.2830254137516022}.\n",
      "Epoch 279 | Batch 60/100 | Loss 0.377793\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27961 < 29024; dropping {'train/loss': 0.3321654200553894}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27962 < 29024; dropping {'train/loss': 0.2008177489042282}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27963 < 29024; dropping {'train/loss': 0.3515492081642151}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27964 < 29024; dropping {'train/loss': 0.42041540145874023}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27965 < 29024; dropping {'train/loss': 0.3259661793708801}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27966 < 29024; dropping {'train/loss': 0.29644209146499634}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27967 < 29024; dropping {'train/loss': 0.22980816662311554}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27968 < 29024; dropping {'train/loss': 0.20394356548786163}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27969 < 29024; dropping {'train/loss': 0.4579417109489441}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27970 < 29024; dropping {'train/loss': 0.4401710629463196}.\n",
      "Epoch 279 | Batch 70/100 | Loss 0.370383\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27971 < 29024; dropping {'train/loss': 0.42323678731918335}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27972 < 29024; dropping {'train/loss': 0.29365184903144836}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27973 < 29024; dropping {'train/loss': 0.16353797912597656}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27974 < 29024; dropping {'train/loss': 0.6091803312301636}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27975 < 29024; dropping {'train/loss': 0.4629911482334137}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27976 < 29024; dropping {'train/loss': 0.5551261901855469}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27977 < 29024; dropping {'train/loss': 0.33848026394844055}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27978 < 29024; dropping {'train/loss': 0.23853270709514618}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27979 < 29024; dropping {'train/loss': 0.8000414967536926}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27980 < 29024; dropping {'train/loss': 0.430241197347641}.\n",
      "Epoch 279 | Batch 80/100 | Loss 0.378023\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27981 < 29024; dropping {'train/loss': 0.27679648995399475}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27982 < 29024; dropping {'train/loss': 0.3862459361553192}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27983 < 29024; dropping {'train/loss': 0.3942008316516876}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27984 < 29024; dropping {'train/loss': 0.17958956956863403}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27985 < 29024; dropping {'train/loss': 0.3358352780342102}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27986 < 29024; dropping {'train/loss': 0.6648537516593933}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27987 < 29024; dropping {'train/loss': 0.22131283581256866}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27988 < 29024; dropping {'train/loss': 0.2570244073867798}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27989 < 29024; dropping {'train/loss': 0.39963290095329285}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27990 < 29024; dropping {'train/loss': 0.21859577298164368}.\n",
      "Epoch 279 | Batch 90/100 | Loss 0.373066\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27991 < 29024; dropping {'train/loss': 0.6470813751220703}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27992 < 29024; dropping {'train/loss': 0.22512316703796387}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27993 < 29024; dropping {'train/loss': 0.4871768355369568}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27994 < 29024; dropping {'train/loss': 0.4234541356563568}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27995 < 29024; dropping {'train/loss': 0.18138399720191956}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27996 < 29024; dropping {'train/loss': 0.19710808992385864}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27997 < 29024; dropping {'train/loss': 0.5327998399734497}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27998 < 29024; dropping {'train/loss': 0.2599906325340271}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 27999 < 29024; dropping {'train/loss': 0.3046844005584717}.\n",
      "Epoch 279 | Batch 100/100 | Loss 0.371890wandb: WARNING Step must only increase in log calls.  Step 28000 < 29024; dropping {'train/loss': 0.35428133606910706}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28001 < 29024; dropping {'train/loss': 0.36783701181411743}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28002 < 29024; dropping {'train/loss': 0.22584930062294006}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28003 < 29024; dropping {'train/loss': 0.32579198479652405}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28004 < 29024; dropping {'train/loss': 0.5526168346405029}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28005 < 29024; dropping {'train/loss': 0.4389311671257019}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28006 < 29024; dropping {'train/loss': 0.36962249875068665}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28007 < 29024; dropping {'train/loss': 0.34593886137008667}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28008 < 29024; dropping {'train/loss': 0.3451608717441559}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28009 < 29024; dropping {'train/loss': 0.26318472623825073}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28010 < 29024; dropping {'train/loss': 0.25986427068710327}.\n",
      "Epoch 280 | Batch 10/100 | Loss 0.349480\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28011 < 29024; dropping {'train/loss': 0.3166176676750183}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28012 < 29024; dropping {'train/loss': 0.2720470428466797}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28013 < 29024; dropping {'train/loss': 0.42433103919029236}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28014 < 29024; dropping {'train/loss': 0.5802858471870422}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28015 < 29024; dropping {'train/loss': 0.5820843577384949}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28016 < 29024; dropping {'train/loss': 0.2327357828617096}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28017 < 29024; dropping {'train/loss': 0.33232176303863525}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28018 < 29024; dropping {'train/loss': 0.4283212721347809}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28019 < 29024; dropping {'train/loss': 0.37808483839035034}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28020 < 29024; dropping {'train/loss': 0.6419658064842224}.\n",
      "Epoch 280 | Batch 20/100 | Loss 0.384180\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28021 < 29024; dropping {'train/loss': 0.18200896680355072}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28022 < 29024; dropping {'train/loss': 0.3225884735584259}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28023 < 29024; dropping {'train/loss': 0.41688084602355957}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28024 < 29024; dropping {'train/loss': 0.5577532052993774}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28025 < 29024; dropping {'train/loss': 0.43623900413513184}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28026 < 29024; dropping {'train/loss': 0.4456794261932373}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28027 < 29024; dropping {'train/loss': 0.187787264585495}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28028 < 29024; dropping {'train/loss': 0.38315367698669434}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28029 < 29024; dropping {'train/loss': 0.24736428260803223}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28030 < 29024; dropping {'train/loss': 0.40234261751174927}.\n",
      "Epoch 280 | Batch 30/100 | Loss 0.375513\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28031 < 29024; dropping {'train/loss': 0.3980439603328705}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28032 < 29024; dropping {'train/loss': 0.39889830350875854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28033 < 29024; dropping {'train/loss': 0.6323131918907166}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28034 < 29024; dropping {'train/loss': 0.1981409639120102}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28035 < 29024; dropping {'train/loss': 0.5475034713745117}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28036 < 29024; dropping {'train/loss': 0.36689144372940063}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28037 < 29024; dropping {'train/loss': 0.3452177047729492}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28038 < 29024; dropping {'train/loss': 0.2711322009563446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28039 < 29024; dropping {'train/loss': 0.1745121031999588}.\n",
      "Epoch 280 | Batch 40/100 | Loss 0.370884wandb: WARNING Step must only increase in log calls.  Step 28040 < 29024; dropping {'train/loss': 0.23730675876140594}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28041 < 29024; dropping {'train/loss': 0.49728602170944214}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28042 < 29024; dropping {'train/loss': 0.2791219651699066}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28043 < 29024; dropping {'train/loss': 0.2633288502693176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28044 < 29024; dropping {'train/loss': 0.36886322498321533}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28045 < 29024; dropping {'train/loss': 0.5436111688613892}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28046 < 29024; dropping {'train/loss': 0.29930078983306885}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28047 < 29024; dropping {'train/loss': 0.2600427269935608}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28048 < 29024; dropping {'train/loss': 0.38526487350463867}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28049 < 29024; dropping {'train/loss': 0.37256672978401184}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28050 < 29024; dropping {'train/loss': 0.2937944531440735}.\n",
      "Epoch 280 | Batch 50/100 | Loss 0.367971\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28051 < 29024; dropping {'train/loss': 0.623563289642334}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28052 < 29024; dropping {'train/loss': 0.48363152146339417}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28053 < 29024; dropping {'train/loss': 0.5487651228904724}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28054 < 29024; dropping {'train/loss': 0.3523356318473816}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28055 < 29024; dropping {'train/loss': 0.131862610578537}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28056 < 29024; dropping {'train/loss': 0.17447233200073242}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28057 < 29024; dropping {'train/loss': 0.2616679072380066}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28058 < 29024; dropping {'train/loss': 0.5892142057418823}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28059 < 29024; dropping {'train/loss': 0.20501773059368134}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28060 < 29024; dropping {'train/loss': 0.4090755581855774}.\n",
      "Epoch 280 | Batch 60/100 | Loss 0.369636\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28061 < 29024; dropping {'train/loss': 0.6554180383682251}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28062 < 29024; dropping {'train/loss': 0.4095827043056488}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28063 < 29024; dropping {'train/loss': 0.2992745041847229}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28064 < 29024; dropping {'train/loss': 0.38260477781295776}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28065 < 29024; dropping {'train/loss': 0.35894474387168884}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28066 < 29024; dropping {'train/loss': 0.5311124920845032}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28067 < 29024; dropping {'train/loss': 0.5860701203346252}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28068 < 29024; dropping {'train/loss': 0.3556842803955078}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28069 < 29024; dropping {'train/loss': 0.20650699734687805}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28070 < 29024; dropping {'train/loss': 0.20876964926719666}.\n",
      "Epoch 280 | Batch 70/100 | Loss 0.373887\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28071 < 29024; dropping {'train/loss': 0.21764710545539856}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28072 < 29024; dropping {'train/loss': 0.40883222222328186}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28073 < 29024; dropping {'train/loss': 0.4770798683166504}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28074 < 29024; dropping {'train/loss': 0.37179338932037354}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28075 < 29024; dropping {'train/loss': 0.2618505656719208}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28076 < 29024; dropping {'train/loss': 0.12166963517665863}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28077 < 29024; dropping {'train/loss': 0.4436418414115906}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28078 < 29024; dropping {'train/loss': 0.37044358253479004}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28079 < 29024; dropping {'train/loss': 0.31759113073349}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28080 < 29024; dropping {'train/loss': 0.4378257393836975}.\n",
      "Epoch 280 | Batch 80/100 | Loss 0.370006\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28081 < 29024; dropping {'train/loss': 0.534909188747406}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28082 < 29024; dropping {'train/loss': 0.39992278814315796}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28083 < 29024; dropping {'train/loss': 0.520095944404602}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28084 < 29024; dropping {'train/loss': 0.23113825917243958}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28085 < 29024; dropping {'train/loss': 0.24839362502098083}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28086 < 29024; dropping {'train/loss': 0.290619820356369}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28087 < 29024; dropping {'train/loss': 0.5681847333908081}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28088 < 29024; dropping {'train/loss': 0.2896689176559448}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28089 < 29024; dropping {'train/loss': 0.4467007517814636}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28090 < 29024; dropping {'train/loss': 0.2528688609600067}.\n",
      "Epoch 280 | Batch 90/100 | Loss 0.370922\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28091 < 29024; dropping {'train/loss': 0.5267071723937988}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28092 < 29024; dropping {'train/loss': 0.362480491399765}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28093 < 29024; dropping {'train/loss': 0.1923569142818451}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28094 < 29024; dropping {'train/loss': 0.6281108260154724}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28095 < 29024; dropping {'train/loss': 0.4176417887210846}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28096 < 29024; dropping {'train/loss': 0.5629128813743591}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28097 < 29024; dropping {'train/loss': 0.6128373146057129}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28098 < 29024; dropping {'train/loss': 0.3400253653526306}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28099 < 29024; dropping {'train/loss': 0.3453313708305359}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28100 < 29024; dropping {'train/loss': 0.809246838092804}.\n",
      "Epoch 280 | Batch 100/100 | Loss 0.381806\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28101 < 29024; dropping {'train/loss': 0.5172752141952515}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28102 < 29024; dropping {'train/loss': 0.19811174273490906}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28103 < 29024; dropping {'train/loss': 0.3386719226837158}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28104 < 29024; dropping {'train/loss': 0.19740992784500122}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28105 < 29024; dropping {'train/loss': 0.2764752209186554}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28106 < 29024; dropping {'train/loss': 0.39921531081199646}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28107 < 29024; dropping {'train/loss': 0.3964530825614929}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28108 < 29024; dropping {'train/loss': 0.2361229956150055}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28109 < 29024; dropping {'train/loss': 0.18450632691383362}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28110 < 29024; dropping {'train/loss': 0.5143698453903198}.\n",
      "Epoch 281 | Batch 10/100 | Loss 0.325861\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28111 < 29024; dropping {'train/loss': 0.31054118275642395}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28112 < 29024; dropping {'train/loss': 0.2846899628639221}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28113 < 29024; dropping {'train/loss': 0.4091476500034332}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28114 < 29024; dropping {'train/loss': 0.5682841539382935}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28115 < 29024; dropping {'train/loss': 0.2884921133518219}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28116 < 29024; dropping {'train/loss': 0.3075537085533142}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28117 < 29024; dropping {'train/loss': 0.18981875479221344}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28118 < 29024; dropping {'train/loss': 0.44179248809814453}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28119 < 29024; dropping {'train/loss': 0.4611336588859558}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28120 < 29024; dropping {'train/loss': 0.1478480100631714}.\n",
      "Epoch 281 | Batch 20/100 | Loss 0.333396\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28121 < 29024; dropping {'train/loss': 0.37313657999038696}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28122 < 29024; dropping {'train/loss': 0.6138726472854614}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28123 < 29024; dropping {'train/loss': 0.2752784788608551}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28124 < 29024; dropping {'train/loss': 0.2786010503768921}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28125 < 29024; dropping {'train/loss': 0.43130502104759216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28126 < 29024; dropping {'train/loss': 0.17260897159576416}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28127 < 29024; dropping {'train/loss': 0.427178293466568}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28128 < 29024; dropping {'train/loss': 0.18139837682247162}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28129 < 29024; dropping {'train/loss': 0.32258331775665283}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28130 < 29024; dropping {'train/loss': 0.28303369879722595}.\n",
      "Epoch 281 | Batch 30/100 | Loss 0.334230\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28131 < 29024; dropping {'train/loss': 0.27661556005477905}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28132 < 29024; dropping {'train/loss': 0.1938936561346054}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28133 < 29024; dropping {'train/loss': 0.24766293168067932}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28134 < 29024; dropping {'train/loss': 0.2898397445678711}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28135 < 29024; dropping {'train/loss': 0.3800196349620819}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28136 < 29024; dropping {'train/loss': 0.38653886318206787}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28137 < 29024; dropping {'train/loss': 0.582214891910553}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28138 < 29024; dropping {'train/loss': 0.5025071501731873}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28139 < 29024; dropping {'train/loss': 0.3003506362438202}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28140 < 29024; dropping {'train/loss': 0.2420273721218109}.\n",
      "Epoch 281 | Batch 40/100 | Loss 0.335715\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28141 < 29024; dropping {'train/loss': 0.34711724519729614}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28142 < 29024; dropping {'train/loss': 0.21074239909648895}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28143 < 29024; dropping {'train/loss': 0.28083816170692444}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28144 < 29024; dropping {'train/loss': 0.2348102629184723}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28145 < 29024; dropping {'train/loss': 0.37040039896965027}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28146 < 29024; dropping {'train/loss': 0.18639054894447327}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28147 < 29024; dropping {'train/loss': 0.1944987177848816}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28148 < 29024; dropping {'train/loss': 0.5920035243034363}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28149 < 29024; dropping {'train/loss': 0.4069576859474182}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28150 < 29024; dropping {'train/loss': 0.2847084403038025}.\n",
      "Epoch 281 | Batch 50/100 | Loss 0.330741\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28151 < 29024; dropping {'train/loss': 0.3036174476146698}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28152 < 29024; dropping {'train/loss': 0.27889588475227356}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28153 < 29024; dropping {'train/loss': 0.28926464915275574}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28154 < 29024; dropping {'train/loss': 0.8490262031555176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28155 < 29024; dropping {'train/loss': 0.5998705625534058}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28156 < 29024; dropping {'train/loss': 0.4671528935432434}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28157 < 29024; dropping {'train/loss': 0.27171704173088074}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28158 < 29024; dropping {'train/loss': 0.5449376106262207}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28159 < 29024; dropping {'train/loss': 0.2135835886001587}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28160 < 29024; dropping {'train/loss': 0.40176957845687866}.\n",
      "Epoch 281 | Batch 60/100 | Loss 0.345948\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28161 < 29024; dropping {'train/loss': 0.407487154006958}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28162 < 29024; dropping {'train/loss': 0.2532653212547302}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28163 < 29024; dropping {'train/loss': 0.4190630316734314}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28164 < 29024; dropping {'train/loss': 0.4467603266239166}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28165 < 29024; dropping {'train/loss': 0.4016541540622711}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28166 < 29024; dropping {'train/loss': 0.29924076795578003}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28167 < 29024; dropping {'train/loss': 0.318777471780777}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28168 < 29024; dropping {'train/loss': 0.5204514265060425}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28169 < 29024; dropping {'train/loss': 0.718153178691864}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28170 < 29024; dropping {'train/loss': 0.2600669860839844}.\n",
      "Epoch 281 | Batch 70/100 | Loss 0.354311\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28171 < 29024; dropping {'train/loss': 0.37794166803359985}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28172 < 29024; dropping {'train/loss': 0.38443586230278015}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28173 < 29024; dropping {'train/loss': 0.18284234404563904}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28174 < 29024; dropping {'train/loss': 0.38772642612457275}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28175 < 29024; dropping {'train/loss': 0.44651857018470764}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28176 < 29024; dropping {'train/loss': 0.3264772295951843}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28177 < 29024; dropping {'train/loss': 0.3716030716896057}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28178 < 29024; dropping {'train/loss': 0.1773463487625122}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28179 < 29024; dropping {'train/loss': 0.24358832836151123}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28180 < 29024; dropping {'train/loss': 0.5398823618888855}.\n",
      "Epoch 281 | Batch 80/100 | Loss 0.353002\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28181 < 29024; dropping {'train/loss': 0.372852623462677}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28182 < 29024; dropping {'train/loss': 0.35343024134635925}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28183 < 29024; dropping {'train/loss': 0.32004231214523315}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28184 < 29024; dropping {'train/loss': 0.2863864004611969}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28185 < 29024; dropping {'train/loss': 0.2858593165874481}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28186 < 29024; dropping {'train/loss': 0.39182811975479126}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28187 < 29024; dropping {'train/loss': 0.37125539779663086}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28188 < 29024; dropping {'train/loss': 0.3311123847961426}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28189 < 29024; dropping {'train/loss': 0.21735253930091858}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28190 < 29024; dropping {'train/loss': 0.23458215594291687}.\n",
      "Epoch 281 | Batch 90/100 | Loss 0.348943\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28191 < 29024; dropping {'train/loss': 0.4807528555393219}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28192 < 29024; dropping {'train/loss': 0.44829678535461426}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28193 < 29024; dropping {'train/loss': 0.4415912628173828}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28194 < 29024; dropping {'train/loss': 0.41952353715896606}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28195 < 29024; dropping {'train/loss': 0.3551015555858612}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28196 < 29024; dropping {'train/loss': 0.18017612397670746}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28197 < 29024; dropping {'train/loss': 0.36059316992759705}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28198 < 29024; dropping {'train/loss': 0.3626071512699127}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28199 < 29024; dropping {'train/loss': 0.3377140462398529}.\n",
      "Epoch 281 | Batch 100/100 | Loss 0.349229\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28200 < 29024; dropping {'train/loss': 0.13171468675136566}.\n",
      "100 Test Protonet Acc = 81.15% +- 1.44%\n",
      "best model! save...\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28201 < 29024; dropping {'train/loss': 0.2605842053890228}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28202 < 29024; dropping {'train/loss': 0.3928380012512207}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28203 < 29024; dropping {'train/loss': 0.2393537014722824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28204 < 29024; dropping {'train/loss': 0.3101140856742859}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28205 < 29024; dropping {'train/loss': 0.25353896617889404}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28206 < 29024; dropping {'train/loss': 0.398231565952301}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28207 < 29024; dropping {'train/loss': 0.16393910348415375}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28208 < 29024; dropping {'train/loss': 0.18979978561401367}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28209 < 29024; dropping {'train/loss': 0.5554266571998596}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28210 < 29024; dropping {'train/loss': 0.28217241168022156}.\n",
      "Epoch 282 | Batch 10/100 | Loss 0.304600\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28211 < 29024; dropping {'train/loss': 0.20488771796226501}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28212 < 29024; dropping {'train/loss': 0.38442912697792053}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28213 < 29024; dropping {'train/loss': 0.40379923582077026}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28214 < 29024; dropping {'train/loss': 0.26818493008613586}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28215 < 29024; dropping {'train/loss': 0.36975398659706116}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28216 < 29024; dropping {'train/loss': 0.22506916522979736}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28217 < 29024; dropping {'train/loss': 0.32369405031204224}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28218 < 29024; dropping {'train/loss': 0.7077109217643738}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28219 < 29024; dropping {'train/loss': 0.27116426825523376}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28220 < 29024; dropping {'train/loss': 0.1994013637304306}.\n",
      "Epoch 282 | Batch 20/100 | Loss 0.320205\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28221 < 29024; dropping {'train/loss': 0.22022011876106262}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28222 < 29024; dropping {'train/loss': 0.3508152663707733}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28223 < 29024; dropping {'train/loss': 0.4163574278354645}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28224 < 29024; dropping {'train/loss': 0.4531591534614563}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28225 < 29024; dropping {'train/loss': 0.319716215133667}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28226 < 29024; dropping {'train/loss': 0.4632403254508972}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28227 < 29024; dropping {'train/loss': 0.16336219012737274}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28228 < 29024; dropping {'train/loss': 0.23973152041435242}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28229 < 29024; dropping {'train/loss': 0.22354361414909363}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28230 < 29024; dropping {'train/loss': 0.4290347099304199}.\n",
      "Epoch 282 | Batch 30/100 | Loss 0.322776\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28231 < 29024; dropping {'train/loss': 0.17556509375572205}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28232 < 29024; dropping {'train/loss': 0.6849466562271118}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28233 < 29024; dropping {'train/loss': 0.2974148690700531}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28234 < 29024; dropping {'train/loss': 0.26433783769607544}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28235 < 29024; dropping {'train/loss': 0.2732371687889099}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28236 < 29024; dropping {'train/loss': 0.3801942765712738}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28237 < 29024; dropping {'train/loss': 0.31109723448753357}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28238 < 29024; dropping {'train/loss': 0.4478905200958252}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28239 < 29024; dropping {'train/loss': 0.5190894603729248}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28240 < 29024; dropping {'train/loss': 0.35205328464508057}.\n",
      "Epoch 282 | Batch 40/100 | Loss 0.334728\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28241 < 29024; dropping {'train/loss': 0.6362797617912292}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28242 < 29024; dropping {'train/loss': 0.7687925696372986}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28243 < 29024; dropping {'train/loss': 0.6099960803985596}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28244 < 29024; dropping {'train/loss': 0.15770649909973145}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28245 < 29024; dropping {'train/loss': 0.38907721638679504}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28246 < 29024; dropping {'train/loss': 0.2976863980293274}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28247 < 29024; dropping {'train/loss': 0.606339693069458}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28248 < 29024; dropping {'train/loss': 0.3506931662559509}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28249 < 29024; dropping {'train/loss': 0.2628777325153351}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28250 < 29024; dropping {'train/loss': 0.5712406039237976}.\n",
      "Epoch 282 | Batch 50/100 | Loss 0.360796\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28251 < 29024; dropping {'train/loss': 0.5166295766830444}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28252 < 29024; dropping {'train/loss': 0.3416883051395416}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28253 < 29024; dropping {'train/loss': 0.5137527585029602}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28254 < 29024; dropping {'train/loss': 0.45979738235473633}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28255 < 29024; dropping {'train/loss': 0.20626866817474365}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28256 < 29024; dropping {'train/loss': 0.3804014325141907}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28257 < 29024; dropping {'train/loss': 0.3845202922821045}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28258 < 29024; dropping {'train/loss': 0.4063877463340759}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28259 < 29024; dropping {'train/loss': 0.34811681509017944}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28260 < 29024; dropping {'train/loss': 0.4818694591522217}.\n",
      "Epoch 282 | Batch 60/100 | Loss 0.367987\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28261 < 29024; dropping {'train/loss': 0.2622934579849243}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28262 < 29024; dropping {'train/loss': 0.20747092366218567}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28263 < 29024; dropping {'train/loss': 0.4744196832180023}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28264 < 29024; dropping {'train/loss': 0.3734954595565796}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28265 < 29024; dropping {'train/loss': 0.36545759439468384}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28266 < 29024; dropping {'train/loss': 0.4968726634979248}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28267 < 29024; dropping {'train/loss': 0.6351231336593628}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28268 < 29024; dropping {'train/loss': 0.3189179003238678}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28269 < 29024; dropping {'train/loss': 0.5331020355224609}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28270 < 29024; dropping {'train/loss': 0.5094572901725769}.\n",
      "Epoch 282 | Batch 70/100 | Loss 0.375083\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28271 < 29024; dropping {'train/loss': 0.33618563413619995}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28272 < 29024; dropping {'train/loss': 0.38762664794921875}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28273 < 29024; dropping {'train/loss': 0.3827205300331116}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28274 < 29024; dropping {'train/loss': 0.5748537182807922}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28275 < 29024; dropping {'train/loss': 0.3532439172267914}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28276 < 29024; dropping {'train/loss': 0.3112602233886719}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28277 < 29024; dropping {'train/loss': 0.3681389093399048}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28278 < 29024; dropping {'train/loss': 0.4564923644065857}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28279 < 29024; dropping {'train/loss': 0.2651676833629608}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28280 < 29024; dropping {'train/loss': 0.28915756940841675}.\n",
      "Epoch 282 | Batch 80/100 | Loss 0.374758\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28281 < 29024; dropping {'train/loss': 0.15727034211158752}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28282 < 29024; dropping {'train/loss': 0.16659685969352722}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28283 < 29024; dropping {'train/loss': 0.41876858472824097}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28284 < 29024; dropping {'train/loss': 0.36229777336120605}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28285 < 29024; dropping {'train/loss': 0.3237899839878082}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28286 < 29024; dropping {'train/loss': 0.2173197716474533}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28287 < 29024; dropping {'train/loss': 0.27079933881759644}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28288 < 29024; dropping {'train/loss': 0.33333492279052734}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28289 < 29024; dropping {'train/loss': 0.2541310489177704}.\n",
      "Epoch 282 | Batch 90/100 | Loss 0.363426\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28290 < 29024; dropping {'train/loss': 0.22336098551750183}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28291 < 29024; dropping {'train/loss': 0.3130900263786316}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28292 < 29024; dropping {'train/loss': 0.32660093903541565}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28293 < 29024; dropping {'train/loss': 0.3010839819908142}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28294 < 29024; dropping {'train/loss': 0.36181196570396423}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28295 < 29024; dropping {'train/loss': 0.26180148124694824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28296 < 29024; dropping {'train/loss': 0.2747291922569275}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28297 < 29024; dropping {'train/loss': 0.25991731882095337}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28298 < 29024; dropping {'train/loss': 0.49326491355895996}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28299 < 29024; dropping {'train/loss': 0.28914520144462585}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28300 < 29024; dropping {'train/loss': 0.4415304660797119}.\n",
      "Epoch 282 | Batch 100/100 | Loss 0.360313\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28301 < 29024; dropping {'train/loss': 0.42457327246665955}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28302 < 29024; dropping {'train/loss': 0.616193413734436}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28303 < 29024; dropping {'train/loss': 0.6971467137336731}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28304 < 29024; dropping {'train/loss': 0.45845523476600647}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28305 < 29024; dropping {'train/loss': 0.6708873510360718}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28306 < 29024; dropping {'train/loss': 0.3687102198600769}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28307 < 29024; dropping {'train/loss': 0.29260993003845215}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28308 < 29024; dropping {'train/loss': 0.30594778060913086}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28309 < 29024; dropping {'train/loss': 0.44026532769203186}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28310 < 29024; dropping {'train/loss': 0.3489425778388977}.\n",
      "Epoch 283 | Batch 10/100 | Loss 0.462373\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28311 < 29024; dropping {'train/loss': 0.18008829653263092}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28312 < 29024; dropping {'train/loss': 0.25595226883888245}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28313 < 29024; dropping {'train/loss': 0.3547145128250122}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28314 < 29024; dropping {'train/loss': 0.2536878287792206}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28315 < 29024; dropping {'train/loss': 0.27639561891555786}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28316 < 29024; dropping {'train/loss': 0.43731170892715454}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28317 < 29024; dropping {'train/loss': 0.5718811750411987}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28318 < 29024; dropping {'train/loss': 0.3307643532752991}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28319 < 29024; dropping {'train/loss': 0.2942847013473511}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28320 < 29024; dropping {'train/loss': 0.2821204662322998}.\n",
      "Epoch 283 | Batch 20/100 | Loss 0.393047\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28321 < 29024; dropping {'train/loss': 0.2495647370815277}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28322 < 29024; dropping {'train/loss': 0.34362152218818665}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28323 < 29024; dropping {'train/loss': 0.4454394280910492}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28324 < 29024; dropping {'train/loss': 0.3525504171848297}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28325 < 29024; dropping {'train/loss': 0.6531089544296265}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28326 < 29024; dropping {'train/loss': 0.4361437261104584}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28327 < 29024; dropping {'train/loss': 0.46674197912216187}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28328 < 29024; dropping {'train/loss': 0.3921824097633362}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28329 < 29024; dropping {'train/loss': 0.21481378376483917}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28330 < 29024; dropping {'train/loss': 0.3062611520290375}.\n",
      "Epoch 283 | Batch 30/100 | Loss 0.390712\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28331 < 29024; dropping {'train/loss': 0.28283387422561646}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28332 < 29024; dropping {'train/loss': 0.3011181950569153}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28333 < 29024; dropping {'train/loss': 0.4577785134315491}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28334 < 29024; dropping {'train/loss': 0.2435966432094574}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28335 < 29024; dropping {'train/loss': 0.1809014528989792}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28336 < 29024; dropping {'train/loss': 0.5160664916038513}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28337 < 29024; dropping {'train/loss': 0.3458074927330017}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28338 < 29024; dropping {'train/loss': 0.17372582852840424}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28339 < 29024; dropping {'train/loss': 0.3072536885738373}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28340 < 29024; dropping {'train/loss': 0.2895585894584656}.\n",
      "Epoch 283 | Batch 40/100 | Loss 0.370500\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28341 < 29024; dropping {'train/loss': 0.28490787744522095}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28342 < 29024; dropping {'train/loss': 0.22844263911247253}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28343 < 29024; dropping {'train/loss': 0.6313184499740601}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28344 < 29024; dropping {'train/loss': 0.22784166038036346}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28345 < 29024; dropping {'train/loss': 0.4022236764431}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28346 < 29024; dropping {'train/loss': 0.14007997512817383}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28347 < 29024; dropping {'train/loss': 0.39792436361312866}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28348 < 29024; dropping {'train/loss': 0.12781503796577454}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28349 < 29024; dropping {'train/loss': 0.4309011399745941}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28350 < 29024; dropping {'train/loss': 0.45831045508384705}.\n",
      "Epoch 283 | Batch 50/100 | Loss 0.362995\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28351 < 29024; dropping {'train/loss': 0.332704097032547}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28352 < 29024; dropping {'train/loss': 0.4295266568660736}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28353 < 29024; dropping {'train/loss': 0.2444438934326172}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28354 < 29024; dropping {'train/loss': 0.45150572061538696}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28355 < 29024; dropping {'train/loss': 0.3330325186252594}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28356 < 29024; dropping {'train/loss': 0.3141609728336334}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28357 < 29024; dropping {'train/loss': 0.23121008276939392}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28358 < 29024; dropping {'train/loss': 0.40433692932128906}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28359 < 29024; dropping {'train/loss': 0.45903730392456055}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28360 < 29024; dropping {'train/loss': 0.41891831159591675}.\n",
      "Epoch 283 | Batch 60/100 | Loss 0.362811\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28361 < 29024; dropping {'train/loss': 0.432964563369751}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28362 < 29024; dropping {'train/loss': 0.23727376759052277}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28363 < 29024; dropping {'train/loss': 0.7387720346450806}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28364 < 29024; dropping {'train/loss': 0.4354391098022461}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28365 < 29024; dropping {'train/loss': 0.3860529065132141}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28366 < 29024; dropping {'train/loss': 0.32117801904678345}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28367 < 29024; dropping {'train/loss': 0.4548684060573578}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28368 < 29024; dropping {'train/loss': 0.5039247870445251}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28369 < 29024; dropping {'train/loss': 0.32465529441833496}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28370 < 29024; dropping {'train/loss': 0.17338109016418457}.\n",
      "Epoch 283 | Batch 70/100 | Loss 0.368245\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28371 < 29024; dropping {'train/loss': 0.5885353088378906}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28372 < 29024; dropping {'train/loss': 0.2025338113307953}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28373 < 29024; dropping {'train/loss': 0.38437122106552124}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28374 < 29024; dropping {'train/loss': 0.3727334439754486}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28375 < 29024; dropping {'train/loss': 0.21343445777893066}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28376 < 29024; dropping {'train/loss': 0.2796400189399719}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28377 < 29024; dropping {'train/loss': 0.29403263330459595}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28378 < 29024; dropping {'train/loss': 0.2551628649234772}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28379 < 29024; dropping {'train/loss': 0.4425605833530426}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28380 < 29024; dropping {'train/loss': 0.5066245794296265}.\n",
      "Epoch 283 | Batch 80/100 | Loss 0.366460\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28381 < 29024; dropping {'train/loss': 0.3286105990409851}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28382 < 29024; dropping {'train/loss': 0.36339062452316284}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28383 < 29024; dropping {'train/loss': 0.34521403908729553}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28384 < 29024; dropping {'train/loss': 0.30420005321502686}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28385 < 29024; dropping {'train/loss': 0.19558313488960266}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28386 < 29024; dropping {'train/loss': 0.3874439597129822}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28387 < 29024; dropping {'train/loss': 0.43086114525794983}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28388 < 29024; dropping {'train/loss': 0.3534870147705078}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28389 < 29024; dropping {'train/loss': 0.5766804218292236}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28390 < 29024; dropping {'train/loss': 0.27841538190841675}.\n",
      "Epoch 283 | Batch 90/100 | Loss 0.365341\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28391 < 29024; dropping {'train/loss': 0.802163302898407}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28392 < 29024; dropping {'train/loss': 0.3985763192176819}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28393 < 29024; dropping {'train/loss': 0.23572273552417755}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28394 < 29024; dropping {'train/loss': 0.31640806794166565}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28395 < 29024; dropping {'train/loss': 0.6011423468589783}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28396 < 29024; dropping {'train/loss': 0.6604924201965332}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28397 < 29024; dropping {'train/loss': 0.34354740381240845}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28398 < 29024; dropping {'train/loss': 0.4936370849609375}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28399 < 29024; dropping {'train/loss': 0.20971456170082092}.\n",
      "Epoch 283 | Batch 100/100 | Loss 0.373367wandb: WARNING Step must only increase in log calls.  Step 28400 < 29024; dropping {'train/loss': 0.3946612775325775}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28401 < 29024; dropping {'train/loss': 0.2779305577278137}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28402 < 29024; dropping {'train/loss': 0.43962112069129944}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28403 < 29024; dropping {'train/loss': 0.23147520422935486}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28404 < 29024; dropping {'train/loss': 0.3160974383354187}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28405 < 29024; dropping {'train/loss': 0.616036057472229}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28406 < 29024; dropping {'train/loss': 0.48546379804611206}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28407 < 29024; dropping {'train/loss': 0.4803891181945801}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28408 < 29024; dropping {'train/loss': 0.23829177021980286}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28409 < 29024; dropping {'train/loss': 0.2733057141304016}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28410 < 29024; dropping {'train/loss': 0.15555711090564728}.\n",
      "Epoch 284 | Batch 10/100 | Loss 0.351417\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28411 < 29024; dropping {'train/loss': 0.49338454008102417}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28412 < 29024; dropping {'train/loss': 0.40836745500564575}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28413 < 29024; dropping {'train/loss': 0.30128175020217896}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28414 < 29024; dropping {'train/loss': 0.24998128414154053}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28415 < 29024; dropping {'train/loss': 0.42053884267807007}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28416 < 29024; dropping {'train/loss': 0.30770450830459595}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28417 < 29024; dropping {'train/loss': 0.5180460810661316}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28418 < 29024; dropping {'train/loss': 0.40717020630836487}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28419 < 29024; dropping {'train/loss': 0.26749280095100403}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28420 < 29024; dropping {'train/loss': 0.38630348443984985}.\n",
      "Epoch 284 | Batch 20/100 | Loss 0.363722\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28421 < 29024; dropping {'train/loss': 0.17384806275367737}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28422 < 29024; dropping {'train/loss': 0.36417633295059204}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28423 < 29024; dropping {'train/loss': 0.4350341260433197}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28424 < 29024; dropping {'train/loss': 0.10561038553714752}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28425 < 29024; dropping {'train/loss': 0.41581982374191284}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28426 < 29024; dropping {'train/loss': 0.35810819268226624}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28427 < 29024; dropping {'train/loss': 0.5807353258132935}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28428 < 29024; dropping {'train/loss': 0.4274725019931793}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28429 < 29024; dropping {'train/loss': 0.532625675201416}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28430 < 29024; dropping {'train/loss': 0.41970229148864746}.\n",
      "Epoch 284 | Batch 30/100 | Loss 0.369586\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28431 < 29024; dropping {'train/loss': 0.354343056678772}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28432 < 29024; dropping {'train/loss': 0.26377004384994507}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28433 < 29024; dropping {'train/loss': 0.3045410215854645}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28434 < 29024; dropping {'train/loss': 0.3814173638820648}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28435 < 29024; dropping {'train/loss': 0.22242379188537598}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28436 < 29024; dropping {'train/loss': 0.18371638655662537}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28437 < 29024; dropping {'train/loss': 0.2798861861228943}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28438 < 29024; dropping {'train/loss': 0.2615249752998352}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28439 < 29024; dropping {'train/loss': 0.3749846816062927}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28440 < 29024; dropping {'train/loss': 0.41458192467689514}.\n",
      "Epoch 284 | Batch 40/100 | Loss 0.353219\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28441 < 29024; dropping {'train/loss': 0.23144778609275818}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28442 < 29024; dropping {'train/loss': 0.23850798606872559}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28443 < 29024; dropping {'train/loss': 0.268049418926239}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28444 < 29024; dropping {'train/loss': 0.1745930016040802}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28445 < 29024; dropping {'train/loss': 0.5075243711471558}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28446 < 29024; dropping {'train/loss': 0.36093196272850037}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28447 < 29024; dropping {'train/loss': 0.15836794674396515}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28448 < 29024; dropping {'train/loss': 0.46675658226013184}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28449 < 29024; dropping {'train/loss': 0.5857564806938171}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28450 < 29024; dropping {'train/loss': 0.3740686774253845}.\n",
      "Epoch 284 | Batch 50/100 | Loss 0.349895\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28451 < 29024; dropping {'train/loss': 0.572233259677887}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28452 < 29024; dropping {'train/loss': 0.328080952167511}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28453 < 29024; dropping {'train/loss': 0.30291855335235596}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28454 < 29024; dropping {'train/loss': 0.5407648086547852}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28455 < 29024; dropping {'train/loss': 0.5088392496109009}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28456 < 29024; dropping {'train/loss': 0.5648280382156372}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28457 < 29024; dropping {'train/loss': 0.5753694772720337}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28458 < 29024; dropping {'train/loss': 0.2925157845020294}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28459 < 29024; dropping {'train/loss': 0.3530879616737366}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28460 < 29024; dropping {'train/loss': 0.5309801697731018}.\n",
      "Epoch 284 | Batch 60/100 | Loss 0.367740\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28461 < 29024; dropping {'train/loss': 0.3527967929840088}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28462 < 29024; dropping {'train/loss': 0.610716700553894}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28463 < 29024; dropping {'train/loss': 0.15142619609832764}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28464 < 29024; dropping {'train/loss': 0.3934282958507538}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28465 < 29024; dropping {'train/loss': 0.4108067452907562}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28466 < 29024; dropping {'train/loss': 0.27398064732551575}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28467 < 29024; dropping {'train/loss': 0.21852728724479675}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28468 < 29024; dropping {'train/loss': 0.2981606721878052}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28469 < 29024; dropping {'train/loss': 0.4445345997810364}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28470 < 29024; dropping {'train/loss': 0.12704864144325256}.\n",
      "Epoch 284 | Batch 70/100 | Loss 0.362083\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28471 < 29024; dropping {'train/loss': 0.5947718620300293}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28472 < 29024; dropping {'train/loss': 0.38860782980918884}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28473 < 29024; dropping {'train/loss': 0.326133668422699}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28474 < 29024; dropping {'train/loss': 0.0947558581829071}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28475 < 29024; dropping {'train/loss': 0.3015214800834656}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28476 < 29024; dropping {'train/loss': 0.4365328848361969}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28477 < 29024; dropping {'train/loss': 0.2590029537677765}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28478 < 29024; dropping {'train/loss': 0.35406720638275146}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28479 < 29024; dropping {'train/loss': 0.28804364800453186}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28480 < 29024; dropping {'train/loss': 0.2305978238582611}.\n",
      "Epoch 284 | Batch 80/100 | Loss 0.357748\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28481 < 29024; dropping {'train/loss': 0.17618463933467865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28482 < 29024; dropping {'train/loss': 0.37500834465026855}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28483 < 29024; dropping {'train/loss': 0.828021228313446}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28484 < 29024; dropping {'train/loss': 0.37196916341781616}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28485 < 29024; dropping {'train/loss': 0.3222258687019348}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28486 < 29024; dropping {'train/loss': 0.23939867317676544}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28487 < 29024; dropping {'train/loss': 0.38447248935699463}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28488 < 29024; dropping {'train/loss': 0.38437288999557495}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28489 < 29024; dropping {'train/loss': 0.5024774074554443}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28490 < 29024; dropping {'train/loss': 0.3577274680137634}.\n",
      "Epoch 284 | Batch 90/100 | Loss 0.361797\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28491 < 29024; dropping {'train/loss': 0.3036379814147949}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28492 < 29024; dropping {'train/loss': 0.461099237203598}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28493 < 29024; dropping {'train/loss': 0.32100123167037964}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28494 < 29024; dropping {'train/loss': 0.5779610872268677}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28495 < 29024; dropping {'train/loss': 0.3365476727485657}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28496 < 29024; dropping {'train/loss': 0.20449677109718323}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28497 < 29024; dropping {'train/loss': 0.27568894624710083}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28498 < 29024; dropping {'train/loss': 0.16336145997047424}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28499 < 29024; dropping {'train/loss': 0.7013022303581238}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28500 < 29024; dropping {'train/loss': 0.6289395093917847}.\n",
      "Epoch 284 | Batch 100/100 | Loss 0.365357\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28501 < 29024; dropping {'train/loss': 0.17258229851722717}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28502 < 29024; dropping {'train/loss': 0.4311443865299225}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28503 < 29024; dropping {'train/loss': 0.20404621958732605}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28504 < 29024; dropping {'train/loss': 0.4250367283821106}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28505 < 29024; dropping {'train/loss': 0.4317433834075928}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28506 < 29024; dropping {'train/loss': 0.2127346694469452}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28507 < 29024; dropping {'train/loss': 0.3252198100090027}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28508 < 29024; dropping {'train/loss': 0.2901919186115265}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28509 < 29024; dropping {'train/loss': 0.15122398734092712}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28510 < 29024; dropping {'train/loss': 0.6450439095497131}.\n",
      "Epoch 285 | Batch 10/100 | Loss 0.328897\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28511 < 29024; dropping {'train/loss': 0.4765849709510803}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28512 < 29024; dropping {'train/loss': 0.29986098408699036}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28513 < 29024; dropping {'train/loss': 0.4048992991447449}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28514 < 29024; dropping {'train/loss': 0.3093031346797943}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28515 < 29024; dropping {'train/loss': 0.41136425733566284}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28516 < 29024; dropping {'train/loss': 0.4527345597743988}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28517 < 29024; dropping {'train/loss': 0.23987019062042236}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28518 < 29024; dropping {'train/loss': 0.5030553340911865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28519 < 29024; dropping {'train/loss': 0.6299825310707092}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28520 < 29024; dropping {'train/loss': 0.3257008492946625}.\n",
      "Epoch 285 | Batch 20/100 | Loss 0.367116\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28521 < 29024; dropping {'train/loss': 0.2519473731517792}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28522 < 29024; dropping {'train/loss': 0.31387075781822205}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28523 < 29024; dropping {'train/loss': 0.2618091404438019}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28524 < 29024; dropping {'train/loss': 0.31199121475219727}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28525 < 29024; dropping {'train/loss': 0.6397017240524292}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28526 < 29024; dropping {'train/loss': 0.331474244594574}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28527 < 29024; dropping {'train/loss': 0.37155404686927795}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28528 < 29024; dropping {'train/loss': 0.3312845826148987}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28529 < 29024; dropping {'train/loss': 0.30754855275154114}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28530 < 29024; dropping {'train/loss': 0.4016074240207672}.\n",
      "Epoch 285 | Batch 30/100 | Loss 0.362170\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28531 < 29024; dropping {'train/loss': 0.2528451383113861}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28532 < 29024; dropping {'train/loss': 0.42727240920066833}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28533 < 29024; dropping {'train/loss': 0.23254147171974182}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28534 < 29024; dropping {'train/loss': 0.2731805443763733}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28535 < 29024; dropping {'train/loss': 0.4552672803401947}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28536 < 29024; dropping {'train/loss': 0.4425068497657776}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28537 < 29024; dropping {'train/loss': 0.31752848625183105}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28538 < 29024; dropping {'train/loss': 0.6120322942733765}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28539 < 29024; dropping {'train/loss': 0.3529258668422699}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28540 < 29024; dropping {'train/loss': 0.5652430057525635}.\n",
      "Epoch 285 | Batch 40/100 | Loss 0.369911\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28541 < 29024; dropping {'train/loss': 0.47830113768577576}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28542 < 29024; dropping {'train/loss': 0.3759068250656128}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28543 < 29024; dropping {'train/loss': 0.30277565121650696}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28544 < 29024; dropping {'train/loss': 0.17169643938541412}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28545 < 29024; dropping {'train/loss': 0.6331495642662048}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28546 < 29024; dropping {'train/loss': 0.35565194487571716}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28547 < 29024; dropping {'train/loss': 0.15443144738674164}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28548 < 29024; dropping {'train/loss': 0.14088021218776703}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28549 < 29024; dropping {'train/loss': 0.264542818069458}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28550 < 29024; dropping {'train/loss': 0.4851326048374176}.\n",
      "Epoch 285 | Batch 50/100 | Loss 0.363178\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28551 < 29024; dropping {'train/loss': 0.38344576954841614}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28552 < 29024; dropping {'train/loss': 0.2270374745130539}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28553 < 29024; dropping {'train/loss': 0.24657098948955536}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28554 < 29024; dropping {'train/loss': 0.6172906756401062}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28555 < 29024; dropping {'train/loss': 0.5780190825462341}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28556 < 29024; dropping {'train/loss': 0.1835707724094391}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28557 < 29024; dropping {'train/loss': 0.35797327756881714}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28558 < 29024; dropping {'train/loss': 0.5899309515953064}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28559 < 29024; dropping {'train/loss': 0.5101715922355652}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28560 < 29024; dropping {'train/loss': 0.3443893492221832}.\n",
      "Epoch 285 | Batch 60/100 | Loss 0.369955\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28561 < 29024; dropping {'train/loss': 0.27244800329208374}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28562 < 29024; dropping {'train/loss': 0.3219345808029175}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28563 < 29024; dropping {'train/loss': 0.14869901537895203}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28564 < 29024; dropping {'train/loss': 0.42253151535987854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28565 < 29024; dropping {'train/loss': 0.4971223771572113}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28566 < 29024; dropping {'train/loss': 0.5393507480621338}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28567 < 29024; dropping {'train/loss': 0.3476686179637909}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28568 < 29024; dropping {'train/loss': 0.22890758514404297}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28569 < 29024; dropping {'train/loss': 0.4485796391963959}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28570 < 29024; dropping {'train/loss': 0.32146233320236206}.\n",
      "Epoch 285 | Batch 70/100 | Loss 0.367800\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28571 < 29024; dropping {'train/loss': 0.42589935660362244}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28572 < 29024; dropping {'train/loss': 0.38682281970977783}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28573 < 29024; dropping {'train/loss': 0.534051775932312}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28574 < 29024; dropping {'train/loss': 0.4170229434967041}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28575 < 29024; dropping {'train/loss': 0.13055309653282166}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28576 < 29024; dropping {'train/loss': 0.294658899307251}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28577 < 29024; dropping {'train/loss': 0.1883825957775116}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28578 < 29024; dropping {'train/loss': 0.6357394456863403}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28579 < 29024; dropping {'train/loss': 0.5492023825645447}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28580 < 29024; dropping {'train/loss': 0.34161633253097534}.\n",
      "Epoch 285 | Batch 80/100 | Loss 0.370625\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28581 < 29024; dropping {'train/loss': 0.3799608647823334}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28582 < 29024; dropping {'train/loss': 0.2503966689109802}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28583 < 29024; dropping {'train/loss': 0.35521453619003296}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28584 < 29024; dropping {'train/loss': 0.5410090088844299}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28585 < 29024; dropping {'train/loss': 0.4020225405693054}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28586 < 29024; dropping {'train/loss': 0.40826505422592163}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28587 < 29024; dropping {'train/loss': 0.2743929624557495}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28588 < 29024; dropping {'train/loss': 0.3347651958465576}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28589 < 29024; dropping {'train/loss': 0.2919870913028717}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28590 < 29024; dropping {'train/loss': 0.3684947192668915}.\n",
      "Epoch 285 | Batch 90/100 | Loss 0.369517\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28591 < 29024; dropping {'train/loss': 0.25288620591163635}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28592 < 29024; dropping {'train/loss': 0.2768813669681549}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28593 < 29024; dropping {'train/loss': 0.2941468358039856}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28594 < 29024; dropping {'train/loss': 0.2887611389160156}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28595 < 29024; dropping {'train/loss': 0.6144503355026245}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28596 < 29024; dropping {'train/loss': 0.26821503043174744}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28597 < 29024; dropping {'train/loss': 0.3941911458969116}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28598 < 29024; dropping {'train/loss': 0.25430911779403687}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28599 < 29024; dropping {'train/loss': 0.17944687604904175}.\n",
      "Epoch 285 | Batch 100/100 | Loss 0.364501wandb: WARNING Step must only increase in log calls.  Step 28600 < 29024; dropping {'train/loss': 0.37029796838760376}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28601 < 29024; dropping {'train/loss': 0.520516037940979}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28602 < 29024; dropping {'train/loss': 0.4138818383216858}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28603 < 29024; dropping {'train/loss': 0.34782612323760986}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28604 < 29024; dropping {'train/loss': 0.5362791419029236}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28605 < 29024; dropping {'train/loss': 0.515973687171936}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28606 < 29024; dropping {'train/loss': 0.2354123294353485}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28607 < 29024; dropping {'train/loss': 0.25640466809272766}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28608 < 29024; dropping {'train/loss': 0.1477920264005661}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28609 < 29024; dropping {'train/loss': 0.6245884299278259}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28610 < 29024; dropping {'train/loss': 0.30891934037208557}.\n",
      "Epoch 286 | Batch 10/100 | Loss 0.390759\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28611 < 29024; dropping {'train/loss': 0.2562141716480255}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28612 < 29024; dropping {'train/loss': 0.24315378069877625}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28613 < 29024; dropping {'train/loss': 0.16811995208263397}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28614 < 29024; dropping {'train/loss': 0.308865487575531}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28615 < 29024; dropping {'train/loss': 0.11187592893838882}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28616 < 29024; dropping {'train/loss': 0.11239087581634521}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28617 < 29024; dropping {'train/loss': 0.3673645555973053}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28618 < 29024; dropping {'train/loss': 0.7075420618057251}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28619 < 29024; dropping {'train/loss': 0.3954647183418274}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28620 < 29024; dropping {'train/loss': 0.37215936183929443}.\n",
      "Epoch 286 | Batch 20/100 | Loss 0.347537\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28621 < 29024; dropping {'train/loss': 0.4018954336643219}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28622 < 29024; dropping {'train/loss': 0.46751755475997925}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28623 < 29024; dropping {'train/loss': 0.40590041875839233}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28624 < 29024; dropping {'train/loss': 0.28444764018058777}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28625 < 29024; dropping {'train/loss': 0.08190043270587921}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28626 < 29024; dropping {'train/loss': 0.3495253324508667}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28627 < 29024; dropping {'train/loss': 0.34076470136642456}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28628 < 29024; dropping {'train/loss': 0.5103765726089478}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28629 < 29024; dropping {'train/loss': 0.3066573441028595}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28630 < 29024; dropping {'train/loss': 0.29137611389160156}.\n",
      "Epoch 286 | Batch 30/100 | Loss 0.346370\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28631 < 29024; dropping {'train/loss': 0.10921253263950348}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28632 < 29024; dropping {'train/loss': 0.20639464259147644}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28633 < 29024; dropping {'train/loss': 0.2558048367500305}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28634 < 29024; dropping {'train/loss': 0.46150368452072144}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28635 < 29024; dropping {'train/loss': 0.6758317947387695}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28636 < 29024; dropping {'train/loss': 0.3298267722129822}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28637 < 29024; dropping {'train/loss': 0.2567563056945801}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28638 < 29024; dropping {'train/loss': 0.2143803834915161}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28639 < 29024; dropping {'train/loss': 0.36759620904922485}.\n",
      "Epoch 286 | Batch 40/100 | Loss 0.338931\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28640 < 29024; dropping {'train/loss': 0.2888159155845642}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28641 < 29024; dropping {'train/loss': 0.3997516334056854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28642 < 29024; dropping {'train/loss': 0.7728231549263}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28643 < 29024; dropping {'train/loss': 0.20600397884845734}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28644 < 29024; dropping {'train/loss': 0.21791931986808777}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28645 < 29024; dropping {'train/loss': 0.43163055181503296}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28646 < 29024; dropping {'train/loss': 0.256803423166275}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28647 < 29024; dropping {'train/loss': 0.426216185092926}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28648 < 29024; dropping {'train/loss': 0.3831580877304077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28649 < 29024; dropping {'train/loss': 0.522203266620636}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28650 < 29024; dropping {'train/loss': 0.3781854212284088}.\n",
      "Epoch 286 | Batch 50/100 | Loss 0.351038\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28651 < 29024; dropping {'train/loss': 0.3173202872276306}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28652 < 29024; dropping {'train/loss': 0.2486414909362793}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28653 < 29024; dropping {'train/loss': 0.19464609026908875}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28654 < 29024; dropping {'train/loss': 0.3179798424243927}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28655 < 29024; dropping {'train/loss': 0.3728103041648865}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28656 < 29024; dropping {'train/loss': 0.3745163381099701}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28657 < 29024; dropping {'train/loss': 0.4633830189704895}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28658 < 29024; dropping {'train/loss': 0.31937846541404724}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28659 < 29024; dropping {'train/loss': 0.28525564074516296}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28660 < 29024; dropping {'train/loss': 0.2937954068183899}.\n",
      "Epoch 286 | Batch 60/100 | Loss 0.345661\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28661 < 29024; dropping {'train/loss': 0.542658269405365}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28662 < 29024; dropping {'train/loss': 0.42458677291870117}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28663 < 29024; dropping {'train/loss': 0.11640772968530655}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28664 < 29024; dropping {'train/loss': 0.5187527537345886}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28665 < 29024; dropping {'train/loss': 0.2537224292755127}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28666 < 29024; dropping {'train/loss': 0.52793288230896}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28667 < 29024; dropping {'train/loss': 0.4446207880973816}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28668 < 29024; dropping {'train/loss': 0.4199642241001129}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28669 < 29024; dropping {'train/loss': 0.37415140867233276}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28670 < 29024; dropping {'train/loss': 0.3408806324005127}.\n",
      "Epoch 286 | Batch 70/100 | Loss 0.352905\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28671 < 29024; dropping {'train/loss': 0.3196398615837097}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28672 < 29024; dropping {'train/loss': 0.35201728343963623}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28673 < 29024; dropping {'train/loss': 0.4208475947380066}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28674 < 29024; dropping {'train/loss': 0.22563385963439941}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28675 < 29024; dropping {'train/loss': 0.3616923391819}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28676 < 29024; dropping {'train/loss': 0.2873806655406952}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28677 < 29024; dropping {'train/loss': 0.4776328504085541}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28678 < 29024; dropping {'train/loss': 0.3605605363845825}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28679 < 29024; dropping {'train/loss': 0.19690725207328796}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28680 < 29024; dropping {'train/loss': 0.22907158732414246}.\n",
      "Epoch 286 | Batch 80/100 | Loss 0.349184\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28681 < 29024; dropping {'train/loss': 0.28394415974617004}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28682 < 29024; dropping {'train/loss': 0.25200802087783813}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28683 < 29024; dropping {'train/loss': 0.344032347202301}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28684 < 29024; dropping {'train/loss': 0.3028995990753174}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28685 < 29024; dropping {'train/loss': 0.38756901025772095}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28686 < 29024; dropping {'train/loss': 0.1305403709411621}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28687 < 29024; dropping {'train/loss': 0.20374374091625214}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28688 < 29024; dropping {'train/loss': 0.19162853062152863}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28689 < 29024; dropping {'train/loss': 0.31815457344055176}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28690 < 29024; dropping {'train/loss': 0.259615957736969}.\n",
      "Epoch 286 | Batch 90/100 | Loss 0.340098\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28691 < 29024; dropping {'train/loss': 0.35519689321517944}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28692 < 29024; dropping {'train/loss': 0.24291321635246277}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28693 < 29024; dropping {'train/loss': 0.1907384693622589}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28694 < 29024; dropping {'train/loss': 0.42539387941360474}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28695 < 29024; dropping {'train/loss': 0.31950873136520386}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28696 < 29024; dropping {'train/loss': 0.49066051840782166}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28697 < 29024; dropping {'train/loss': 0.30218759179115295}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28698 < 29024; dropping {'train/loss': 0.19836299121379852}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28699 < 29024; dropping {'train/loss': 0.3385918140411377}.\n",
      "Epoch 286 | Batch 100/100 | Loss 0.337870wandb: WARNING Step must only increase in log calls.  Step 28700 < 29024; dropping {'train/loss': 0.3146122097969055}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28701 < 29024; dropping {'train/loss': 0.5065393447875977}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28702 < 29024; dropping {'train/loss': 0.3794802725315094}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28703 < 29024; dropping {'train/loss': 0.45112451910972595}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28704 < 29024; dropping {'train/loss': 0.4618074893951416}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28705 < 29024; dropping {'train/loss': 0.34703558683395386}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28706 < 29024; dropping {'train/loss': 0.31218433380126953}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28707 < 29024; dropping {'train/loss': 0.574118971824646}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28708 < 29024; dropping {'train/loss': 0.19854560494422913}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28709 < 29024; dropping {'train/loss': 0.30374330282211304}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28710 < 29024; dropping {'train/loss': 0.803801417350769}.\n",
      "Epoch 287 | Batch 10/100 | Loss 0.433838\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28711 < 29024; dropping {'train/loss': 0.30788153409957886}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28712 < 29024; dropping {'train/loss': 0.49292653799057007}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28713 < 29024; dropping {'train/loss': 0.2689210772514343}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28714 < 29024; dropping {'train/loss': 0.19137156009674072}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28715 < 29024; dropping {'train/loss': 0.4399508833885193}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28716 < 29024; dropping {'train/loss': 0.07243739813566208}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28717 < 29024; dropping {'train/loss': 0.5776373147964478}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28718 < 29024; dropping {'train/loss': 0.4169786870479584}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28719 < 29024; dropping {'train/loss': 0.4069136083126068}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28720 < 29024; dropping {'train/loss': 0.836478054523468}.\n",
      "Epoch 287 | Batch 20/100 | Loss 0.417494\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28721 < 29024; dropping {'train/loss': 0.22896268963813782}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28722 < 29024; dropping {'train/loss': 0.32310712337493896}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28723 < 29024; dropping {'train/loss': 0.22374096512794495}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28724 < 29024; dropping {'train/loss': 0.324434369802475}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28725 < 29024; dropping {'train/loss': 0.322846382856369}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28726 < 29024; dropping {'train/loss': 0.16890422999858856}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28727 < 29024; dropping {'train/loss': 0.33638203144073486}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28728 < 29024; dropping {'train/loss': 0.2563484311103821}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28729 < 29024; dropping {'train/loss': 0.3873504102230072}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28730 < 29024; dropping {'train/loss': 0.22735938429832458}.\n",
      "Epoch 287 | Batch 30/100 | Loss 0.371644\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28731 < 29024; dropping {'train/loss': 0.40246835350990295}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28732 < 29024; dropping {'train/loss': 0.15564760565757751}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28733 < 29024; dropping {'train/loss': 0.49547070264816284}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28734 < 29024; dropping {'train/loss': 0.3485003411769867}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28735 < 29024; dropping {'train/loss': 0.5936652421951294}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28736 < 29024; dropping {'train/loss': 0.41956621408462524}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28737 < 29024; dropping {'train/loss': 0.3640274703502655}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28738 < 29024; dropping {'train/loss': 0.41181111335754395}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28739 < 29024; dropping {'train/loss': 0.2435789853334427}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28740 < 29024; dropping {'train/loss': 0.351605623960495}.\n",
      "Epoch 287 | Batch 40/100 | Loss 0.373391\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28741 < 29024; dropping {'train/loss': 0.4446834623813629}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28742 < 29024; dropping {'train/loss': 0.13128703832626343}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28743 < 29024; dropping {'train/loss': 0.4179772436618805}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28744 < 29024; dropping {'train/loss': 0.5620046854019165}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28745 < 29024; dropping {'train/loss': 0.38844066858291626}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28746 < 29024; dropping {'train/loss': 0.39719974994659424}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28747 < 29024; dropping {'train/loss': 0.489952027797699}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28748 < 29024; dropping {'train/loss': 0.3579452931880951}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28749 < 29024; dropping {'train/loss': 0.309757262468338}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28750 < 29024; dropping {'train/loss': 0.3442402780056}.\n",
      "Epoch 287 | Batch 50/100 | Loss 0.375583\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28751 < 29024; dropping {'train/loss': 0.19454948604106903}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28752 < 29024; dropping {'train/loss': 0.49549001455307007}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28753 < 29024; dropping {'train/loss': 0.4648188054561615}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28754 < 29024; dropping {'train/loss': 0.48281025886535645}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28755 < 29024; dropping {'train/loss': 0.1800578385591507}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28756 < 29024; dropping {'train/loss': 0.20166365802288055}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28757 < 29024; dropping {'train/loss': 0.47828611731529236}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28758 < 29024; dropping {'train/loss': 0.23346486687660217}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28759 < 29024; dropping {'train/loss': 0.2919659912586212}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28760 < 29024; dropping {'train/loss': 0.3306735157966614}.\n",
      "Epoch 287 | Batch 60/100 | Loss 0.368882\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28761 < 29024; dropping {'train/loss': 0.449300616979599}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28762 < 29024; dropping {'train/loss': 0.32156041264533997}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28763 < 29024; dropping {'train/loss': 0.2252274751663208}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28764 < 29024; dropping {'train/loss': 0.22106075286865234}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28765 < 29024; dropping {'train/loss': 0.4302830696105957}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28766 < 29024; dropping {'train/loss': 0.2326851785182953}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28767 < 29024; dropping {'train/loss': 0.335665225982666}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28768 < 29024; dropping {'train/loss': 0.34274405241012573}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28769 < 29024; dropping {'train/loss': 0.5986850261688232}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28770 < 29024; dropping {'train/loss': 0.3454843759536743}.\n",
      "Epoch 287 | Batch 70/100 | Loss 0.366223\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28771 < 29024; dropping {'train/loss': 0.524762749671936}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28772 < 29024; dropping {'train/loss': 0.3712904453277588}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28773 < 29024; dropping {'train/loss': 0.6990858912467957}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28774 < 29024; dropping {'train/loss': 0.28922486305236816}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28775 < 29024; dropping {'train/loss': 0.2603208124637604}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28776 < 29024; dropping {'train/loss': 0.3468745946884155}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28777 < 29024; dropping {'train/loss': 0.48249608278274536}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28778 < 29024; dropping {'train/loss': 0.29451555013656616}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28779 < 29024; dropping {'train/loss': 0.4547666907310486}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28780 < 29024; dropping {'train/loss': 0.11678673326969147}.\n",
      "Epoch 287 | Batch 80/100 | Loss 0.368447\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28781 < 29024; dropping {'train/loss': 0.7286348938941956}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28782 < 29024; dropping {'train/loss': 0.29375705122947693}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28783 < 29024; dropping {'train/loss': 0.29730719327926636}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28784 < 29024; dropping {'train/loss': 0.6379016637802124}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28785 < 29024; dropping {'train/loss': 0.31113535165786743}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28786 < 29024; dropping {'train/loss': 0.4516088366508484}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28787 < 29024; dropping {'train/loss': 0.3832370936870575}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28788 < 29024; dropping {'train/loss': 0.17299282550811768}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28789 < 29024; dropping {'train/loss': 0.27274563908576965}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28790 < 29024; dropping {'train/loss': 0.29499000310897827}.\n",
      "Epoch 287 | Batch 90/100 | Loss 0.370223\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28791 < 29024; dropping {'train/loss': 0.23264817893505096}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28792 < 29024; dropping {'train/loss': 0.29901304841041565}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28793 < 29024; dropping {'train/loss': 0.38108935952186584}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28794 < 29024; dropping {'train/loss': 0.31776517629623413}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28795 < 29024; dropping {'train/loss': 0.4644748270511627}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28796 < 29024; dropping {'train/loss': 0.45504230260849}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28797 < 29024; dropping {'train/loss': 0.33152738213539124}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28798 < 29024; dropping {'train/loss': 0.28254932165145874}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28799 < 29024; dropping {'train/loss': 0.5138942003250122}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28800 < 29024; dropping {'train/loss': 0.4545603394508362}.\n",
      "Epoch 287 | Batch 100/100 | Loss 0.370526\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28801 < 29024; dropping {'train/loss': 0.34484249353408813}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28802 < 29024; dropping {'train/loss': 0.5199790596961975}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28803 < 29024; dropping {'train/loss': 0.34753602743148804}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28804 < 29024; dropping {'train/loss': 0.3952547609806061}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28805 < 29024; dropping {'train/loss': 0.21164509654045105}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28806 < 29024; dropping {'train/loss': 0.7924193739891052}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28807 < 29024; dropping {'train/loss': 0.1357703059911728}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28808 < 29024; dropping {'train/loss': 0.2614874243736267}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28809 < 29024; dropping {'train/loss': 0.1719578504562378}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28810 < 29024; dropping {'train/loss': 0.15634414553642273}.\n",
      "Epoch 288 | Batch 10/100 | Loss 0.333724\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28811 < 29024; dropping {'train/loss': 0.4198029041290283}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28812 < 29024; dropping {'train/loss': 0.4921911358833313}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28813 < 29024; dropping {'train/loss': 0.43828368186950684}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28814 < 29024; dropping {'train/loss': 0.3585013449192047}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28815 < 29024; dropping {'train/loss': 0.2640893757343292}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28816 < 29024; dropping {'train/loss': 0.28540509939193726}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28817 < 29024; dropping {'train/loss': 0.33815208077430725}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28818 < 29024; dropping {'train/loss': 0.2305869311094284}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28819 < 29024; dropping {'train/loss': 0.2227320671081543}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28820 < 29024; dropping {'train/loss': 0.18994998931884766}.\n",
      "Epoch 288 | Batch 20/100 | Loss 0.328847\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28821 < 29024; dropping {'train/loss': 0.14517655968666077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28822 < 29024; dropping {'train/loss': 0.29972589015960693}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28823 < 29024; dropping {'train/loss': 0.35255464911460876}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28824 < 29024; dropping {'train/loss': 0.627034068107605}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28825 < 29024; dropping {'train/loss': 0.14886724948883057}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28826 < 29024; dropping {'train/loss': 0.4461071491241455}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28827 < 29024; dropping {'train/loss': 0.1457156240940094}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28828 < 29024; dropping {'train/loss': 0.14082910120487213}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28829 < 29024; dropping {'train/loss': 0.5232334136962891}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28830 < 29024; dropping {'train/loss': 0.3923054337501526}.\n",
      "Epoch 288 | Batch 30/100 | Loss 0.326616\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28831 < 29024; dropping {'train/loss': 0.14290006458759308}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28832 < 29024; dropping {'train/loss': 0.3019845485687256}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28833 < 29024; dropping {'train/loss': 0.24412424862384796}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28834 < 29024; dropping {'train/loss': 0.29047203063964844}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28835 < 29024; dropping {'train/loss': 0.3006383776664734}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28836 < 29024; dropping {'train/loss': 0.20263080298900604}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28837 < 29024; dropping {'train/loss': 0.3747100532054901}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28838 < 29024; dropping {'train/loss': 0.324081152677536}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28839 < 29024; dropping {'train/loss': 0.18521644175052643}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28840 < 29024; dropping {'train/loss': 0.2986529767513275}.\n",
      "Epoch 288 | Batch 40/100 | Loss 0.311597\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28841 < 29024; dropping {'train/loss': 0.3016003966331482}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28842 < 29024; dropping {'train/loss': 0.5575705766677856}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28843 < 29024; dropping {'train/loss': 0.5470682382583618}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28844 < 29024; dropping {'train/loss': 0.1186503991484642}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28845 < 29024; dropping {'train/loss': 0.2584471106529236}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28846 < 29024; dropping {'train/loss': 0.4051669239997864}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28847 < 29024; dropping {'train/loss': 0.38435834646224976}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28848 < 29024; dropping {'train/loss': 0.359563946723938}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28849 < 29024; dropping {'train/loss': 0.2204103171825409}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28850 < 29024; dropping {'train/loss': 0.3792415261268616}.\n",
      "Epoch 288 | Batch 50/100 | Loss 0.319919\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28851 < 29024; dropping {'train/loss': 0.2922375798225403}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28852 < 29024; dropping {'train/loss': 0.34215348958969116}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28853 < 29024; dropping {'train/loss': 0.20098289847373962}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28854 < 29024; dropping {'train/loss': 0.22337667644023895}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28855 < 29024; dropping {'train/loss': 0.3594823479652405}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28856 < 29024; dropping {'train/loss': 0.3862226605415344}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28857 < 29024; dropping {'train/loss': 0.3194713592529297}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28858 < 29024; dropping {'train/loss': 0.5770248174667358}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28859 < 29024; dropping {'train/loss': 0.45253482460975647}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28860 < 29024; dropping {'train/loss': 0.2392665147781372}.\n",
      "Epoch 288 | Batch 60/100 | Loss 0.323145\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28861 < 29024; dropping {'train/loss': 0.3318313956260681}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28862 < 29024; dropping {'train/loss': 0.4341236650943756}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28863 < 29024; dropping {'train/loss': 0.43856996297836304}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28864 < 29024; dropping {'train/loss': 0.14615920186042786}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28865 < 29024; dropping {'train/loss': 0.22977598011493683}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28866 < 29024; dropping {'train/loss': 0.5172032117843628}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28867 < 29024; dropping {'train/loss': 0.6090258359909058}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28868 < 29024; dropping {'train/loss': 0.25214362144470215}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28869 < 29024; dropping {'train/loss': 0.3670486807823181}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28870 < 29024; dropping {'train/loss': 0.5210925936698914}.\n",
      "Epoch 288 | Batch 70/100 | Loss 0.331939\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28871 < 29024; dropping {'train/loss': 0.43303370475769043}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28872 < 29024; dropping {'train/loss': 0.3391724228858948}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28873 < 29024; dropping {'train/loss': 0.6126394867897034}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28874 < 29024; dropping {'train/loss': 0.21152719855308533}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28875 < 29024; dropping {'train/loss': 0.3729885220527649}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28876 < 29024; dropping {'train/loss': 0.601831316947937}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28877 < 29024; dropping {'train/loss': 0.5306856632232666}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28878 < 29024; dropping {'train/loss': 0.3762672245502472}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28879 < 29024; dropping {'train/loss': 0.25702017545700073}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28880 < 29024; dropping {'train/loss': 0.30641666054725647}.\n",
      "Epoch 288 | Batch 80/100 | Loss 0.340966\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28881 < 29024; dropping {'train/loss': 0.38253456354141235}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28882 < 29024; dropping {'train/loss': 0.34125274419784546}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28883 < 29024; dropping {'train/loss': 0.4144771695137024}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28884 < 29024; dropping {'train/loss': 0.2253182828426361}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28885 < 29024; dropping {'train/loss': 0.35184159874916077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28886 < 29024; dropping {'train/loss': 0.4200381636619568}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28887 < 29024; dropping {'train/loss': 0.2956336736679077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28888 < 29024; dropping {'train/loss': 0.26469796895980835}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28889 < 29024; dropping {'train/loss': 0.28263822197914124}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28890 < 29024; dropping {'train/loss': 0.3080296814441681}.\n",
      "Epoch 288 | Batch 90/100 | Loss 0.339597\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28891 < 29024; dropping {'train/loss': 0.5670369863510132}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28892 < 29024; dropping {'train/loss': 0.3504786789417267}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28893 < 29024; dropping {'train/loss': 0.6205238103866577}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28894 < 29024; dropping {'train/loss': 0.29618194699287415}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28895 < 29024; dropping {'train/loss': 0.33247166872024536}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28896 < 29024; dropping {'train/loss': 0.3897446095943451}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28897 < 29024; dropping {'train/loss': 0.5104008913040161}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28898 < 29024; dropping {'train/loss': 0.07984684407711029}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28899 < 29024; dropping {'train/loss': 0.20036479830741882}.\n",
      "Epoch 288 | Batch 100/100 | Loss 0.342407wandb: WARNING Step must only increase in log calls.  Step 28900 < 29024; dropping {'train/loss': 0.32990363240242004}.\n",
      "\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28901 < 29024; dropping {'train/loss': 0.2347298562526703}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28902 < 29024; dropping {'train/loss': 0.09024752676486969}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28903 < 29024; dropping {'train/loss': 0.22412213683128357}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28904 < 29024; dropping {'train/loss': 0.38533276319503784}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28905 < 29024; dropping {'train/loss': 0.27257290482521057}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28906 < 29024; dropping {'train/loss': 0.2688630223274231}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28907 < 29024; dropping {'train/loss': 0.1797817051410675}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28908 < 29024; dropping {'train/loss': 0.21670517325401306}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28909 < 29024; dropping {'train/loss': 0.2355996072292328}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28910 < 29024; dropping {'train/loss': 0.15561352670192719}.\n",
      "Epoch 289 | Batch 10/100 | Loss 0.226357\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28911 < 29024; dropping {'train/loss': 0.27186718583106995}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28912 < 29024; dropping {'train/loss': 0.7351700067520142}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28913 < 29024; dropping {'train/loss': 0.2829282879829407}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28914 < 29024; dropping {'train/loss': 0.18895933032035828}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28915 < 29024; dropping {'train/loss': 0.5501973032951355}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28916 < 29024; dropping {'train/loss': 0.4269729256629944}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28917 < 29024; dropping {'train/loss': 0.4240999221801758}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28918 < 29024; dropping {'train/loss': 0.19812504947185516}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28919 < 29024; dropping {'train/loss': 0.39098045229911804}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28920 < 29024; dropping {'train/loss': 0.3347829282283783}.\n",
      "Epoch 289 | Batch 20/100 | Loss 0.303383\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28921 < 29024; dropping {'train/loss': 0.20109319686889648}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28922 < 29024; dropping {'train/loss': 0.1681709587574005}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28923 < 29024; dropping {'train/loss': 0.4846019148826599}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28924 < 29024; dropping {'train/loss': 0.3317851424217224}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28925 < 29024; dropping {'train/loss': 0.4600437581539154}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28926 < 29024; dropping {'train/loss': 0.2926446497440338}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28927 < 29024; dropping {'train/loss': 0.3555411696434021}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28928 < 29024; dropping {'train/loss': 0.4935123324394226}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28929 < 29024; dropping {'train/loss': 0.21207401156425476}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28930 < 29024; dropping {'train/loss': 0.2726500630378723}.\n",
      "Epoch 289 | Batch 30/100 | Loss 0.311326\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28931 < 29024; dropping {'train/loss': 0.29607704281806946}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28932 < 29024; dropping {'train/loss': 0.302992582321167}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28933 < 29024; dropping {'train/loss': 0.1872793734073639}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28934 < 29024; dropping {'train/loss': 0.3182242512702942}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28935 < 29024; dropping {'train/loss': 0.27942246198654175}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28936 < 29024; dropping {'train/loss': 0.2861187756061554}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28937 < 29024; dropping {'train/loss': 0.4164406657218933}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28938 < 29024; dropping {'train/loss': 0.2035442590713501}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28939 < 29024; dropping {'train/loss': 0.20305165648460388}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28940 < 29024; dropping {'train/loss': 0.2850079834461212}.\n",
      "Epoch 289 | Batch 40/100 | Loss 0.302948\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28941 < 29024; dropping {'train/loss': 0.5276182889938354}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28942 < 29024; dropping {'train/loss': 0.3145914673805237}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28943 < 29024; dropping {'train/loss': 0.4768865704536438}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28944 < 29024; dropping {'train/loss': 0.3198557496070862}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28945 < 29024; dropping {'train/loss': 0.5259274840354919}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28946 < 29024; dropping {'train/loss': 0.35381168127059937}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28947 < 29024; dropping {'train/loss': 0.3949190080165863}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28948 < 29024; dropping {'train/loss': 0.27287086844444275}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28949 < 29024; dropping {'train/loss': 0.4318665862083435}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28950 < 29024; dropping {'train/loss': 0.42001527547836304}.\n",
      "Epoch 289 | Batch 50/100 | Loss 0.323126\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28951 < 29024; dropping {'train/loss': 0.4392954409122467}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28952 < 29024; dropping {'train/loss': 0.415492445230484}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28953 < 29024; dropping {'train/loss': 0.3240721523761749}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28954 < 29024; dropping {'train/loss': 0.34873002767562866}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28955 < 29024; dropping {'train/loss': 0.28261178731918335}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28956 < 29024; dropping {'train/loss': 0.2353270947933197}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28957 < 29024; dropping {'train/loss': 0.30598780512809753}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28958 < 29024; dropping {'train/loss': 0.2211368978023529}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28959 < 29024; dropping {'train/loss': 0.3853203356266022}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28960 < 29024; dropping {'train/loss': 0.33789709210395813}.\n",
      "Epoch 289 | Batch 60/100 | Loss 0.324203\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28961 < 29024; dropping {'train/loss': 0.287438303232193}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28962 < 29024; dropping {'train/loss': 0.44818025827407837}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28963 < 29024; dropping {'train/loss': 0.7753208875656128}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28964 < 29024; dropping {'train/loss': 0.42011094093322754}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28965 < 29024; dropping {'train/loss': 0.5875149965286255}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28966 < 29024; dropping {'train/loss': 0.3250654637813568}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28967 < 29024; dropping {'train/loss': 0.3824799656867981}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28968 < 29024; dropping {'train/loss': 0.31659454107284546}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28969 < 29024; dropping {'train/loss': 0.36079466342926025}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28970 < 29024; dropping {'train/loss': 0.7619255781173706}.\n",
      "Epoch 289 | Batch 70/100 | Loss 0.344537\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28971 < 29024; dropping {'train/loss': 0.21206681430339813}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28972 < 29024; dropping {'train/loss': 0.13364091515541077}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28973 < 29024; dropping {'train/loss': 0.3117424547672272}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28974 < 29024; dropping {'train/loss': 0.2659887373447418}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28975 < 29024; dropping {'train/loss': 0.326504111289978}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28976 < 29024; dropping {'train/loss': 0.3022787570953369}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28977 < 29024; dropping {'train/loss': 0.3403068482875824}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28978 < 29024; dropping {'train/loss': 0.44590073823928833}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28979 < 29024; dropping {'train/loss': 0.4603975713253021}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28980 < 29024; dropping {'train/loss': 0.47414177656173706}.\n",
      "Epoch 289 | Batch 80/100 | Loss 0.342382\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28981 < 29024; dropping {'train/loss': 0.42417973279953003}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28982 < 29024; dropping {'train/loss': 0.3035431504249573}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28983 < 29024; dropping {'train/loss': 0.18645277619361877}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28984 < 29024; dropping {'train/loss': 0.2509475648403168}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28985 < 29024; dropping {'train/loss': 0.1732620894908905}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28986 < 29024; dropping {'train/loss': 0.33741357922554016}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28987 < 29024; dropping {'train/loss': 0.2807635962963104}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28988 < 29024; dropping {'train/loss': 0.18476012349128723}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28989 < 29024; dropping {'train/loss': 0.22400423884391785}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28990 < 29024; dropping {'train/loss': 0.3853972256183624}.\n",
      "Epoch 289 | Batch 90/100 | Loss 0.334903\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28991 < 29024; dropping {'train/loss': 0.34397926926612854}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28992 < 29024; dropping {'train/loss': 0.5041126012802124}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28993 < 29024; dropping {'train/loss': 0.7687419652938843}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28994 < 29024; dropping {'train/loss': 0.37351903319358826}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28995 < 29024; dropping {'train/loss': 0.4360610544681549}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28996 < 29024; dropping {'train/loss': 0.254533588886261}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28997 < 29024; dropping {'train/loss': 0.22177176177501678}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28998 < 29024; dropping {'train/loss': 0.31515297293663025}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 28999 < 29024; dropping {'train/loss': 0.37909460067749023}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29000 < 29024; dropping {'train/loss': 0.29606348276138306}.\n",
      "Epoch 289 | Batch 100/100 | Loss 0.340343\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29001 < 29024; dropping {'train/loss': 0.19944702088832855}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29002 < 29024; dropping {'train/loss': 0.2774200439453125}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29003 < 29024; dropping {'train/loss': 0.15306563675403595}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29004 < 29024; dropping {'train/loss': 0.5475567579269409}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29005 < 29024; dropping {'train/loss': 0.29998624324798584}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29006 < 29024; dropping {'train/loss': 0.39499807357788086}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29007 < 29024; dropping {'train/loss': 0.43029507994651794}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29008 < 29024; dropping {'train/loss': 0.467184841632843}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29009 < 29024; dropping {'train/loss': 0.3633784353733063}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29010 < 29024; dropping {'train/loss': 0.5689516663551331}.\n",
      "Epoch 290 | Batch 10/100 | Loss 0.370228\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29011 < 29024; dropping {'train/loss': 0.2236596643924713}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29012 < 29024; dropping {'train/loss': 0.4696560502052307}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29013 < 29024; dropping {'train/loss': 0.32402628660202026}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29014 < 29024; dropping {'train/loss': 0.3905884921550751}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29015 < 29024; dropping {'train/loss': 0.2406630516052246}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29016 < 29024; dropping {'train/loss': 0.3593359589576721}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29017 < 29024; dropping {'train/loss': 0.40573129057884216}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29018 < 29024; dropping {'train/loss': 0.5210844278335571}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29019 < 29024; dropping {'train/loss': 0.21991264820098877}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29020 < 29024; dropping {'train/loss': 0.23227104544639587}.\n",
      "Epoch 290 | Batch 20/100 | Loss 0.354461\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29021 < 29024; dropping {'train/loss': 0.4569489061832428}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29022 < 29024; dropping {'train/loss': 0.386843740940094}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 29023 < 29024; dropping {'train/loss': 0.32231947779655457}.\n",
      "Epoch 290 | Batch 30/100 | Loss 0.355081\n",
      "Epoch 290 | Batch 40/100 | Loss 0.357919\n",
      "Epoch 290 | Batch 50/100 | Loss 0.350594\n",
      "Epoch 290 | Batch 60/100 | Loss 0.341737\n",
      "Epoch 290 | Batch 70/100 | Loss 0.333961\n",
      "Epoch 290 | Batch 80/100 | Loss 0.336390\n",
      "Epoch 290 | Batch 90/100 | Loss 0.343468\n",
      "Epoch 290 | Batch 100/100 | Loss 0.343769\n",
      "Epoch 291 | Batch 10/100 | Loss 0.370179\n",
      "Epoch 291 | Batch 20/100 | Loss 0.382247\n",
      "Epoch 291 | Batch 30/100 | Loss 0.382242\n",
      "Epoch 291 | Batch 40/100 | Loss 0.357013\n",
      "Epoch 291 | Batch 50/100 | Loss 0.354553\n",
      "Epoch 291 | Batch 60/100 | Loss 0.345328\n",
      "Epoch 291 | Batch 70/100 | Loss 0.344084\n",
      "Epoch 291 | Batch 80/100 | Loss 0.351682\n",
      "Epoch 291 | Batch 90/100 | Loss 0.354722\n",
      "Epoch 291 | Batch 100/100 | Loss 0.358138\n",
      "Epoch 292 | Batch 10/100 | Loss 0.391077\n",
      "Epoch 292 | Batch 20/100 | Loss 0.369643\n",
      "Epoch 292 | Batch 30/100 | Loss 0.361128\n",
      "Epoch 292 | Batch 40/100 | Loss 0.386853\n",
      "Epoch 292 | Batch 50/100 | Loss 0.399593\n",
      "Epoch 292 | Batch 60/100 | Loss 0.391539\n",
      "Epoch 292 | Batch 70/100 | Loss 0.390275\n",
      "Epoch 292 | Batch 80/100 | Loss 0.393108\n",
      "Epoch 292 | Batch 90/100 | Loss 0.391287\n",
      "Epoch 292 | Batch 100/100 | Loss 0.388846\n",
      "Epoch 293 | Batch 10/100 | Loss 0.326920\n",
      "Epoch 293 | Batch 20/100 | Loss 0.352976\n",
      "Epoch 293 | Batch 30/100 | Loss 0.362963\n",
      "Epoch 293 | Batch 40/100 | Loss 0.369136\n",
      "Epoch 293 | Batch 50/100 | Loss 0.363826\n",
      "Epoch 293 | Batch 60/100 | Loss 0.359579\n",
      "Epoch 293 | Batch 70/100 | Loss 0.359165\n",
      "Epoch 293 | Batch 80/100 | Loss 0.362191\n",
      "Epoch 293 | Batch 90/100 | Loss 0.364434\n",
      "Epoch 293 | Batch 100/100 | Loss 0.361427\n",
      "Epoch 294 | Batch 10/100 | Loss 0.388797\n",
      "Epoch 294 | Batch 20/100 | Loss 0.337440\n",
      "Epoch 294 | Batch 30/100 | Loss 0.347384\n",
      "Epoch 294 | Batch 40/100 | Loss 0.348128\n",
      "Epoch 294 | Batch 50/100 | Loss 0.335930\n",
      "Epoch 294 | Batch 60/100 | Loss 0.341718\n",
      "Epoch 294 | Batch 70/100 | Loss 0.341734\n",
      "Epoch 294 | Batch 80/100 | Loss 0.348223\n",
      "Epoch 294 | Batch 90/100 | Loss 0.343298\n",
      "Epoch 294 | Batch 100/100 | Loss 0.343454\n",
      "Epoch 295 | Batch 10/100 | Loss 0.355245\n",
      "Epoch 295 | Batch 20/100 | Loss 0.355325\n",
      "Epoch 295 | Batch 30/100 | Loss 0.334784\n",
      "Epoch 295 | Batch 40/100 | Loss 0.332606\n",
      "Epoch 295 | Batch 50/100 | Loss 0.335814\n",
      "Epoch 295 | Batch 60/100 | Loss 0.338275\n",
      "Epoch 295 | Batch 70/100 | Loss 0.343103\n",
      "Epoch 295 | Batch 80/100 | Loss 0.342953\n",
      "Epoch 295 | Batch 90/100 | Loss 0.335005\n",
      "Epoch 295 | Batch 100/100 | Loss 0.341054\n",
      "Epoch 296 | Batch 10/100 | Loss 0.356025\n",
      "Epoch 296 | Batch 20/100 | Loss 0.366825\n",
      "Epoch 296 | Batch 30/100 | Loss 0.360456\n",
      "Epoch 296 | Batch 40/100 | Loss 0.338986\n",
      "Epoch 296 | Batch 50/100 | Loss 0.334195\n",
      "Epoch 296 | Batch 60/100 | Loss 0.331414\n",
      "Epoch 296 | Batch 70/100 | Loss 0.337538\n",
      "Epoch 296 | Batch 80/100 | Loss 0.340069\n",
      "Epoch 296 | Batch 90/100 | Loss 0.345914\n",
      "Epoch 296 | Batch 100/100 | Loss 0.351218\n",
      "Epoch 297 | Batch 10/100 | Loss 0.302022\n",
      "Epoch 297 | Batch 20/100 | Loss 0.286439\n",
      "Epoch 297 | Batch 30/100 | Loss 0.309951\n",
      "Epoch 297 | Batch 40/100 | Loss 0.326487\n",
      "Epoch 297 | Batch 50/100 | Loss 0.336325\n",
      "Epoch 297 | Batch 60/100 | Loss 0.339947\n",
      "Epoch 297 | Batch 70/100 | Loss 0.340099\n",
      "Epoch 297 | Batch 80/100 | Loss 0.344711\n",
      "Epoch 297 | Batch 90/100 | Loss 0.346764\n",
      "Epoch 297 | Batch 100/100 | Loss 0.349153\n",
      "Epoch 298 | Batch 10/100 | Loss 0.405973\n",
      "Epoch 298 | Batch 20/100 | Loss 0.424509\n",
      "Epoch 298 | Batch 30/100 | Loss 0.417493\n",
      "Epoch 298 | Batch 40/100 | Loss 0.388581\n",
      "Epoch 298 | Batch 50/100 | Loss 0.372949\n",
      "Epoch 298 | Batch 60/100 | Loss 0.368737\n",
      "Epoch 298 | Batch 70/100 | Loss 0.359845\n",
      "Epoch 298 | Batch 80/100 | Loss 0.355616\n",
      "Epoch 298 | Batch 90/100 | Loss 0.360339\n",
      "Epoch 298 | Batch 100/100 | Loss 0.355449\n",
      "Epoch 299 | Batch 10/100 | Loss 0.335172\n",
      "Epoch 299 | Batch 20/100 | Loss 0.336346\n",
      "Epoch 299 | Batch 30/100 | Loss 0.341088\n",
      "Epoch 299 | Batch 40/100 | Loss 0.334457\n",
      "Epoch 299 | Batch 50/100 | Loss 0.341713\n",
      "Epoch 299 | Batch 60/100 | Loss 0.340744\n",
      "Epoch 299 | Batch 70/100 | Loss 0.337729\n",
      "Epoch 299 | Batch 80/100 | Loss 0.341871\n",
      "Epoch 299 | Batch 90/100 | Loss 0.334933\n",
      "Epoch 299 | Batch 100/100 | Loss 0.333364\n",
      "Epoch 300 | Batch 10/100 | Loss 0.382874\n",
      "Epoch 300 | Batch 20/100 | Loss 0.381133\n",
      "Epoch 300 | Batch 30/100 | Loss 0.363409\n",
      "Epoch 300 | Batch 40/100 | Loss 0.365724\n",
      "Epoch 300 | Batch 50/100 | Loss 0.362323\n",
      "Epoch 300 | Batch 60/100 | Loss 0.357774\n",
      "Epoch 300 | Batch 70/100 | Loss 0.350870\n",
      "Epoch 300 | Batch 80/100 | Loss 0.347900\n",
      "Epoch 300 | Batch 90/100 | Loss 0.353960\n",
      "Epoch 300 | Batch 100/100 | Loss 0.357533\n",
      "Epoch 301 | Batch 10/100 | Loss 0.260103\n",
      "Epoch 301 | Batch 20/100 | Loss 0.306934\n",
      "Epoch 301 | Batch 30/100 | Loss 0.302778\n",
      "Epoch 301 | Batch 40/100 | Loss 0.312456\n",
      "Epoch 301 | Batch 50/100 | Loss 0.330684\n",
      "Epoch 301 | Batch 60/100 | Loss 0.340506\n",
      "Epoch 301 | Batch 70/100 | Loss 0.336181\n",
      "Epoch 301 | Batch 80/100 | Loss 0.333398\n",
      "Epoch 301 | Batch 90/100 | Loss 0.331610\n",
      "Epoch 301 | Batch 100/100 | Loss 0.331322\n",
      "100 Test Protonet Acc = 81.12% +- 1.41%\n",
      "Epoch 302 | Batch 10/100 | Loss 0.367551\n",
      "Epoch 302 | Batch 20/100 | Loss 0.368513\n",
      "Epoch 302 | Batch 30/100 | Loss 0.358075\n",
      "Epoch 302 | Batch 40/100 | Loss 0.380661\n",
      "Epoch 302 | Batch 50/100 | Loss 0.375965\n",
      "Epoch 302 | Batch 60/100 | Loss 0.374752\n",
      "Epoch 302 | Batch 70/100 | Loss 0.362878\n",
      "Epoch 302 | Batch 80/100 | Loss 0.366090\n",
      "Epoch 302 | Batch 90/100 | Loss 0.359499\n",
      "Epoch 302 | Batch 100/100 | Loss 0.361381\n",
      "Epoch 303 | Batch 10/100 | Loss 0.323346\n",
      "Epoch 303 | Batch 20/100 | Loss 0.339084\n",
      "Epoch 303 | Batch 30/100 | Loss 0.339738\n",
      "Epoch 303 | Batch 40/100 | Loss 0.346759\n",
      "Epoch 303 | Batch 50/100 | Loss 0.348359\n",
      "Epoch 303 | Batch 60/100 | Loss 0.345399\n",
      "Epoch 303 | Batch 70/100 | Loss 0.351835\n",
      "Epoch 303 | Batch 80/100 | Loss 0.345034\n",
      "Epoch 303 | Batch 90/100 | Loss 0.342946\n",
      "Epoch 303 | Batch 100/100 | Loss 0.347591\n",
      "Epoch 304 | Batch 10/100 | Loss 0.408228\n",
      "Epoch 304 | Batch 20/100 | Loss 0.348701\n",
      "Epoch 304 | Batch 30/100 | Loss 0.371243\n",
      "Epoch 304 | Batch 40/100 | Loss 0.362468\n",
      "Epoch 304 | Batch 50/100 | Loss 0.351137\n",
      "Epoch 304 | Batch 60/100 | Loss 0.344113\n",
      "Epoch 304 | Batch 70/100 | Loss 0.344053\n",
      "Epoch 304 | Batch 80/100 | Loss 0.344685\n",
      "Epoch 304 | Batch 90/100 | Loss 0.341626\n",
      "Epoch 304 | Batch 100/100 | Loss 0.329792\n",
      "Epoch 305 | Batch 10/100 | Loss 0.299506\n",
      "Epoch 305 | Batch 20/100 | Loss 0.336869\n",
      "Epoch 305 | Batch 30/100 | Loss 0.325196\n",
      "Epoch 305 | Batch 40/100 | Loss 0.334521\n",
      "Epoch 305 | Batch 50/100 | Loss 0.333627\n",
      "Epoch 305 | Batch 60/100 | Loss 0.327494\n",
      "Epoch 305 | Batch 70/100 | Loss 0.334984\n",
      "Epoch 305 | Batch 80/100 | Loss 0.342762\n",
      "Epoch 305 | Batch 90/100 | Loss 0.339581\n",
      "Epoch 305 | Batch 100/100 | Loss 0.343464\n",
      "Epoch 306 | Batch 10/100 | Loss 0.382176\n",
      "Epoch 306 | Batch 20/100 | Loss 0.374376\n",
      "Epoch 306 | Batch 30/100 | Loss 0.371667\n",
      "Epoch 306 | Batch 40/100 | Loss 0.352751\n",
      "Epoch 306 | Batch 50/100 | Loss 0.340950\n",
      "Epoch 306 | Batch 60/100 | Loss 0.341623\n",
      "Epoch 306 | Batch 70/100 | Loss 0.333664\n",
      "Epoch 306 | Batch 80/100 | Loss 0.341202\n",
      "Epoch 306 | Batch 90/100 | Loss 0.345589\n",
      "Epoch 306 | Batch 100/100 | Loss 0.345398\n",
      "Epoch 307 | Batch 10/100 | Loss 0.318528\n",
      "Epoch 307 | Batch 20/100 | Loss 0.323996\n",
      "Epoch 307 | Batch 30/100 | Loss 0.325907\n",
      "Epoch 307 | Batch 40/100 | Loss 0.300759\n",
      "Epoch 307 | Batch 50/100 | Loss 0.298434\n",
      "Epoch 307 | Batch 60/100 | Loss 0.312688\n",
      "Epoch 307 | Batch 70/100 | Loss 0.310546\n",
      "Epoch 307 | Batch 80/100 | Loss 0.309481\n",
      "Epoch 307 | Batch 90/100 | Loss 0.314605\n",
      "Epoch 307 | Batch 100/100 | Loss 0.315261\n",
      "Epoch 308 | Batch 10/100 | Loss 0.266396\n",
      "Epoch 308 | Batch 20/100 | Loss 0.289148\n",
      "Epoch 308 | Batch 30/100 | Loss 0.311721\n",
      "Epoch 308 | Batch 40/100 | Loss 0.323502\n",
      "Epoch 308 | Batch 50/100 | Loss 0.326231\n",
      "Epoch 308 | Batch 60/100 | Loss 0.334938\n",
      "Epoch 308 | Batch 70/100 | Loss 0.333875\n",
      "Epoch 308 | Batch 80/100 | Loss 0.332090\n",
      "Epoch 308 | Batch 90/100 | Loss 0.335753\n",
      "Epoch 308 | Batch 100/100 | Loss 0.336434\n",
      "Epoch 309 | Batch 10/100 | Loss 0.426823\n",
      "Epoch 309 | Batch 20/100 | Loss 0.361693\n",
      "Epoch 309 | Batch 30/100 | Loss 0.351662\n",
      "Epoch 309 | Batch 40/100 | Loss 0.369308\n",
      "Epoch 309 | Batch 50/100 | Loss 0.351153\n",
      "Epoch 309 | Batch 60/100 | Loss 0.354830\n",
      "Epoch 309 | Batch 70/100 | Loss 0.354429\n",
      "Epoch 309 | Batch 80/100 | Loss 0.367624\n",
      "Epoch 309 | Batch 90/100 | Loss 0.359716\n",
      "Epoch 309 | Batch 100/100 | Loss 0.357622\n",
      "Epoch 310 | Batch 10/100 | Loss 0.335399\n",
      "Epoch 310 | Batch 20/100 | Loss 0.328086\n",
      "Epoch 310 | Batch 30/100 | Loss 0.326365\n",
      "Epoch 310 | Batch 40/100 | Loss 0.326885\n",
      "Epoch 310 | Batch 50/100 | Loss 0.334672\n",
      "Epoch 310 | Batch 60/100 | Loss 0.335165\n",
      "Epoch 310 | Batch 70/100 | Loss 0.343742\n",
      "Epoch 310 | Batch 80/100 | Loss 0.346868\n",
      "Epoch 310 | Batch 90/100 | Loss 0.342038\n",
      "Epoch 310 | Batch 100/100 | Loss 0.335556\n",
      "Epoch 311 | Batch 10/100 | Loss 0.358164\n",
      "Epoch 311 | Batch 20/100 | Loss 0.385823\n",
      "Epoch 311 | Batch 30/100 | Loss 0.370549\n",
      "Epoch 311 | Batch 40/100 | Loss 0.358049\n",
      "Epoch 311 | Batch 50/100 | Loss 0.346936\n",
      "Epoch 311 | Batch 60/100 | Loss 0.343359\n",
      "Epoch 311 | Batch 70/100 | Loss 0.343322\n",
      "Epoch 311 | Batch 80/100 | Loss 0.342240\n",
      "Epoch 311 | Batch 90/100 | Loss 0.340672\n",
      "Epoch 311 | Batch 100/100 | Loss 0.337031\n",
      "Epoch 312 | Batch 10/100 | Loss 0.326837\n",
      "Epoch 312 | Batch 20/100 | Loss 0.338711\n",
      "Epoch 312 | Batch 30/100 | Loss 0.322289\n",
      "Epoch 312 | Batch 40/100 | Loss 0.335744\n",
      "Epoch 312 | Batch 50/100 | Loss 0.339233\n",
      "Epoch 312 | Batch 60/100 | Loss 0.334070\n",
      "Epoch 312 | Batch 70/100 | Loss 0.338772\n",
      "Epoch 312 | Batch 80/100 | Loss 0.331282\n",
      "Epoch 312 | Batch 90/100 | Loss 0.325920\n",
      "Epoch 312 | Batch 100/100 | Loss 0.330968\n",
      "Epoch 313 | Batch 10/100 | Loss 0.301732\n",
      "Epoch 313 | Batch 20/100 | Loss 0.289383\n",
      "Epoch 313 | Batch 30/100 | Loss 0.278777\n",
      "Epoch 313 | Batch 40/100 | Loss 0.293893\n",
      "Epoch 313 | Batch 50/100 | Loss 0.312557\n",
      "Epoch 313 | Batch 60/100 | Loss 0.329686\n",
      "Epoch 313 | Batch 70/100 | Loss 0.323409\n",
      "Epoch 313 | Batch 80/100 | Loss 0.325664\n",
      "Epoch 313 | Batch 90/100 | Loss 0.327651\n",
      "Epoch 313 | Batch 100/100 | Loss 0.327233\n",
      "Epoch 314 | Batch 10/100 | Loss 0.290343\n",
      "Epoch 314 | Batch 20/100 | Loss 0.303325\n",
      "Epoch 314 | Batch 30/100 | Loss 0.288282\n",
      "Epoch 314 | Batch 40/100 | Loss 0.287252\n",
      "Epoch 314 | Batch 50/100 | Loss 0.300727\n",
      "Epoch 314 | Batch 60/100 | Loss 0.324241\n",
      "Epoch 314 | Batch 70/100 | Loss 0.318267\n",
      "Epoch 314 | Batch 80/100 | Loss 0.314480\n",
      "Epoch 314 | Batch 90/100 | Loss 0.315998\n",
      "Epoch 314 | Batch 100/100 | Loss 0.320299\n",
      "Epoch 315 | Batch 10/100 | Loss 0.433680\n",
      "Epoch 315 | Batch 20/100 | Loss 0.365329\n",
      "Epoch 315 | Batch 30/100 | Loss 0.350588\n",
      "Epoch 315 | Batch 40/100 | Loss 0.348627\n",
      "Epoch 315 | Batch 50/100 | Loss 0.365376\n",
      "Epoch 315 | Batch 60/100 | Loss 0.348608\n",
      "Epoch 315 | Batch 70/100 | Loss 0.344751\n",
      "Epoch 315 | Batch 80/100 | Loss 0.340425\n",
      "Epoch 315 | Batch 90/100 | Loss 0.331344\n",
      "Epoch 315 | Batch 100/100 | Loss 0.335103\n",
      "Epoch 316 | Batch 10/100 | Loss 0.325349\n",
      "Epoch 316 | Batch 20/100 | Loss 0.316762\n",
      "Epoch 316 | Batch 30/100 | Loss 0.331248\n",
      "Epoch 316 | Batch 40/100 | Loss 0.326812\n",
      "Epoch 316 | Batch 50/100 | Loss 0.319213\n",
      "Epoch 316 | Batch 60/100 | Loss 0.319703\n",
      "Epoch 316 | Batch 70/100 | Loss 0.318047\n",
      "Epoch 316 | Batch 80/100 | Loss 0.312756\n",
      "Epoch 316 | Batch 90/100 | Loss 0.310569\n",
      "Epoch 316 | Batch 100/100 | Loss 0.315890\n",
      "Epoch 317 | Batch 10/100 | Loss 0.275032\n",
      "Epoch 317 | Batch 20/100 | Loss 0.331739\n",
      "Epoch 317 | Batch 30/100 | Loss 0.323078\n",
      "Epoch 317 | Batch 40/100 | Loss 0.326400\n",
      "Epoch 317 | Batch 50/100 | Loss 0.335844\n",
      "Epoch 317 | Batch 60/100 | Loss 0.333960\n",
      "Epoch 317 | Batch 70/100 | Loss 0.331834\n",
      "Epoch 317 | Batch 80/100 | Loss 0.318833\n",
      "Epoch 317 | Batch 90/100 | Loss 0.321590\n",
      "Epoch 317 | Batch 100/100 | Loss 0.319464\n",
      "Epoch 318 | Batch 10/100 | Loss 0.430570\n",
      "Epoch 318 | Batch 20/100 | Loss 0.369551\n",
      "Epoch 318 | Batch 30/100 | Loss 0.361564\n",
      "Epoch 318 | Batch 40/100 | Loss 0.354328\n",
      "Epoch 318 | Batch 50/100 | Loss 0.345901\n",
      "Epoch 318 | Batch 60/100 | Loss 0.350117\n",
      "Epoch 318 | Batch 70/100 | Loss 0.343876\n",
      "Epoch 318 | Batch 80/100 | Loss 0.341121\n",
      "Epoch 318 | Batch 90/100 | Loss 0.339364\n",
      "Epoch 318 | Batch 100/100 | Loss 0.336428\n",
      "Epoch 319 | Batch 10/100 | Loss 0.306243\n",
      "Epoch 319 | Batch 20/100 | Loss 0.296933\n",
      "Epoch 319 | Batch 30/100 | Loss 0.292418\n",
      "Epoch 319 | Batch 40/100 | Loss 0.297600\n",
      "Epoch 319 | Batch 50/100 | Loss 0.306005\n",
      "Epoch 319 | Batch 60/100 | Loss 0.318540\n",
      "Epoch 319 | Batch 70/100 | Loss 0.321618\n",
      "Epoch 319 | Batch 80/100 | Loss 0.316365\n",
      "Epoch 319 | Batch 90/100 | Loss 0.314444\n",
      "Epoch 319 | Batch 100/100 | Loss 0.317858\n",
      "Epoch 320 | Batch 10/100 | Loss 0.288217\n",
      "Epoch 320 | Batch 20/100 | Loss 0.297630\n",
      "Epoch 320 | Batch 30/100 | Loss 0.302930\n",
      "Epoch 320 | Batch 40/100 | Loss 0.305683\n",
      "Epoch 320 | Batch 50/100 | Loss 0.310037\n",
      "Epoch 320 | Batch 60/100 | Loss 0.306921\n",
      "Epoch 320 | Batch 70/100 | Loss 0.306738\n",
      "Epoch 320 | Batch 80/100 | Loss 0.307070\n",
      "Epoch 320 | Batch 90/100 | Loss 0.303641\n",
      "Epoch 320 | Batch 100/100 | Loss 0.302251\n",
      "Epoch 321 | Batch 10/100 | Loss 0.310739\n",
      "Epoch 321 | Batch 20/100 | Loss 0.293217\n",
      "Epoch 321 | Batch 30/100 | Loss 0.281145\n",
      "Epoch 321 | Batch 40/100 | Loss 0.296579\n",
      "Epoch 321 | Batch 50/100 | Loss 0.309157\n",
      "Epoch 321 | Batch 60/100 | Loss 0.305898\n",
      "Epoch 321 | Batch 70/100 | Loss 0.298772\n",
      "Epoch 321 | Batch 80/100 | Loss 0.304508\n",
      "Epoch 321 | Batch 90/100 | Loss 0.307602\n",
      "Epoch 321 | Batch 100/100 | Loss 0.309267\n",
      "100 Test Protonet Acc = 81.38% +- 1.49%\n",
      "best model! save...\n",
      "Epoch 322 | Batch 10/100 | Loss 0.276177\n",
      "Epoch 322 | Batch 20/100 | Loss 0.278912\n",
      "Epoch 322 | Batch 30/100 | Loss 0.299455\n",
      "Epoch 322 | Batch 40/100 | Loss 0.288663\n",
      "Epoch 322 | Batch 50/100 | Loss 0.293039\n",
      "Epoch 322 | Batch 60/100 | Loss 0.293378\n",
      "Epoch 322 | Batch 70/100 | Loss 0.296599\n",
      "Epoch 322 | Batch 80/100 | Loss 0.295043\n",
      "Epoch 322 | Batch 90/100 | Loss 0.297063\n",
      "Epoch 322 | Batch 100/100 | Loss 0.292136\n",
      "Epoch 323 | Batch 10/100 | Loss 0.286017\n",
      "Epoch 323 | Batch 20/100 | Loss 0.267765\n",
      "Epoch 323 | Batch 30/100 | Loss 0.286503\n",
      "Epoch 323 | Batch 40/100 | Loss 0.297551\n",
      "Epoch 323 | Batch 50/100 | Loss 0.302789\n",
      "Epoch 323 | Batch 60/100 | Loss 0.296351\n",
      "Epoch 323 | Batch 70/100 | Loss 0.297619\n",
      "Epoch 323 | Batch 80/100 | Loss 0.291197\n",
      "Epoch 323 | Batch 90/100 | Loss 0.299470\n",
      "Epoch 323 | Batch 100/100 | Loss 0.304336\n",
      "Epoch 324 | Batch 10/100 | Loss 0.326990\n",
      "Epoch 324 | Batch 20/100 | Loss 0.339721\n",
      "Epoch 324 | Batch 30/100 | Loss 0.351057\n",
      "Epoch 324 | Batch 40/100 | Loss 0.339441\n",
      "Epoch 324 | Batch 50/100 | Loss 0.327084\n",
      "Epoch 324 | Batch 60/100 | Loss 0.315426\n",
      "Epoch 324 | Batch 70/100 | Loss 0.309499\n",
      "Epoch 324 | Batch 80/100 | Loss 0.306376\n",
      "Epoch 324 | Batch 90/100 | Loss 0.314321\n",
      "Epoch 324 | Batch 100/100 | Loss 0.317858\n",
      "Epoch 325 | Batch 10/100 | Loss 0.296663\n",
      "Epoch 325 | Batch 20/100 | Loss 0.319838\n",
      "Epoch 325 | Batch 30/100 | Loss 0.305816\n",
      "Epoch 325 | Batch 40/100 | Loss 0.315680\n",
      "Epoch 325 | Batch 50/100 | Loss 0.326610\n",
      "Epoch 325 | Batch 60/100 | Loss 0.330062\n",
      "Epoch 325 | Batch 70/100 | Loss 0.328895\n",
      "Epoch 325 | Batch 80/100 | Loss 0.326516\n",
      "Epoch 325 | Batch 90/100 | Loss 0.327652\n",
      "Epoch 325 | Batch 100/100 | Loss 0.326666\n",
      "Epoch 326 | Batch 10/100 | Loss 0.328349\n",
      "Epoch 326 | Batch 20/100 | Loss 0.293123\n",
      "Epoch 326 | Batch 30/100 | Loss 0.306570\n",
      "Epoch 326 | Batch 40/100 | Loss 0.310059\n",
      "Epoch 326 | Batch 50/100 | Loss 0.316178\n",
      "Epoch 326 | Batch 60/100 | Loss 0.314465\n",
      "Epoch 326 | Batch 70/100 | Loss 0.309953\n",
      "Epoch 326 | Batch 80/100 | Loss 0.316523\n",
      "Epoch 326 | Batch 90/100 | Loss 0.317070\n",
      "Epoch 326 | Batch 100/100 | Loss 0.316751\n",
      "Epoch 327 | Batch 10/100 | Loss 0.344858\n",
      "Epoch 327 | Batch 20/100 | Loss 0.349643\n",
      "Epoch 327 | Batch 30/100 | Loss 0.360852\n",
      "Epoch 327 | Batch 40/100 | Loss 0.344865\n",
      "Epoch 327 | Batch 50/100 | Loss 0.330444\n",
      "Epoch 327 | Batch 60/100 | Loss 0.328793\n",
      "Epoch 327 | Batch 70/100 | Loss 0.331039\n",
      "Epoch 327 | Batch 80/100 | Loss 0.329605\n",
      "Epoch 327 | Batch 90/100 | Loss 0.331688\n",
      "Epoch 327 | Batch 100/100 | Loss 0.330601\n",
      "Epoch 328 | Batch 10/100 | Loss 0.305659\n",
      "Epoch 328 | Batch 20/100 | Loss 0.287445\n",
      "Epoch 328 | Batch 30/100 | Loss 0.284333\n",
      "Epoch 328 | Batch 40/100 | Loss 0.285551\n",
      "Epoch 328 | Batch 50/100 | Loss 0.281163\n",
      "Epoch 328 | Batch 60/100 | Loss 0.272770\n",
      "Epoch 328 | Batch 70/100 | Loss 0.283224\n",
      "Epoch 328 | Batch 80/100 | Loss 0.285018\n",
      "Epoch 328 | Batch 90/100 | Loss 0.288561\n",
      "Epoch 328 | Batch 100/100 | Loss 0.290841\n",
      "Epoch 329 | Batch 10/100 | Loss 0.320226\n",
      "Epoch 329 | Batch 20/100 | Loss 0.348128\n",
      "Epoch 329 | Batch 30/100 | Loss 0.336705\n",
      "Epoch 329 | Batch 40/100 | Loss 0.334697\n",
      "Epoch 329 | Batch 50/100 | Loss 0.320280\n",
      "Epoch 329 | Batch 60/100 | Loss 0.322932\n",
      "Epoch 329 | Batch 70/100 | Loss 0.326391\n",
      "Epoch 329 | Batch 80/100 | Loss 0.321912\n",
      "Epoch 329 | Batch 90/100 | Loss 0.321972\n",
      "Epoch 329 | Batch 100/100 | Loss 0.320810\n",
      "Epoch 330 | Batch 10/100 | Loss 0.335000\n",
      "Epoch 330 | Batch 20/100 | Loss 0.335382\n",
      "Epoch 330 | Batch 30/100 | Loss 0.347081\n",
      "Epoch 330 | Batch 40/100 | Loss 0.358690\n",
      "Epoch 330 | Batch 50/100 | Loss 0.352326\n",
      "Epoch 330 | Batch 60/100 | Loss 0.353442\n",
      "Epoch 330 | Batch 70/100 | Loss 0.360143\n",
      "Epoch 330 | Batch 80/100 | Loss 0.361132\n",
      "Epoch 330 | Batch 90/100 | Loss 0.352569\n",
      "Epoch 330 | Batch 100/100 | Loss 0.346143\n",
      "Epoch 331 | Batch 10/100 | Loss 0.275245\n",
      "Epoch 331 | Batch 20/100 | Loss 0.276132\n",
      "Epoch 331 | Batch 30/100 | Loss 0.280018\n",
      "Epoch 331 | Batch 40/100 | Loss 0.283323\n",
      "Epoch 331 | Batch 50/100 | Loss 0.281433\n",
      "Epoch 331 | Batch 60/100 | Loss 0.287333\n",
      "Epoch 331 | Batch 70/100 | Loss 0.294108\n",
      "Epoch 331 | Batch 80/100 | Loss 0.301672\n",
      "Epoch 331 | Batch 90/100 | Loss 0.301959\n",
      "Epoch 331 | Batch 100/100 | Loss 0.298351\n",
      "Epoch 332 | Batch 10/100 | Loss 0.254254\n",
      "Epoch 332 | Batch 20/100 | Loss 0.293583\n",
      "Epoch 332 | Batch 30/100 | Loss 0.297000\n",
      "Epoch 332 | Batch 40/100 | Loss 0.283412\n",
      "Epoch 332 | Batch 50/100 | Loss 0.294625\n",
      "Epoch 332 | Batch 60/100 | Loss 0.307716\n",
      "Epoch 332 | Batch 70/100 | Loss 0.308226\n",
      "Epoch 332 | Batch 80/100 | Loss 0.308618\n",
      "Epoch 332 | Batch 90/100 | Loss 0.313772\n",
      "Epoch 332 | Batch 100/100 | Loss 0.314906\n",
      "Epoch 333 | Batch 10/100 | Loss 0.295556\n",
      "Epoch 333 | Batch 20/100 | Loss 0.301761\n",
      "Epoch 333 | Batch 30/100 | Loss 0.290961\n",
      "Epoch 333 | Batch 40/100 | Loss 0.291922\n",
      "Epoch 333 | Batch 50/100 | Loss 0.292870\n",
      "Epoch 333 | Batch 60/100 | Loss 0.299303\n",
      "Epoch 333 | Batch 70/100 | Loss 0.298475\n",
      "Epoch 333 | Batch 80/100 | Loss 0.294938\n",
      "Epoch 333 | Batch 90/100 | Loss 0.298379\n",
      "Epoch 333 | Batch 100/100 | Loss 0.299452\n",
      "Epoch 334 | Batch 10/100 | Loss 0.287706\n",
      "Epoch 334 | Batch 20/100 | Loss 0.297255\n",
      "Epoch 334 | Batch 30/100 | Loss 0.321423\n",
      "Epoch 334 | Batch 40/100 | Loss 0.329563\n",
      "Epoch 334 | Batch 50/100 | Loss 0.330839\n",
      "Epoch 334 | Batch 60/100 | Loss 0.328968\n",
      "Epoch 334 | Batch 70/100 | Loss 0.332416\n",
      "Epoch 334 | Batch 80/100 | Loss 0.330856\n",
      "Epoch 334 | Batch 90/100 | Loss 0.333596\n",
      "Epoch 334 | Batch 100/100 | Loss 0.333126\n",
      "Epoch 335 | Batch 10/100 | Loss 0.311342\n",
      "Epoch 335 | Batch 20/100 | Loss 0.293121\n",
      "Epoch 335 | Batch 30/100 | Loss 0.309206\n",
      "Epoch 335 | Batch 40/100 | Loss 0.306060\n",
      "Epoch 335 | Batch 50/100 | Loss 0.308057\n",
      "Epoch 335 | Batch 60/100 | Loss 0.301928\n",
      "Epoch 335 | Batch 70/100 | Loss 0.299700\n",
      "Epoch 335 | Batch 80/100 | Loss 0.300294\n",
      "Epoch 335 | Batch 90/100 | Loss 0.292493\n",
      "Epoch 335 | Batch 100/100 | Loss 0.291420\n",
      "Epoch 336 | Batch 10/100 | Loss 0.333396\n",
      "Epoch 336 | Batch 20/100 | Loss 0.314844\n",
      "Epoch 336 | Batch 30/100 | Loss 0.311309\n",
      "Epoch 336 | Batch 40/100 | Loss 0.297789\n",
      "Epoch 336 | Batch 50/100 | Loss 0.289503\n",
      "Epoch 336 | Batch 60/100 | Loss 0.292793\n",
      "Epoch 336 | Batch 70/100 | Loss 0.289140\n",
      "Epoch 336 | Batch 80/100 | Loss 0.289839\n",
      "Epoch 336 | Batch 90/100 | Loss 0.290227\n",
      "Epoch 336 | Batch 100/100 | Loss 0.295843\n",
      "Epoch 337 | Batch 10/100 | Loss 0.416488\n",
      "Epoch 337 | Batch 20/100 | Loss 0.353676\n",
      "Epoch 337 | Batch 30/100 | Loss 0.353915\n",
      "Epoch 337 | Batch 40/100 | Loss 0.362539\n",
      "Epoch 337 | Batch 50/100 | Loss 0.356428\n",
      "Epoch 337 | Batch 60/100 | Loss 0.346045\n",
      "Epoch 337 | Batch 70/100 | Loss 0.337101\n",
      "Epoch 337 | Batch 80/100 | Loss 0.333559\n",
      "Epoch 337 | Batch 90/100 | Loss 0.329550\n",
      "Epoch 337 | Batch 100/100 | Loss 0.331648\n",
      "Epoch 338 | Batch 10/100 | Loss 0.279980\n",
      "Epoch 338 | Batch 20/100 | Loss 0.294224\n",
      "Epoch 338 | Batch 30/100 | Loss 0.287897\n",
      "Epoch 338 | Batch 40/100 | Loss 0.296044\n",
      "Epoch 338 | Batch 50/100 | Loss 0.288351\n",
      "Epoch 338 | Batch 60/100 | Loss 0.285303\n",
      "Epoch 338 | Batch 70/100 | Loss 0.284770\n",
      "Epoch 338 | Batch 80/100 | Loss 0.284952\n",
      "Epoch 338 | Batch 90/100 | Loss 0.287661\n",
      "Epoch 338 | Batch 100/100 | Loss 0.282334\n",
      "Epoch 339 | Batch 10/100 | Loss 0.243503\n",
      "Epoch 339 | Batch 20/100 | Loss 0.260651\n",
      "Epoch 339 | Batch 30/100 | Loss 0.271089\n",
      "Epoch 339 | Batch 40/100 | Loss 0.283181\n",
      "Epoch 339 | Batch 50/100 | Loss 0.294034\n",
      "Epoch 339 | Batch 60/100 | Loss 0.301084\n",
      "Epoch 339 | Batch 70/100 | Loss 0.310148\n",
      "Epoch 339 | Batch 80/100 | Loss 0.313826\n",
      "Epoch 339 | Batch 90/100 | Loss 0.306129\n",
      "Epoch 339 | Batch 100/100 | Loss 0.299563\n",
      "Epoch 340 | Batch 10/100 | Loss 0.326781\n",
      "Epoch 340 | Batch 20/100 | Loss 0.300998\n",
      "Epoch 340 | Batch 30/100 | Loss 0.330211\n",
      "Epoch 340 | Batch 40/100 | Loss 0.308036\n",
      "Epoch 340 | Batch 50/100 | Loss 0.306328\n",
      "Epoch 340 | Batch 60/100 | Loss 0.298551\n",
      "Epoch 340 | Batch 70/100 | Loss 0.292120\n",
      "Epoch 340 | Batch 80/100 | Loss 0.290848\n",
      "Epoch 340 | Batch 90/100 | Loss 0.296734\n",
      "Epoch 340 | Batch 100/100 | Loss 0.296792\n",
      "Epoch 341 | Batch 10/100 | Loss 0.303205\n",
      "Epoch 341 | Batch 20/100 | Loss 0.295794\n",
      "Epoch 341 | Batch 30/100 | Loss 0.305977\n",
      "Epoch 341 | Batch 40/100 | Loss 0.305050\n",
      "Epoch 341 | Batch 50/100 | Loss 0.297010\n",
      "Epoch 341 | Batch 60/100 | Loss 0.292354\n",
      "Epoch 341 | Batch 70/100 | Loss 0.291884\n",
      "Epoch 341 | Batch 80/100 | Loss 0.298844\n",
      "Epoch 341 | Batch 90/100 | Loss 0.297699\n",
      "Epoch 341 | Batch 100/100 | Loss 0.303103\n",
      "100 Test Protonet Acc = 79.03% +- 1.38%\n",
      "Epoch 342 | Batch 10/100 | Loss 0.370247\n",
      "Epoch 342 | Batch 20/100 | Loss 0.345952\n",
      "Epoch 342 | Batch 30/100 | Loss 0.318696\n",
      "Epoch 342 | Batch 40/100 | Loss 0.313514\n",
      "Epoch 342 | Batch 50/100 | Loss 0.317372\n",
      "Epoch 342 | Batch 60/100 | Loss 0.314337\n",
      "Epoch 342 | Batch 70/100 | Loss 0.311527\n",
      "Epoch 342 | Batch 80/100 | Loss 0.312568\n",
      "Epoch 342 | Batch 90/100 | Loss 0.308235\n",
      "Epoch 342 | Batch 100/100 | Loss 0.317607\n",
      "Epoch 343 | Batch 10/100 | Loss 0.311037\n",
      "Epoch 343 | Batch 20/100 | Loss 0.333852\n",
      "Epoch 343 | Batch 30/100 | Loss 0.318944\n",
      "Epoch 343 | Batch 40/100 | Loss 0.305697\n",
      "Epoch 343 | Batch 50/100 | Loss 0.312073\n",
      "Epoch 343 | Batch 60/100 | Loss 0.303409\n",
      "Epoch 343 | Batch 70/100 | Loss 0.307175\n",
      "Epoch 343 | Batch 80/100 | Loss 0.304527\n",
      "Epoch 343 | Batch 90/100 | Loss 0.303191\n",
      "Epoch 343 | Batch 100/100 | Loss 0.307114\n",
      "Epoch 344 | Batch 10/100 | Loss 0.275546\n",
      "Epoch 344 | Batch 20/100 | Loss 0.267700\n",
      "Epoch 344 | Batch 30/100 | Loss 0.274984\n",
      "Epoch 344 | Batch 40/100 | Loss 0.277809\n",
      "Epoch 344 | Batch 50/100 | Loss 0.284304\n",
      "Epoch 344 | Batch 60/100 | Loss 0.280851\n",
      "Epoch 344 | Batch 70/100 | Loss 0.284240\n",
      "Epoch 344 | Batch 80/100 | Loss 0.281743\n",
      "Epoch 344 | Batch 90/100 | Loss 0.280850\n",
      "Epoch 344 | Batch 100/100 | Loss 0.281193\n",
      "Epoch 345 | Batch 10/100 | Loss 0.313592\n",
      "Epoch 345 | Batch 20/100 | Loss 0.316230\n",
      "Epoch 345 | Batch 30/100 | Loss 0.335156\n",
      "Epoch 345 | Batch 40/100 | Loss 0.327719\n",
      "Epoch 345 | Batch 50/100 | Loss 0.322120\n",
      "Epoch 345 | Batch 60/100 | Loss 0.310393\n",
      "Epoch 345 | Batch 70/100 | Loss 0.307661\n",
      "Epoch 345 | Batch 80/100 | Loss 0.308519\n",
      "Epoch 345 | Batch 90/100 | Loss 0.307537\n",
      "Epoch 345 | Batch 100/100 | Loss 0.307659\n",
      "Epoch 346 | Batch 10/100 | Loss 0.271663\n",
      "Epoch 346 | Batch 20/100 | Loss 0.311381\n",
      "Epoch 346 | Batch 30/100 | Loss 0.319228\n",
      "Epoch 346 | Batch 40/100 | Loss 0.308815\n",
      "Epoch 346 | Batch 50/100 | Loss 0.311082\n",
      "Epoch 346 | Batch 60/100 | Loss 0.305941\n",
      "Epoch 346 | Batch 70/100 | Loss 0.316080\n",
      "Epoch 346 | Batch 80/100 | Loss 0.314209\n",
      "Epoch 346 | Batch 90/100 | Loss 0.311390\n",
      "Epoch 346 | Batch 100/100 | Loss 0.308148\n",
      "Epoch 347 | Batch 10/100 | Loss 0.355411\n",
      "Epoch 347 | Batch 20/100 | Loss 0.314263\n",
      "Epoch 347 | Batch 30/100 | Loss 0.313376\n",
      "Epoch 347 | Batch 40/100 | Loss 0.327824\n",
      "Epoch 347 | Batch 50/100 | Loss 0.329003\n",
      "Epoch 347 | Batch 60/100 | Loss 0.325980\n",
      "Epoch 347 | Batch 70/100 | Loss 0.321992\n",
      "Epoch 347 | Batch 80/100 | Loss 0.314210\n",
      "Epoch 347 | Batch 90/100 | Loss 0.317653\n",
      "Epoch 347 | Batch 100/100 | Loss 0.314370\n",
      "Epoch 348 | Batch 10/100 | Loss 0.266709\n",
      "Epoch 348 | Batch 20/100 | Loss 0.287345\n",
      "Epoch 348 | Batch 30/100 | Loss 0.311128\n",
      "Epoch 348 | Batch 40/100 | Loss 0.309009\n",
      "Epoch 348 | Batch 50/100 | Loss 0.315272\n",
      "Epoch 348 | Batch 60/100 | Loss 0.315870\n",
      "Epoch 348 | Batch 70/100 | Loss 0.316590\n",
      "Epoch 348 | Batch 80/100 | Loss 0.315589\n",
      "Epoch 348 | Batch 90/100 | Loss 0.317966\n",
      "Epoch 348 | Batch 100/100 | Loss 0.315591\n",
      "Epoch 349 | Batch 10/100 | Loss 0.286326\n",
      "Epoch 349 | Batch 20/100 | Loss 0.309500\n",
      "Epoch 349 | Batch 30/100 | Loss 0.309741\n",
      "Epoch 349 | Batch 40/100 | Loss 0.324192\n",
      "Epoch 349 | Batch 50/100 | Loss 0.317747\n",
      "Epoch 349 | Batch 60/100 | Loss 0.318026\n",
      "Epoch 349 | Batch 70/100 | Loss 0.319924\n",
      "Epoch 349 | Batch 80/100 | Loss 0.320380\n",
      "Epoch 349 | Batch 90/100 | Loss 0.320497\n",
      "Epoch 349 | Batch 100/100 | Loss 0.319183\n",
      "Epoch 350 | Batch 10/100 | Loss 0.312109\n",
      "Epoch 350 | Batch 20/100 | Loss 0.337682\n",
      "Epoch 350 | Batch 30/100 | Loss 0.342929\n",
      "Epoch 350 | Batch 40/100 | Loss 0.349804\n",
      "Epoch 350 | Batch 50/100 | Loss 0.328577\n",
      "Epoch 350 | Batch 60/100 | Loss 0.318468\n",
      "Epoch 350 | Batch 70/100 | Loss 0.312061\n",
      "Epoch 350 | Batch 80/100 | Loss 0.306521\n",
      "Epoch 350 | Batch 90/100 | Loss 0.301937\n",
      "Epoch 350 | Batch 100/100 | Loss 0.301878\n",
      "Epoch 351 | Batch 10/100 | Loss 0.301751\n",
      "Epoch 351 | Batch 20/100 | Loss 0.303185\n",
      "Epoch 351 | Batch 30/100 | Loss 0.283936\n",
      "Epoch 351 | Batch 40/100 | Loss 0.283189\n",
      "Epoch 351 | Batch 50/100 | Loss 0.291029\n",
      "Epoch 351 | Batch 60/100 | Loss 0.298542\n",
      "Epoch 351 | Batch 70/100 | Loss 0.301862\n",
      "Epoch 351 | Batch 80/100 | Loss 0.295502\n",
      "Epoch 351 | Batch 90/100 | Loss 0.293264\n",
      "Epoch 351 | Batch 100/100 | Loss 0.292230\n",
      "Epoch 352 | Batch 10/100 | Loss 0.266344\n",
      "Epoch 352 | Batch 20/100 | Loss 0.293112\n",
      "Epoch 352 | Batch 30/100 | Loss 0.302907\n",
      "Epoch 352 | Batch 40/100 | Loss 0.317575\n",
      "Epoch 352 | Batch 50/100 | Loss 0.318755\n",
      "Epoch 352 | Batch 60/100 | Loss 0.311255\n",
      "Epoch 352 | Batch 70/100 | Loss 0.316300\n",
      "Epoch 352 | Batch 80/100 | Loss 0.316229\n",
      "Epoch 352 | Batch 90/100 | Loss 0.311338\n",
      "Epoch 352 | Batch 100/100 | Loss 0.316593\n",
      "Epoch 353 | Batch 10/100 | Loss 0.281038\n",
      "Epoch 353 | Batch 20/100 | Loss 0.295350\n",
      "Epoch 353 | Batch 30/100 | Loss 0.279841\n",
      "Epoch 353 | Batch 40/100 | Loss 0.277436\n",
      "Epoch 353 | Batch 50/100 | Loss 0.293007\n",
      "Epoch 353 | Batch 60/100 | Loss 0.298171\n",
      "Epoch 353 | Batch 70/100 | Loss 0.294793\n",
      "Epoch 353 | Batch 80/100 | Loss 0.293436\n",
      "Epoch 353 | Batch 90/100 | Loss 0.297851\n",
      "Epoch 353 | Batch 100/100 | Loss 0.301499\n",
      "Epoch 354 | Batch 10/100 | Loss 0.279473\n",
      "Epoch 354 | Batch 20/100 | Loss 0.301027\n",
      "Epoch 354 | Batch 30/100 | Loss 0.304371\n",
      "Epoch 354 | Batch 40/100 | Loss 0.304153\n",
      "Epoch 354 | Batch 50/100 | Loss 0.310502\n",
      "Epoch 354 | Batch 60/100 | Loss 0.301851\n",
      "Epoch 354 | Batch 70/100 | Loss 0.303287\n",
      "Epoch 354 | Batch 80/100 | Loss 0.300416\n",
      "Epoch 354 | Batch 90/100 | Loss 0.300970\n",
      "Epoch 354 | Batch 100/100 | Loss 0.302643\n",
      "Epoch 355 | Batch 10/100 | Loss 0.239505\n",
      "Epoch 355 | Batch 20/100 | Loss 0.259186\n",
      "Epoch 355 | Batch 30/100 | Loss 0.269792\n",
      "Epoch 355 | Batch 40/100 | Loss 0.267537\n",
      "Epoch 355 | Batch 50/100 | Loss 0.275177\n",
      "Epoch 355 | Batch 60/100 | Loss 0.279968\n",
      "Epoch 355 | Batch 70/100 | Loss 0.274097\n",
      "Epoch 355 | Batch 80/100 | Loss 0.278883\n",
      "Epoch 355 | Batch 90/100 | Loss 0.276882\n",
      "Epoch 355 | Batch 100/100 | Loss 0.280353\n",
      "Epoch 356 | Batch 10/100 | Loss 0.231080\n",
      "Epoch 356 | Batch 20/100 | Loss 0.236121\n",
      "Epoch 356 | Batch 30/100 | Loss 0.257756\n",
      "Epoch 356 | Batch 40/100 | Loss 0.248790\n",
      "Epoch 356 | Batch 50/100 | Loss 0.254444\n",
      "Epoch 356 | Batch 60/100 | Loss 0.254462\n",
      "Epoch 356 | Batch 70/100 | Loss 0.256576\n",
      "Epoch 356 | Batch 80/100 | Loss 0.259184\n",
      "Epoch 356 | Batch 90/100 | Loss 0.266154\n",
      "Epoch 356 | Batch 100/100 | Loss 0.268023\n",
      "Epoch 357 | Batch 10/100 | Loss 0.313258\n",
      "Epoch 357 | Batch 20/100 | Loss 0.298191\n",
      "Epoch 357 | Batch 30/100 | Loss 0.312074\n",
      "Epoch 357 | Batch 40/100 | Loss 0.295084\n",
      "Epoch 357 | Batch 50/100 | Loss 0.290847\n",
      "Epoch 357 | Batch 60/100 | Loss 0.291326\n",
      "Epoch 357 | Batch 70/100 | Loss 0.288980\n",
      "Epoch 357 | Batch 80/100 | Loss 0.293971\n",
      "Epoch 357 | Batch 90/100 | Loss 0.288932\n",
      "Epoch 357 | Batch 100/100 | Loss 0.290794\n",
      "Epoch 358 | Batch 10/100 | Loss 0.321984\n",
      "Epoch 358 | Batch 20/100 | Loss 0.331512\n",
      "Epoch 358 | Batch 30/100 | Loss 0.310277\n",
      "Epoch 358 | Batch 40/100 | Loss 0.302596\n",
      "Epoch 358 | Batch 50/100 | Loss 0.290512\n",
      "Epoch 358 | Batch 60/100 | Loss 0.295899\n",
      "Epoch 358 | Batch 70/100 | Loss 0.290753\n",
      "Epoch 358 | Batch 80/100 | Loss 0.287173\n",
      "Epoch 358 | Batch 90/100 | Loss 0.282642\n",
      "Epoch 358 | Batch 100/100 | Loss 0.286507\n",
      "Epoch 359 | Batch 10/100 | Loss 0.275597\n",
      "Epoch 359 | Batch 20/100 | Loss 0.289113\n",
      "Epoch 359 | Batch 30/100 | Loss 0.285228\n",
      "Epoch 359 | Batch 40/100 | Loss 0.278487\n",
      "Epoch 359 | Batch 50/100 | Loss 0.277171\n",
      "Epoch 359 | Batch 60/100 | Loss 0.279613\n",
      "Epoch 359 | Batch 70/100 | Loss 0.275431\n",
      "Epoch 359 | Batch 80/100 | Loss 0.283255\n",
      "Epoch 359 | Batch 90/100 | Loss 0.286463\n",
      "Epoch 359 | Batch 100/100 | Loss 0.287024\n",
      "Epoch 360 | Batch 10/100 | Loss 0.271383\n",
      "Epoch 360 | Batch 20/100 | Loss 0.268632\n",
      "Epoch 360 | Batch 30/100 | Loss 0.266654\n",
      "Epoch 360 | Batch 40/100 | Loss 0.271173\n",
      "Epoch 360 | Batch 50/100 | Loss 0.271695\n",
      "Epoch 360 | Batch 60/100 | Loss 0.282004\n",
      "Epoch 360 | Batch 70/100 | Loss 0.277021\n",
      "Epoch 360 | Batch 80/100 | Loss 0.276168\n",
      "Epoch 360 | Batch 90/100 | Loss 0.277356\n",
      "Epoch 360 | Batch 100/100 | Loss 0.278554\n",
      "Epoch 361 | Batch 10/100 | Loss 0.292982\n",
      "Epoch 361 | Batch 20/100 | Loss 0.292203\n",
      "Epoch 361 | Batch 30/100 | Loss 0.308612\n",
      "Epoch 361 | Batch 40/100 | Loss 0.296277\n",
      "Epoch 361 | Batch 50/100 | Loss 0.305578\n",
      "Epoch 361 | Batch 60/100 | Loss 0.298003\n",
      "Epoch 361 | Batch 70/100 | Loss 0.302866\n",
      "Epoch 361 | Batch 80/100 | Loss 0.294448\n",
      "Epoch 361 | Batch 90/100 | Loss 0.295739\n",
      "Epoch 361 | Batch 100/100 | Loss 0.289606\n",
      "100 Test Protonet Acc = 80.88% +- 1.29%\n",
      "Epoch 362 | Batch 10/100 | Loss 0.305562\n",
      "Epoch 362 | Batch 20/100 | Loss 0.316194\n",
      "Epoch 362 | Batch 30/100 | Loss 0.295570\n",
      "Epoch 362 | Batch 40/100 | Loss 0.298164\n",
      "Epoch 362 | Batch 50/100 | Loss 0.293705\n",
      "Epoch 362 | Batch 60/100 | Loss 0.292025\n",
      "Epoch 362 | Batch 70/100 | Loss 0.286904\n",
      "Epoch 362 | Batch 80/100 | Loss 0.283156\n",
      "Epoch 362 | Batch 90/100 | Loss 0.285798\n",
      "Epoch 362 | Batch 100/100 | Loss 0.286040\n",
      "Epoch 363 | Batch 10/100 | Loss 0.310473\n",
      "Epoch 363 | Batch 20/100 | Loss 0.295529\n",
      "Epoch 363 | Batch 30/100 | Loss 0.278235\n",
      "Epoch 363 | Batch 40/100 | Loss 0.276453\n",
      "Epoch 363 | Batch 50/100 | Loss 0.278051\n",
      "Epoch 363 | Batch 60/100 | Loss 0.274605\n",
      "Epoch 363 | Batch 70/100 | Loss 0.275474\n",
      "Epoch 363 | Batch 80/100 | Loss 0.275599\n",
      "Epoch 363 | Batch 90/100 | Loss 0.277202\n",
      "Epoch 363 | Batch 100/100 | Loss 0.283183\n",
      "Epoch 364 | Batch 10/100 | Loss 0.287998\n",
      "Epoch 364 | Batch 20/100 | Loss 0.323297\n",
      "Epoch 364 | Batch 30/100 | Loss 0.300997\n",
      "Epoch 364 | Batch 40/100 | Loss 0.301893\n",
      "Epoch 364 | Batch 50/100 | Loss 0.298690\n",
      "Epoch 364 | Batch 60/100 | Loss 0.296130\n",
      "Epoch 364 | Batch 70/100 | Loss 0.290145\n",
      "Epoch 364 | Batch 80/100 | Loss 0.291506\n",
      "Epoch 364 | Batch 90/100 | Loss 0.290841\n",
      "Epoch 364 | Batch 100/100 | Loss 0.290685\n",
      "Epoch 365 | Batch 10/100 | Loss 0.249389\n",
      "Epoch 365 | Batch 20/100 | Loss 0.246142\n",
      "Epoch 365 | Batch 30/100 | Loss 0.263192\n",
      "Epoch 365 | Batch 40/100 | Loss 0.288647\n",
      "Epoch 365 | Batch 50/100 | Loss 0.287243\n",
      "Epoch 365 | Batch 60/100 | Loss 0.289063\n",
      "Epoch 365 | Batch 70/100 | Loss 0.285662\n",
      "Epoch 365 | Batch 80/100 | Loss 0.286338\n",
      "Epoch 365 | Batch 90/100 | Loss 0.280423\n",
      "Epoch 365 | Batch 100/100 | Loss 0.280371\n",
      "Epoch 366 | Batch 10/100 | Loss 0.316401\n",
      "Epoch 366 | Batch 20/100 | Loss 0.308330\n",
      "Epoch 366 | Batch 30/100 | Loss 0.322741\n",
      "Epoch 366 | Batch 40/100 | Loss 0.314662\n",
      "Epoch 366 | Batch 50/100 | Loss 0.310021\n",
      "Epoch 366 | Batch 60/100 | Loss 0.316224\n",
      "Epoch 366 | Batch 70/100 | Loss 0.311022\n",
      "Epoch 366 | Batch 80/100 | Loss 0.307370\n",
      "Epoch 366 | Batch 90/100 | Loss 0.306672\n",
      "Epoch 366 | Batch 100/100 | Loss 0.306768\n",
      "Epoch 367 | Batch 10/100 | Loss 0.267269\n",
      "Epoch 367 | Batch 20/100 | Loss 0.288992\n",
      "Epoch 367 | Batch 30/100 | Loss 0.282384\n",
      "Epoch 367 | Batch 40/100 | Loss 0.279816\n",
      "Epoch 367 | Batch 50/100 | Loss 0.280073\n",
      "Epoch 367 | Batch 60/100 | Loss 0.277104\n",
      "Epoch 367 | Batch 70/100 | Loss 0.283123\n",
      "Epoch 367 | Batch 80/100 | Loss 0.285997\n",
      "Epoch 367 | Batch 90/100 | Loss 0.285979\n",
      "Epoch 367 | Batch 100/100 | Loss 0.287892\n",
      "Epoch 368 | Batch 10/100 | Loss 0.266983\n",
      "Epoch 368 | Batch 20/100 | Loss 0.273039\n",
      "Epoch 368 | Batch 30/100 | Loss 0.288071\n",
      "Epoch 368 | Batch 40/100 | Loss 0.286302\n",
      "Epoch 368 | Batch 50/100 | Loss 0.285312\n",
      "Epoch 368 | Batch 60/100 | Loss 0.283740\n",
      "Epoch 368 | Batch 70/100 | Loss 0.287793\n",
      "Epoch 368 | Batch 80/100 | Loss 0.288946\n",
      "Epoch 368 | Batch 90/100 | Loss 0.290472\n",
      "Epoch 368 | Batch 100/100 | Loss 0.287232\n",
      "Epoch 369 | Batch 10/100 | Loss 0.357602\n",
      "Epoch 369 | Batch 20/100 | Loss 0.329453\n",
      "Epoch 369 | Batch 30/100 | Loss 0.315926\n",
      "Epoch 369 | Batch 40/100 | Loss 0.312341\n",
      "Epoch 369 | Batch 50/100 | Loss 0.307022\n",
      "Epoch 369 | Batch 60/100 | Loss 0.307730\n",
      "Epoch 369 | Batch 70/100 | Loss 0.302599\n",
      "Epoch 369 | Batch 80/100 | Loss 0.294363\n",
      "Epoch 369 | Batch 90/100 | Loss 0.294719\n",
      "Epoch 369 | Batch 100/100 | Loss 0.294283\n",
      "Epoch 370 | Batch 10/100 | Loss 0.264094\n",
      "Epoch 370 | Batch 20/100 | Loss 0.279816\n",
      "Epoch 370 | Batch 30/100 | Loss 0.274362\n",
      "Epoch 370 | Batch 40/100 | Loss 0.268586\n",
      "Epoch 370 | Batch 50/100 | Loss 0.263542\n",
      "Epoch 370 | Batch 60/100 | Loss 0.263891\n",
      "Epoch 370 | Batch 70/100 | Loss 0.275639\n",
      "Epoch 370 | Batch 80/100 | Loss 0.282023\n",
      "Epoch 370 | Batch 90/100 | Loss 0.289816\n",
      "Epoch 370 | Batch 100/100 | Loss 0.286449\n",
      "Epoch 371 | Batch 10/100 | Loss 0.316538\n",
      "Epoch 371 | Batch 20/100 | Loss 0.313696\n",
      "Epoch 371 | Batch 30/100 | Loss 0.301322\n",
      "Epoch 371 | Batch 40/100 | Loss 0.292758\n",
      "Epoch 371 | Batch 50/100 | Loss 0.293920\n",
      "Epoch 371 | Batch 60/100 | Loss 0.297398\n",
      "Epoch 371 | Batch 70/100 | Loss 0.292032\n",
      "Epoch 371 | Batch 80/100 | Loss 0.297313\n",
      "Epoch 371 | Batch 90/100 | Loss 0.291107\n",
      "Epoch 371 | Batch 100/100 | Loss 0.287665\n",
      "Epoch 372 | Batch 10/100 | Loss 0.318367\n",
      "Epoch 372 | Batch 20/100 | Loss 0.326703\n",
      "Epoch 372 | Batch 30/100 | Loss 0.311422\n",
      "Epoch 372 | Batch 40/100 | Loss 0.310832\n",
      "Epoch 372 | Batch 50/100 | Loss 0.310432\n",
      "Epoch 372 | Batch 60/100 | Loss 0.306577\n",
      "Epoch 372 | Batch 70/100 | Loss 0.298981\n",
      "Epoch 372 | Batch 80/100 | Loss 0.289456\n",
      "Epoch 372 | Batch 90/100 | Loss 0.289401\n",
      "Epoch 372 | Batch 100/100 | Loss 0.288137\n",
      "Epoch 373 | Batch 10/100 | Loss 0.358053\n",
      "Epoch 373 | Batch 20/100 | Loss 0.332226\n",
      "Epoch 373 | Batch 30/100 | Loss 0.316027\n",
      "Epoch 373 | Batch 40/100 | Loss 0.298770\n",
      "Epoch 373 | Batch 50/100 | Loss 0.279357\n",
      "Epoch 373 | Batch 60/100 | Loss 0.286278\n",
      "Epoch 373 | Batch 70/100 | Loss 0.280055\n",
      "Epoch 373 | Batch 80/100 | Loss 0.280230\n",
      "Epoch 373 | Batch 90/100 | Loss 0.281659\n",
      "Epoch 373 | Batch 100/100 | Loss 0.284131\n",
      "Epoch 374 | Batch 10/100 | Loss 0.278557\n",
      "Epoch 374 | Batch 20/100 | Loss 0.275737\n",
      "Epoch 374 | Batch 30/100 | Loss 0.299074\n",
      "Epoch 374 | Batch 40/100 | Loss 0.291632\n",
      "Epoch 374 | Batch 50/100 | Loss 0.286782\n",
      "Epoch 374 | Batch 60/100 | Loss 0.285769\n",
      "Epoch 374 | Batch 70/100 | Loss 0.299500\n",
      "Epoch 374 | Batch 80/100 | Loss 0.296254\n",
      "Epoch 374 | Batch 90/100 | Loss 0.293857\n",
      "Epoch 374 | Batch 100/100 | Loss 0.296114\n",
      "Epoch 375 | Batch 10/100 | Loss 0.337724\n",
      "Epoch 375 | Batch 20/100 | Loss 0.285287\n",
      "Epoch 375 | Batch 30/100 | Loss 0.289641\n",
      "Epoch 375 | Batch 40/100 | Loss 0.287553\n",
      "Epoch 375 | Batch 50/100 | Loss 0.286122\n",
      "Epoch 375 | Batch 60/100 | Loss 0.294150\n",
      "Epoch 375 | Batch 70/100 | Loss 0.300809\n",
      "Epoch 375 | Batch 80/100 | Loss 0.299333\n",
      "Epoch 375 | Batch 90/100 | Loss 0.298103\n",
      "Epoch 375 | Batch 100/100 | Loss 0.294442\n",
      "Epoch 376 | Batch 10/100 | Loss 0.323860\n",
      "Epoch 376 | Batch 20/100 | Loss 0.319221\n",
      "Epoch 376 | Batch 30/100 | Loss 0.313679\n",
      "Epoch 376 | Batch 40/100 | Loss 0.314836\n",
      "Epoch 376 | Batch 50/100 | Loss 0.309371\n",
      "Epoch 376 | Batch 60/100 | Loss 0.300699\n",
      "Epoch 376 | Batch 70/100 | Loss 0.301430\n",
      "Epoch 376 | Batch 80/100 | Loss 0.296350\n",
      "Epoch 376 | Batch 90/100 | Loss 0.295060\n",
      "Epoch 376 | Batch 100/100 | Loss 0.302975\n",
      "Epoch 377 | Batch 10/100 | Loss 0.292220\n",
      "Epoch 377 | Batch 20/100 | Loss 0.277481\n",
      "Epoch 377 | Batch 30/100 | Loss 0.264601\n",
      "Epoch 377 | Batch 40/100 | Loss 0.290648\n",
      "Epoch 377 | Batch 50/100 | Loss 0.294983\n",
      "Epoch 377 | Batch 60/100 | Loss 0.287796\n",
      "Epoch 377 | Batch 70/100 | Loss 0.299440\n",
      "Epoch 377 | Batch 80/100 | Loss 0.297022\n",
      "Epoch 377 | Batch 90/100 | Loss 0.288799\n",
      "Epoch 377 | Batch 100/100 | Loss 0.283449\n",
      "Epoch 378 | Batch 10/100 | Loss 0.316142\n",
      "Epoch 378 | Batch 20/100 | Loss 0.287753\n",
      "Epoch 378 | Batch 30/100 | Loss 0.295227\n",
      "Epoch 378 | Batch 40/100 | Loss 0.301474\n",
      "Epoch 378 | Batch 50/100 | Loss 0.308400\n",
      "Epoch 378 | Batch 60/100 | Loss 0.309432\n",
      "Epoch 378 | Batch 70/100 | Loss 0.305189\n",
      "Epoch 378 | Batch 80/100 | Loss 0.307591\n",
      "Epoch 378 | Batch 90/100 | Loss 0.301425\n",
      "Epoch 378 | Batch 100/100 | Loss 0.298566\n",
      "Epoch 379 | Batch 10/100 | Loss 0.270643\n",
      "Epoch 379 | Batch 20/100 | Loss 0.267398\n",
      "Epoch 379 | Batch 30/100 | Loss 0.273093\n",
      "Epoch 379 | Batch 40/100 | Loss 0.268449\n",
      "Epoch 379 | Batch 50/100 | Loss 0.264861\n",
      "Epoch 379 | Batch 60/100 | Loss 0.264429\n",
      "Epoch 379 | Batch 70/100 | Loss 0.269652\n",
      "Epoch 379 | Batch 80/100 | Loss 0.266693\n",
      "Epoch 379 | Batch 90/100 | Loss 0.266862\n",
      "Epoch 379 | Batch 100/100 | Loss 0.264664\n",
      "Epoch 380 | Batch 10/100 | Loss 0.304614\n",
      "Epoch 380 | Batch 20/100 | Loss 0.258704\n",
      "Epoch 380 | Batch 30/100 | Loss 0.247311\n",
      "Epoch 380 | Batch 40/100 | Loss 0.254679\n",
      "Epoch 380 | Batch 50/100 | Loss 0.268490\n",
      "Epoch 380 | Batch 60/100 | Loss 0.272788\n",
      "Epoch 380 | Batch 70/100 | Loss 0.272805\n",
      "Epoch 380 | Batch 80/100 | Loss 0.280501\n",
      "Epoch 380 | Batch 90/100 | Loss 0.284221\n",
      "Epoch 380 | Batch 100/100 | Loss 0.287133\n",
      "Epoch 381 | Batch 10/100 | Loss 0.291512\n",
      "Epoch 381 | Batch 20/100 | Loss 0.262425\n",
      "Epoch 381 | Batch 30/100 | Loss 0.265179\n",
      "Epoch 381 | Batch 40/100 | Loss 0.275515\n",
      "Epoch 381 | Batch 50/100 | Loss 0.269032\n",
      "Epoch 381 | Batch 60/100 | Loss 0.268091\n",
      "Epoch 381 | Batch 70/100 | Loss 0.265123\n",
      "Epoch 381 | Batch 80/100 | Loss 0.275010\n",
      "Epoch 381 | Batch 90/100 | Loss 0.271496\n",
      "Epoch 381 | Batch 100/100 | Loss 0.271914\n",
      "100 Test Protonet Acc = 81.20% +- 1.54%\n",
      "Epoch 382 | Batch 10/100 | Loss 0.283948\n",
      "Epoch 382 | Batch 20/100 | Loss 0.274508\n",
      "Epoch 382 | Batch 30/100 | Loss 0.281698\n",
      "Epoch 382 | Batch 40/100 | Loss 0.273219\n",
      "Epoch 382 | Batch 50/100 | Loss 0.278522\n",
      "Epoch 382 | Batch 60/100 | Loss 0.281593\n",
      "Epoch 382 | Batch 70/100 | Loss 0.275242\n",
      "Epoch 382 | Batch 80/100 | Loss 0.280189\n",
      "Epoch 382 | Batch 90/100 | Loss 0.285383\n",
      "Epoch 382 | Batch 100/100 | Loss 0.284760\n",
      "Epoch 383 | Batch 10/100 | Loss 0.293597\n",
      "Epoch 383 | Batch 20/100 | Loss 0.290408\n",
      "Epoch 383 | Batch 30/100 | Loss 0.293700\n",
      "Epoch 383 | Batch 40/100 | Loss 0.289524\n",
      "Epoch 383 | Batch 50/100 | Loss 0.286248\n",
      "Epoch 383 | Batch 60/100 | Loss 0.276900\n",
      "Epoch 383 | Batch 70/100 | Loss 0.276740\n",
      "Epoch 383 | Batch 80/100 | Loss 0.274235\n",
      "Epoch 383 | Batch 90/100 | Loss 0.273915\n",
      "Epoch 383 | Batch 100/100 | Loss 0.274645\n",
      "Epoch 384 | Batch 10/100 | Loss 0.259264\n",
      "Epoch 384 | Batch 20/100 | Loss 0.254358\n",
      "Epoch 384 | Batch 30/100 | Loss 0.265893\n",
      "Epoch 384 | Batch 40/100 | Loss 0.273417\n",
      "Epoch 384 | Batch 50/100 | Loss 0.270701\n",
      "Epoch 384 | Batch 60/100 | Loss 0.280287\n",
      "Epoch 384 | Batch 70/100 | Loss 0.284943\n",
      "Epoch 384 | Batch 80/100 | Loss 0.286616\n",
      "Epoch 384 | Batch 90/100 | Loss 0.290564\n",
      "Epoch 384 | Batch 100/100 | Loss 0.287803\n",
      "Epoch 385 | Batch 10/100 | Loss 0.313513\n",
      "Epoch 385 | Batch 20/100 | Loss 0.300512\n",
      "Epoch 385 | Batch 30/100 | Loss 0.302544\n",
      "Epoch 385 | Batch 40/100 | Loss 0.296362\n",
      "Epoch 385 | Batch 50/100 | Loss 0.292483\n",
      "Epoch 385 | Batch 60/100 | Loss 0.286802\n",
      "Epoch 385 | Batch 70/100 | Loss 0.283879\n",
      "Epoch 385 | Batch 80/100 | Loss 0.283797\n",
      "Epoch 385 | Batch 90/100 | Loss 0.284903\n",
      "Epoch 385 | Batch 100/100 | Loss 0.283086\n",
      "Epoch 386 | Batch 10/100 | Loss 0.240953\n",
      "Epoch 386 | Batch 20/100 | Loss 0.264349\n",
      "Epoch 386 | Batch 30/100 | Loss 0.272246\n",
      "Epoch 386 | Batch 40/100 | Loss 0.266540\n",
      "Epoch 386 | Batch 50/100 | Loss 0.247521\n",
      "Epoch 386 | Batch 60/100 | Loss 0.255537\n",
      "Epoch 386 | Batch 70/100 | Loss 0.255926\n",
      "Epoch 386 | Batch 80/100 | Loss 0.252570\n",
      "Epoch 386 | Batch 90/100 | Loss 0.257420\n",
      "Epoch 386 | Batch 100/100 | Loss 0.263480\n",
      "Epoch 387 | Batch 10/100 | Loss 0.269696\n",
      "Epoch 387 | Batch 20/100 | Loss 0.254454\n",
      "Epoch 387 | Batch 30/100 | Loss 0.256627\n",
      "Epoch 387 | Batch 40/100 | Loss 0.279052\n",
      "Epoch 387 | Batch 50/100 | Loss 0.276128\n",
      "Epoch 387 | Batch 60/100 | Loss 0.273619\n",
      "Epoch 387 | Batch 70/100 | Loss 0.270271\n",
      "Epoch 387 | Batch 80/100 | Loss 0.271750\n",
      "Epoch 387 | Batch 90/100 | Loss 0.269061\n",
      "Epoch 387 | Batch 100/100 | Loss 0.270237\n",
      "Epoch 388 | Batch 10/100 | Loss 0.267546\n",
      "Epoch 388 | Batch 20/100 | Loss 0.282341\n",
      "Epoch 388 | Batch 30/100 | Loss 0.273943\n",
      "Epoch 388 | Batch 40/100 | Loss 0.266990\n",
      "Epoch 388 | Batch 50/100 | Loss 0.265441\n",
      "Epoch 388 | Batch 60/100 | Loss 0.261398\n",
      "Epoch 388 | Batch 70/100 | Loss 0.276166\n",
      "Epoch 388 | Batch 80/100 | Loss 0.273307\n",
      "Epoch 388 | Batch 90/100 | Loss 0.268176\n",
      "Epoch 388 | Batch 100/100 | Loss 0.268769\n",
      "Epoch 389 | Batch 10/100 | Loss 0.265722\n",
      "Epoch 389 | Batch 20/100 | Loss 0.269376\n",
      "Epoch 389 | Batch 30/100 | Loss 0.294938\n",
      "Epoch 389 | Batch 40/100 | Loss 0.287834\n",
      "Epoch 389 | Batch 50/100 | Loss 0.287787\n",
      "Epoch 389 | Batch 60/100 | Loss 0.284927\n",
      "Epoch 389 | Batch 70/100 | Loss 0.275828\n",
      "Epoch 389 | Batch 80/100 | Loss 0.270360\n",
      "Epoch 389 | Batch 90/100 | Loss 0.270335\n",
      "Epoch 389 | Batch 100/100 | Loss 0.272928\n",
      "Epoch 390 | Batch 10/100 | Loss 0.265025\n",
      "Epoch 390 | Batch 20/100 | Loss 0.302269\n",
      "Epoch 390 | Batch 30/100 | Loss 0.274465\n",
      "Epoch 390 | Batch 40/100 | Loss 0.283985\n",
      "Epoch 390 | Batch 50/100 | Loss 0.275920\n",
      "Epoch 390 | Batch 60/100 | Loss 0.281167\n",
      "Epoch 390 | Batch 70/100 | Loss 0.276622\n",
      "Epoch 390 | Batch 80/100 | Loss 0.282223\n",
      "Epoch 390 | Batch 90/100 | Loss 0.280895\n",
      "Epoch 390 | Batch 100/100 | Loss 0.280017\n",
      "Epoch 391 | Batch 10/100 | Loss 0.337807\n",
      "Epoch 391 | Batch 20/100 | Loss 0.298734\n",
      "Epoch 391 | Batch 30/100 | Loss 0.296726\n",
      "Epoch 391 | Batch 40/100 | Loss 0.303433\n",
      "Epoch 391 | Batch 50/100 | Loss 0.301892\n",
      "Epoch 391 | Batch 60/100 | Loss 0.305025\n",
      "Epoch 391 | Batch 70/100 | Loss 0.301456\n",
      "Epoch 391 | Batch 80/100 | Loss 0.289281\n",
      "Epoch 391 | Batch 90/100 | Loss 0.283150\n",
      "Epoch 391 | Batch 100/100 | Loss 0.281729\n",
      "Epoch 392 | Batch 10/100 | Loss 0.230842\n",
      "Epoch 392 | Batch 20/100 | Loss 0.249685\n",
      "Epoch 392 | Batch 30/100 | Loss 0.275444\n",
      "Epoch 392 | Batch 40/100 | Loss 0.279198\n",
      "Epoch 392 | Batch 50/100 | Loss 0.263718\n",
      "Epoch 392 | Batch 60/100 | Loss 0.263390\n",
      "Epoch 392 | Batch 70/100 | Loss 0.263110\n",
      "Epoch 392 | Batch 80/100 | Loss 0.260753\n",
      "Epoch 392 | Batch 90/100 | Loss 0.268067\n",
      "Epoch 392 | Batch 100/100 | Loss 0.268509\n",
      "Epoch 393 | Batch 10/100 | Loss 0.290379\n",
      "Epoch 393 | Batch 20/100 | Loss 0.253137\n",
      "Epoch 393 | Batch 30/100 | Loss 0.246696\n",
      "Epoch 393 | Batch 40/100 | Loss 0.242018\n",
      "Epoch 393 | Batch 50/100 | Loss 0.240276\n",
      "Epoch 393 | Batch 60/100 | Loss 0.231355\n",
      "Epoch 393 | Batch 70/100 | Loss 0.241813\n",
      "Epoch 393 | Batch 80/100 | Loss 0.243378\n",
      "Epoch 393 | Batch 90/100 | Loss 0.246746\n",
      "Epoch 393 | Batch 100/100 | Loss 0.246736\n",
      "Epoch 394 | Batch 10/100 | Loss 0.282682\n",
      "Epoch 394 | Batch 20/100 | Loss 0.284404\n",
      "Epoch 394 | Batch 30/100 | Loss 0.276691\n",
      "Epoch 394 | Batch 40/100 | Loss 0.275137\n",
      "Epoch 394 | Batch 50/100 | Loss 0.282642\n",
      "Epoch 394 | Batch 60/100 | Loss 0.275332\n",
      "Epoch 394 | Batch 70/100 | Loss 0.294279\n",
      "Epoch 394 | Batch 80/100 | Loss 0.287037\n",
      "Epoch 394 | Batch 90/100 | Loss 0.284101\n",
      "Epoch 394 | Batch 100/100 | Loss 0.280509\n",
      "Epoch 395 | Batch 10/100 | Loss 0.271119\n",
      "Epoch 395 | Batch 20/100 | Loss 0.270697\n",
      "Epoch 395 | Batch 30/100 | Loss 0.276603\n",
      "Epoch 395 | Batch 40/100 | Loss 0.286340\n",
      "Epoch 395 | Batch 50/100 | Loss 0.273090\n",
      "Epoch 395 | Batch 60/100 | Loss 0.286045\n",
      "Epoch 395 | Batch 70/100 | Loss 0.292302\n",
      "Epoch 395 | Batch 80/100 | Loss 0.285868\n",
      "Epoch 395 | Batch 90/100 | Loss 0.284562\n",
      "Epoch 395 | Batch 100/100 | Loss 0.287164\n",
      "Epoch 396 | Batch 10/100 | Loss 0.252154\n",
      "Epoch 396 | Batch 20/100 | Loss 0.302378\n",
      "Epoch 396 | Batch 30/100 | Loss 0.272351\n",
      "Epoch 396 | Batch 40/100 | Loss 0.265480\n",
      "Epoch 396 | Batch 50/100 | Loss 0.272317\n",
      "Epoch 396 | Batch 60/100 | Loss 0.279711\n",
      "Epoch 396 | Batch 70/100 | Loss 0.276694\n",
      "Epoch 396 | Batch 80/100 | Loss 0.271309\n",
      "Epoch 396 | Batch 90/100 | Loss 0.264718\n",
      "Epoch 396 | Batch 100/100 | Loss 0.263071\n",
      "Epoch 397 | Batch 10/100 | Loss 0.269001\n",
      "Epoch 397 | Batch 20/100 | Loss 0.264705\n",
      "Epoch 397 | Batch 30/100 | Loss 0.268627\n",
      "Epoch 397 | Batch 40/100 | Loss 0.268663\n",
      "Epoch 397 | Batch 50/100 | Loss 0.278420\n",
      "Epoch 397 | Batch 60/100 | Loss 0.269678\n",
      "Epoch 397 | Batch 70/100 | Loss 0.271727\n",
      "Epoch 397 | Batch 80/100 | Loss 0.268331\n",
      "Epoch 397 | Batch 90/100 | Loss 0.268952\n",
      "Epoch 397 | Batch 100/100 | Loss 0.263839\n",
      "Epoch 398 | Batch 10/100 | Loss 0.255359\n",
      "Epoch 398 | Batch 20/100 | Loss 0.245959\n",
      "Epoch 398 | Batch 30/100 | Loss 0.239949\n",
      "Epoch 398 | Batch 40/100 | Loss 0.237679\n",
      "Epoch 398 | Batch 50/100 | Loss 0.244060\n",
      "Epoch 398 | Batch 60/100 | Loss 0.242762\n",
      "Epoch 398 | Batch 70/100 | Loss 0.240953\n",
      "Epoch 398 | Batch 80/100 | Loss 0.249131\n",
      "Epoch 398 | Batch 90/100 | Loss 0.247432\n",
      "Epoch 398 | Batch 100/100 | Loss 0.255975\n",
      "Epoch 399 | Batch 10/100 | Loss 0.242048\n",
      "Epoch 399 | Batch 20/100 | Loss 0.252187\n",
      "Epoch 399 | Batch 30/100 | Loss 0.263243\n",
      "Epoch 399 | Batch 40/100 | Loss 0.266004\n",
      "Epoch 399 | Batch 50/100 | Loss 0.267800\n",
      "Epoch 399 | Batch 60/100 | Loss 0.275932\n",
      "Epoch 399 | Batch 70/100 | Loss 0.278817\n",
      "Epoch 399 | Batch 80/100 | Loss 0.270933\n",
      "Epoch 399 | Batch 90/100 | Loss 0.264238\n",
      "Epoch 399 | Batch 100/100 | Loss 0.262086\n",
      "Epoch 400 | Batch 10/100 | Loss 0.237390\n",
      "Epoch 400 | Batch 20/100 | Loss 0.228755\n",
      "Epoch 400 | Batch 30/100 | Loss 0.235824\n",
      "Epoch 400 | Batch 40/100 | Loss 0.248749\n",
      "Epoch 400 | Batch 50/100 | Loss 0.250112\n",
      "Epoch 400 | Batch 60/100 | Loss 0.245781\n",
      "Epoch 400 | Batch 70/100 | Loss 0.248542\n",
      "Epoch 400 | Batch 80/100 | Loss 0.248553\n",
      "Epoch 400 | Batch 90/100 | Loss 0.247495\n",
      "Epoch 400 | Batch 100/100 | Loss 0.255884\n",
      "Epoch 401 | Batch 10/100 | Loss 0.267979\n",
      "Epoch 401 | Batch 20/100 | Loss 0.274932\n",
      "Epoch 401 | Batch 30/100 | Loss 0.259044\n",
      "Epoch 401 | Batch 40/100 | Loss 0.260104\n",
      "Epoch 401 | Batch 50/100 | Loss 0.278146\n",
      "Epoch 401 | Batch 60/100 | Loss 0.273879\n",
      "Epoch 401 | Batch 70/100 | Loss 0.273752\n",
      "Epoch 401 | Batch 80/100 | Loss 0.277098\n",
      "Epoch 401 | Batch 90/100 | Loss 0.272616\n",
      "Epoch 401 | Batch 100/100 | Loss 0.269633\n",
      "100 Test Protonet Acc = 80.76% +- 1.48%\n",
      "Epoch 402 | Batch 10/100 | Loss 0.242247\n",
      "Epoch 402 | Batch 20/100 | Loss 0.227173\n",
      "Epoch 402 | Batch 30/100 | Loss 0.249008\n",
      "Epoch 402 | Batch 40/100 | Loss 0.259338\n",
      "Epoch 402 | Batch 50/100 | Loss 0.262142\n",
      "Epoch 402 | Batch 60/100 | Loss 0.255088\n",
      "Epoch 402 | Batch 70/100 | Loss 0.257881\n",
      "Epoch 402 | Batch 80/100 | Loss 0.254089\n",
      "Epoch 402 | Batch 90/100 | Loss 0.253472\n",
      "Epoch 402 | Batch 100/100 | Loss 0.255860\n",
      "Epoch 403 | Batch 10/100 | Loss 0.194968\n",
      "Epoch 403 | Batch 20/100 | Loss 0.251339\n",
      "Epoch 403 | Batch 30/100 | Loss 0.261932\n",
      "Epoch 403 | Batch 40/100 | Loss 0.263768\n",
      "Epoch 403 | Batch 50/100 | Loss 0.257408\n",
      "Epoch 403 | Batch 60/100 | Loss 0.259871\n",
      "Epoch 403 | Batch 70/100 | Loss 0.261897\n",
      "Epoch 403 | Batch 80/100 | Loss 0.268847\n",
      "Epoch 403 | Batch 90/100 | Loss 0.261946\n",
      "Epoch 403 | Batch 100/100 | Loss 0.261693\n",
      "Epoch 404 | Batch 10/100 | Loss 0.278244\n",
      "Epoch 404 | Batch 20/100 | Loss 0.300067\n",
      "Epoch 404 | Batch 30/100 | Loss 0.280433\n",
      "Epoch 404 | Batch 40/100 | Loss 0.275779\n",
      "Epoch 404 | Batch 50/100 | Loss 0.264907\n",
      "Epoch 404 | Batch 60/100 | Loss 0.255933\n",
      "Epoch 404 | Batch 70/100 | Loss 0.256546\n",
      "Epoch 404 | Batch 80/100 | Loss 0.253243\n",
      "Epoch 404 | Batch 90/100 | Loss 0.253765\n",
      "Epoch 404 | Batch 100/100 | Loss 0.256777\n",
      "Epoch 405 | Batch 10/100 | Loss 0.279166\n",
      "Epoch 405 | Batch 20/100 | Loss 0.298384\n",
      "Epoch 405 | Batch 30/100 | Loss 0.283976\n",
      "Epoch 405 | Batch 40/100 | Loss 0.275765\n",
      "Epoch 405 | Batch 50/100 | Loss 0.283195\n",
      "Epoch 405 | Batch 60/100 | Loss 0.273971\n",
      "Epoch 405 | Batch 70/100 | Loss 0.271059\n",
      "Epoch 405 | Batch 80/100 | Loss 0.269198\n",
      "Epoch 405 | Batch 90/100 | Loss 0.267965\n",
      "Epoch 405 | Batch 100/100 | Loss 0.270635\n",
      "Epoch 406 | Batch 10/100 | Loss 0.320336\n",
      "Epoch 406 | Batch 20/100 | Loss 0.293039\n",
      "Epoch 406 | Batch 30/100 | Loss 0.292543\n",
      "Epoch 406 | Batch 40/100 | Loss 0.291292\n",
      "Epoch 406 | Batch 50/100 | Loss 0.282690\n",
      "Epoch 406 | Batch 60/100 | Loss 0.278329\n",
      "Epoch 406 | Batch 70/100 | Loss 0.275105\n",
      "Epoch 406 | Batch 80/100 | Loss 0.269855\n",
      "Epoch 406 | Batch 90/100 | Loss 0.268273\n",
      "Epoch 406 | Batch 100/100 | Loss 0.267074\n",
      "Epoch 407 | Batch 10/100 | Loss 0.208096\n",
      "Epoch 407 | Batch 20/100 | Loss 0.237337\n",
      "Epoch 407 | Batch 30/100 | Loss 0.233300\n",
      "Epoch 407 | Batch 40/100 | Loss 0.252155\n",
      "Epoch 407 | Batch 50/100 | Loss 0.244418\n",
      "Epoch 407 | Batch 60/100 | Loss 0.255167\n",
      "Epoch 407 | Batch 70/100 | Loss 0.260653\n",
      "Epoch 407 | Batch 80/100 | Loss 0.266110\n",
      "Epoch 407 | Batch 90/100 | Loss 0.266178\n",
      "Epoch 407 | Batch 100/100 | Loss 0.269051\n",
      "Epoch 408 | Batch 10/100 | Loss 0.261332\n",
      "Epoch 408 | Batch 20/100 | Loss 0.233307\n",
      "Epoch 408 | Batch 30/100 | Loss 0.240404\n",
      "Epoch 408 | Batch 40/100 | Loss 0.235895\n",
      "Epoch 408 | Batch 50/100 | Loss 0.255330\n",
      "Epoch 408 | Batch 60/100 | Loss 0.258179\n",
      "Epoch 408 | Batch 70/100 | Loss 0.259753\n",
      "Epoch 408 | Batch 80/100 | Loss 0.254091\n",
      "Epoch 408 | Batch 90/100 | Loss 0.256567\n",
      "Epoch 408 | Batch 100/100 | Loss 0.257764\n",
      "Epoch 409 | Batch 10/100 | Loss 0.246067\n",
      "Epoch 409 | Batch 20/100 | Loss 0.272455\n",
      "Epoch 409 | Batch 30/100 | Loss 0.276990\n",
      "Epoch 409 | Batch 40/100 | Loss 0.279337\n",
      "Epoch 409 | Batch 50/100 | Loss 0.285854\n",
      "Epoch 409 | Batch 60/100 | Loss 0.271515\n",
      "Epoch 409 | Batch 70/100 | Loss 0.273022\n",
      "Epoch 409 | Batch 80/100 | Loss 0.268636\n",
      "Epoch 409 | Batch 90/100 | Loss 0.268493\n",
      "Epoch 409 | Batch 100/100 | Loss 0.262274\n",
      "Epoch 410 | Batch 10/100 | Loss 0.255360\n",
      "Epoch 410 | Batch 20/100 | Loss 0.254923\n",
      "Epoch 410 | Batch 30/100 | Loss 0.261679\n",
      "Epoch 410 | Batch 40/100 | Loss 0.268301\n",
      "Epoch 410 | Batch 50/100 | Loss 0.258885\n",
      "Epoch 410 | Batch 60/100 | Loss 0.257603\n",
      "Epoch 410 | Batch 70/100 | Loss 0.251565\n",
      "Epoch 410 | Batch 80/100 | Loss 0.257961\n",
      "Epoch 410 | Batch 90/100 | Loss 0.263828\n",
      "Epoch 410 | Batch 100/100 | Loss 0.262684\n",
      "Epoch 411 | Batch 10/100 | Loss 0.262611\n",
      "Epoch 411 | Batch 20/100 | Loss 0.277334\n",
      "Epoch 411 | Batch 30/100 | Loss 0.278141\n",
      "Epoch 411 | Batch 40/100 | Loss 0.269178\n",
      "Epoch 411 | Batch 50/100 | Loss 0.272637\n",
      "Epoch 411 | Batch 60/100 | Loss 0.267749\n",
      "Epoch 411 | Batch 70/100 | Loss 0.265157\n",
      "Epoch 411 | Batch 80/100 | Loss 0.261274\n",
      "Epoch 411 | Batch 90/100 | Loss 0.258369\n",
      "Epoch 411 | Batch 100/100 | Loss 0.258600\n",
      "Epoch 412 | Batch 10/100 | Loss 0.302650\n",
      "Epoch 412 | Batch 20/100 | Loss 0.266789\n",
      "Epoch 412 | Batch 30/100 | Loss 0.264703\n",
      "Epoch 412 | Batch 40/100 | Loss 0.255684\n",
      "Epoch 412 | Batch 50/100 | Loss 0.252538\n",
      "Epoch 412 | Batch 60/100 | Loss 0.257815\n",
      "Epoch 412 | Batch 70/100 | Loss 0.260232\n",
      "Epoch 412 | Batch 80/100 | Loss 0.257777\n",
      "Epoch 412 | Batch 90/100 | Loss 0.257854\n",
      "Epoch 412 | Batch 100/100 | Loss 0.260703\n",
      "Epoch 413 | Batch 10/100 | Loss 0.256183\n",
      "Epoch 413 | Batch 20/100 | Loss 0.264854\n",
      "Epoch 413 | Batch 30/100 | Loss 0.248590\n",
      "Epoch 413 | Batch 40/100 | Loss 0.245992\n",
      "Epoch 413 | Batch 50/100 | Loss 0.247300\n",
      "Epoch 413 | Batch 60/100 | Loss 0.242777\n",
      "Epoch 413 | Batch 70/100 | Loss 0.241594\n",
      "Epoch 413 | Batch 80/100 | Loss 0.239094\n",
      "Epoch 413 | Batch 90/100 | Loss 0.246784\n",
      "Epoch 413 | Batch 100/100 | Loss 0.247622\n",
      "Epoch 414 | Batch 10/100 | Loss 0.250150\n",
      "Epoch 414 | Batch 20/100 | Loss 0.273164\n",
      "Epoch 414 | Batch 30/100 | Loss 0.247100\n",
      "Epoch 414 | Batch 40/100 | Loss 0.237614\n",
      "Epoch 414 | Batch 50/100 | Loss 0.230583\n",
      "Epoch 414 | Batch 60/100 | Loss 0.240897\n",
      "Epoch 414 | Batch 70/100 | Loss 0.232475\n",
      "Epoch 414 | Batch 80/100 | Loss 0.234939\n",
      "Epoch 414 | Batch 90/100 | Loss 0.244355\n",
      "Epoch 414 | Batch 100/100 | Loss 0.246484\n",
      "Epoch 415 | Batch 10/100 | Loss 0.234544\n",
      "Epoch 415 | Batch 20/100 | Loss 0.253091\n",
      "Epoch 415 | Batch 30/100 | Loss 0.266035\n",
      "Epoch 415 | Batch 40/100 | Loss 0.272750\n",
      "Epoch 415 | Batch 50/100 | Loss 0.263125\n",
      "Epoch 415 | Batch 60/100 | Loss 0.270553\n",
      "Epoch 415 | Batch 70/100 | Loss 0.261979\n",
      "Epoch 415 | Batch 80/100 | Loss 0.258888\n",
      "Epoch 415 | Batch 90/100 | Loss 0.252210\n",
      "Epoch 415 | Batch 100/100 | Loss 0.253604\n",
      "Epoch 416 | Batch 10/100 | Loss 0.291745\n",
      "Epoch 416 | Batch 20/100 | Loss 0.291696\n",
      "Epoch 416 | Batch 30/100 | Loss 0.283527\n",
      "Epoch 416 | Batch 40/100 | Loss 0.281440\n",
      "Epoch 416 | Batch 50/100 | Loss 0.283680\n",
      "Epoch 416 | Batch 60/100 | Loss 0.285564\n",
      "Epoch 416 | Batch 70/100 | Loss 0.277164\n",
      "Epoch 416 | Batch 80/100 | Loss 0.279648\n",
      "Epoch 416 | Batch 90/100 | Loss 0.280586\n",
      "Epoch 416 | Batch 100/100 | Loss 0.278616\n",
      "Epoch 417 | Batch 10/100 | Loss 0.323898\n",
      "Epoch 417 | Batch 20/100 | Loss 0.274691\n",
      "Epoch 417 | Batch 30/100 | Loss 0.266508\n",
      "Epoch 417 | Batch 40/100 | Loss 0.260791\n",
      "Epoch 417 | Batch 50/100 | Loss 0.263300\n",
      "Epoch 417 | Batch 60/100 | Loss 0.265070\n",
      "Epoch 417 | Batch 70/100 | Loss 0.271002\n",
      "Epoch 417 | Batch 80/100 | Loss 0.274445\n",
      "Epoch 417 | Batch 90/100 | Loss 0.271603\n",
      "Epoch 417 | Batch 100/100 | Loss 0.269239\n",
      "Epoch 418 | Batch 10/100 | Loss 0.232794\n",
      "Epoch 418 | Batch 20/100 | Loss 0.249461\n",
      "Epoch 418 | Batch 30/100 | Loss 0.241090\n",
      "Epoch 418 | Batch 40/100 | Loss 0.258853\n",
      "Epoch 418 | Batch 50/100 | Loss 0.262760\n",
      "Epoch 418 | Batch 60/100 | Loss 0.254681\n",
      "Epoch 418 | Batch 70/100 | Loss 0.258606\n",
      "Epoch 418 | Batch 80/100 | Loss 0.260724\n",
      "Epoch 418 | Batch 90/100 | Loss 0.254516\n",
      "Epoch 418 | Batch 100/100 | Loss 0.255663\n",
      "Epoch 419 | Batch 10/100 | Loss 0.208714\n",
      "Epoch 419 | Batch 20/100 | Loss 0.235667\n",
      "Epoch 419 | Batch 30/100 | Loss 0.231986\n",
      "Epoch 419 | Batch 40/100 | Loss 0.231956\n",
      "Epoch 419 | Batch 50/100 | Loss 0.229268\n",
      "Epoch 419 | Batch 60/100 | Loss 0.225145\n",
      "Epoch 419 | Batch 70/100 | Loss 0.231426\n",
      "Epoch 419 | Batch 80/100 | Loss 0.237317\n",
      "Epoch 419 | Batch 90/100 | Loss 0.239642\n",
      "Epoch 419 | Batch 100/100 | Loss 0.241649\n",
      "Epoch 420 | Batch 10/100 | Loss 0.281948\n",
      "Epoch 420 | Batch 20/100 | Loss 0.233439\n",
      "Epoch 420 | Batch 30/100 | Loss 0.251748\n",
      "Epoch 420 | Batch 40/100 | Loss 0.255779\n",
      "Epoch 420 | Batch 50/100 | Loss 0.266849\n",
      "Epoch 420 | Batch 60/100 | Loss 0.273073\n",
      "Epoch 420 | Batch 70/100 | Loss 0.270800\n",
      "Epoch 420 | Batch 80/100 | Loss 0.270452\n",
      "Epoch 420 | Batch 90/100 | Loss 0.270284\n",
      "Epoch 420 | Batch 100/100 | Loss 0.274577\n",
      "Epoch 421 | Batch 10/100 | Loss 0.275220\n",
      "Epoch 421 | Batch 20/100 | Loss 0.252408\n",
      "Epoch 421 | Batch 30/100 | Loss 0.263484\n",
      "Epoch 421 | Batch 40/100 | Loss 0.262003\n",
      "Epoch 421 | Batch 50/100 | Loss 0.262807\n",
      "Epoch 421 | Batch 60/100 | Loss 0.258475\n",
      "Epoch 421 | Batch 70/100 | Loss 0.262176\n",
      "Epoch 421 | Batch 80/100 | Loss 0.260830\n",
      "Epoch 421 | Batch 90/100 | Loss 0.261672\n",
      "Epoch 421 | Batch 100/100 | Loss 0.262689\n",
      "100 Test Protonet Acc = 82.86% +- 1.25%\n",
      "best model! save...\n",
      "Epoch 422 | Batch 10/100 | Loss 0.233796\n",
      "Epoch 422 | Batch 20/100 | Loss 0.240891\n",
      "Epoch 422 | Batch 30/100 | Loss 0.244676\n",
      "Epoch 422 | Batch 40/100 | Loss 0.244567\n",
      "Epoch 422 | Batch 50/100 | Loss 0.243870\n",
      "Epoch 422 | Batch 60/100 | Loss 0.246029\n",
      "Epoch 422 | Batch 70/100 | Loss 0.242099\n",
      "Epoch 422 | Batch 80/100 | Loss 0.242841\n",
      "Epoch 422 | Batch 90/100 | Loss 0.246612\n",
      "Epoch 422 | Batch 100/100 | Loss 0.244201\n",
      "Epoch 423 | Batch 10/100 | Loss 0.283636\n",
      "Epoch 423 | Batch 20/100 | Loss 0.282444\n",
      "Epoch 423 | Batch 30/100 | Loss 0.272383\n",
      "Epoch 423 | Batch 40/100 | Loss 0.288528\n",
      "Epoch 423 | Batch 50/100 | Loss 0.282501\n",
      "Epoch 423 | Batch 60/100 | Loss 0.274143\n",
      "Epoch 423 | Batch 70/100 | Loss 0.271851\n",
      "Epoch 423 | Batch 80/100 | Loss 0.272830\n",
      "Epoch 423 | Batch 90/100 | Loss 0.267030\n",
      "Epoch 423 | Batch 100/100 | Loss 0.267520\n",
      "Epoch 424 | Batch 10/100 | Loss 0.244678\n",
      "Epoch 424 | Batch 20/100 | Loss 0.244084\n",
      "Epoch 424 | Batch 30/100 | Loss 0.227611\n",
      "Epoch 424 | Batch 40/100 | Loss 0.229230\n",
      "Epoch 424 | Batch 50/100 | Loss 0.246827\n",
      "Epoch 424 | Batch 60/100 | Loss 0.237810\n",
      "Epoch 424 | Batch 70/100 | Loss 0.242527\n",
      "Epoch 424 | Batch 80/100 | Loss 0.247084\n",
      "Epoch 424 | Batch 90/100 | Loss 0.247690\n",
      "Epoch 424 | Batch 100/100 | Loss 0.244565\n",
      "Epoch 425 | Batch 10/100 | Loss 0.240066\n",
      "Epoch 425 | Batch 20/100 | Loss 0.226333\n",
      "Epoch 425 | Batch 30/100 | Loss 0.242690\n",
      "Epoch 425 | Batch 40/100 | Loss 0.246056\n",
      "Epoch 425 | Batch 50/100 | Loss 0.251159\n",
      "Epoch 425 | Batch 60/100 | Loss 0.254452\n",
      "Epoch 425 | Batch 70/100 | Loss 0.248631\n",
      "Epoch 425 | Batch 80/100 | Loss 0.246198\n",
      "Epoch 425 | Batch 90/100 | Loss 0.248677\n",
      "Epoch 425 | Batch 100/100 | Loss 0.250333\n",
      "Epoch 426 | Batch 10/100 | Loss 0.219068\n",
      "Epoch 426 | Batch 20/100 | Loss 0.189334\n",
      "Epoch 426 | Batch 30/100 | Loss 0.220352\n",
      "Epoch 426 | Batch 40/100 | Loss 0.236018\n",
      "Epoch 426 | Batch 50/100 | Loss 0.229647\n",
      "Epoch 426 | Batch 60/100 | Loss 0.227587\n",
      "Epoch 426 | Batch 70/100 | Loss 0.229243\n",
      "Epoch 426 | Batch 80/100 | Loss 0.231157\n",
      "Epoch 426 | Batch 90/100 | Loss 0.234812\n",
      "Epoch 426 | Batch 100/100 | Loss 0.235832\n",
      "Epoch 427 | Batch 10/100 | Loss 0.272943\n",
      "Epoch 427 | Batch 20/100 | Loss 0.241809\n",
      "Epoch 427 | Batch 30/100 | Loss 0.239052\n",
      "Epoch 427 | Batch 40/100 | Loss 0.238398\n",
      "Epoch 427 | Batch 50/100 | Loss 0.236523\n",
      "Epoch 427 | Batch 60/100 | Loss 0.240651\n",
      "Epoch 427 | Batch 70/100 | Loss 0.243147\n",
      "Epoch 427 | Batch 80/100 | Loss 0.246477\n",
      "Epoch 427 | Batch 90/100 | Loss 0.247438\n",
      "Epoch 427 | Batch 100/100 | Loss 0.253885\n",
      "Epoch 428 | Batch 10/100 | Loss 0.230376\n",
      "Epoch 428 | Batch 20/100 | Loss 0.263277\n",
      "Epoch 428 | Batch 30/100 | Loss 0.259588\n",
      "Epoch 428 | Batch 40/100 | Loss 0.260254\n",
      "Epoch 428 | Batch 50/100 | Loss 0.265262\n",
      "Epoch 428 | Batch 60/100 | Loss 0.272003\n",
      "Epoch 428 | Batch 70/100 | Loss 0.269962\n",
      "Epoch 428 | Batch 80/100 | Loss 0.273596\n",
      "Epoch 428 | Batch 90/100 | Loss 0.270279\n",
      "Epoch 428 | Batch 100/100 | Loss 0.264573\n",
      "Epoch 429 | Batch 10/100 | Loss 0.228210\n",
      "Epoch 429 | Batch 20/100 | Loss 0.259242\n",
      "Epoch 429 | Batch 30/100 | Loss 0.262563\n",
      "Epoch 429 | Batch 40/100 | Loss 0.256746\n",
      "Epoch 429 | Batch 50/100 | Loss 0.248740\n",
      "Epoch 429 | Batch 60/100 | Loss 0.258337\n",
      "Epoch 429 | Batch 70/100 | Loss 0.257416\n",
      "Epoch 429 | Batch 80/100 | Loss 0.264288\n",
      "Epoch 429 | Batch 90/100 | Loss 0.263808\n",
      "Epoch 429 | Batch 100/100 | Loss 0.269236\n",
      "Epoch 430 | Batch 10/100 | Loss 0.213305\n",
      "Epoch 430 | Batch 20/100 | Loss 0.261407\n",
      "Epoch 430 | Batch 30/100 | Loss 0.260571\n",
      "Epoch 430 | Batch 40/100 | Loss 0.256214\n",
      "Epoch 430 | Batch 50/100 | Loss 0.259743\n",
      "Epoch 430 | Batch 60/100 | Loss 0.261142\n",
      "Epoch 430 | Batch 70/100 | Loss 0.264706\n",
      "Epoch 430 | Batch 80/100 | Loss 0.269272\n",
      "Epoch 430 | Batch 90/100 | Loss 0.266939\n",
      "Epoch 430 | Batch 100/100 | Loss 0.266316\n",
      "Epoch 431 | Batch 10/100 | Loss 0.266889\n",
      "Epoch 431 | Batch 20/100 | Loss 0.249545\n",
      "Epoch 431 | Batch 30/100 | Loss 0.244719\n",
      "Epoch 431 | Batch 40/100 | Loss 0.238974\n",
      "Epoch 431 | Batch 50/100 | Loss 0.242157\n",
      "Epoch 431 | Batch 60/100 | Loss 0.242456\n",
      "Epoch 431 | Batch 70/100 | Loss 0.246438\n",
      "Epoch 431 | Batch 80/100 | Loss 0.241647\n",
      "Epoch 431 | Batch 90/100 | Loss 0.256932\n",
      "Epoch 431 | Batch 100/100 | Loss 0.250083\n",
      "Epoch 432 | Batch 10/100 | Loss 0.224832\n",
      "Epoch 432 | Batch 20/100 | Loss 0.241496\n",
      "Epoch 432 | Batch 30/100 | Loss 0.235669\n",
      "Epoch 432 | Batch 40/100 | Loss 0.239095\n",
      "Epoch 432 | Batch 50/100 | Loss 0.234829\n",
      "Epoch 432 | Batch 60/100 | Loss 0.244436\n",
      "Epoch 432 | Batch 70/100 | Loss 0.249734\n",
      "Epoch 432 | Batch 80/100 | Loss 0.250665\n",
      "Epoch 432 | Batch 90/100 | Loss 0.253516\n",
      "Epoch 432 | Batch 100/100 | Loss 0.250546\n",
      "Epoch 433 | Batch 10/100 | Loss 0.250186\n",
      "Epoch 433 | Batch 20/100 | Loss 0.247524\n",
      "Epoch 433 | Batch 30/100 | Loss 0.241905\n",
      "Epoch 433 | Batch 40/100 | Loss 0.247283\n",
      "Epoch 433 | Batch 50/100 | Loss 0.246506\n",
      "Epoch 433 | Batch 60/100 | Loss 0.243162\n",
      "Epoch 433 | Batch 70/100 | Loss 0.241559\n",
      "Epoch 433 | Batch 80/100 | Loss 0.242627\n",
      "Epoch 433 | Batch 90/100 | Loss 0.248894\n",
      "Epoch 433 | Batch 100/100 | Loss 0.247166\n",
      "Epoch 434 | Batch 10/100 | Loss 0.257372\n",
      "Epoch 434 | Batch 20/100 | Loss 0.241259\n",
      "Epoch 434 | Batch 30/100 | Loss 0.245953\n",
      "Epoch 434 | Batch 40/100 | Loss 0.239662\n",
      "Epoch 434 | Batch 50/100 | Loss 0.230932\n",
      "Epoch 434 | Batch 60/100 | Loss 0.238846\n",
      "Epoch 434 | Batch 70/100 | Loss 0.236965\n",
      "Epoch 434 | Batch 80/100 | Loss 0.242702\n",
      "Epoch 434 | Batch 90/100 | Loss 0.240449\n",
      "Epoch 434 | Batch 100/100 | Loss 0.242635\n",
      "Epoch 435 | Batch 10/100 | Loss 0.252506\n",
      "Epoch 435 | Batch 20/100 | Loss 0.232305\n",
      "Epoch 435 | Batch 30/100 | Loss 0.227345\n",
      "Epoch 435 | Batch 40/100 | Loss 0.242121\n",
      "Epoch 435 | Batch 50/100 | Loss 0.248087\n",
      "Epoch 435 | Batch 60/100 | Loss 0.242319\n",
      "Epoch 435 | Batch 70/100 | Loss 0.245153\n",
      "Epoch 435 | Batch 80/100 | Loss 0.249829\n",
      "Epoch 435 | Batch 90/100 | Loss 0.249865\n",
      "Epoch 435 | Batch 100/100 | Loss 0.249818\n",
      "Epoch 436 | Batch 10/100 | Loss 0.250638\n",
      "Epoch 436 | Batch 20/100 | Loss 0.257043\n",
      "Epoch 436 | Batch 30/100 | Loss 0.249280\n",
      "Epoch 436 | Batch 40/100 | Loss 0.244737\n",
      "Epoch 436 | Batch 50/100 | Loss 0.244987\n",
      "Epoch 436 | Batch 60/100 | Loss 0.243658\n",
      "Epoch 436 | Batch 70/100 | Loss 0.245015\n",
      "Epoch 436 | Batch 80/100 | Loss 0.243400\n",
      "Epoch 436 | Batch 90/100 | Loss 0.243283\n",
      "Epoch 436 | Batch 100/100 | Loss 0.244226\n",
      "Epoch 437 | Batch 10/100 | Loss 0.183181\n",
      "Epoch 437 | Batch 20/100 | Loss 0.209712\n",
      "Epoch 437 | Batch 30/100 | Loss 0.197485\n",
      "Epoch 437 | Batch 40/100 | Loss 0.217470\n",
      "Epoch 437 | Batch 50/100 | Loss 0.221246\n",
      "Epoch 437 | Batch 60/100 | Loss 0.211966\n",
      "Epoch 437 | Batch 70/100 | Loss 0.215248\n",
      "Epoch 437 | Batch 80/100 | Loss 0.214778\n",
      "Epoch 437 | Batch 90/100 | Loss 0.211631\n",
      "Epoch 437 | Batch 100/100 | Loss 0.215177\n",
      "Epoch 438 | Batch 10/100 | Loss 0.271867\n",
      "Epoch 438 | Batch 20/100 | Loss 0.245622\n",
      "Epoch 438 | Batch 30/100 | Loss 0.245754\n",
      "Epoch 438 | Batch 40/100 | Loss 0.235759\n",
      "Epoch 438 | Batch 50/100 | Loss 0.236874\n",
      "Epoch 438 | Batch 60/100 | Loss 0.232918\n",
      "Epoch 438 | Batch 70/100 | Loss 0.236068\n",
      "Epoch 438 | Batch 80/100 | Loss 0.237531\n",
      "Epoch 438 | Batch 90/100 | Loss 0.240891\n",
      "Epoch 438 | Batch 100/100 | Loss 0.238831\n",
      "Epoch 439 | Batch 10/100 | Loss 0.195772\n",
      "Epoch 439 | Batch 20/100 | Loss 0.229857\n",
      "Epoch 439 | Batch 30/100 | Loss 0.229602\n",
      "Epoch 439 | Batch 40/100 | Loss 0.232285\n",
      "Epoch 439 | Batch 50/100 | Loss 0.240160\n",
      "Epoch 439 | Batch 60/100 | Loss 0.239667\n",
      "Epoch 439 | Batch 70/100 | Loss 0.253891\n",
      "Epoch 439 | Batch 80/100 | Loss 0.253089\n",
      "Epoch 439 | Batch 90/100 | Loss 0.243528\n",
      "Epoch 439 | Batch 100/100 | Loss 0.239747\n",
      "Epoch 440 | Batch 10/100 | Loss 0.243887\n",
      "Epoch 440 | Batch 20/100 | Loss 0.245220\n",
      "Epoch 440 | Batch 30/100 | Loss 0.243886\n",
      "Epoch 440 | Batch 40/100 | Loss 0.237595\n",
      "Epoch 440 | Batch 50/100 | Loss 0.243977\n",
      "Epoch 440 | Batch 60/100 | Loss 0.247101\n",
      "Epoch 440 | Batch 70/100 | Loss 0.245330\n",
      "Epoch 440 | Batch 80/100 | Loss 0.246695\n",
      "Epoch 440 | Batch 90/100 | Loss 0.243429\n",
      "Epoch 440 | Batch 100/100 | Loss 0.248467\n",
      "Epoch 441 | Batch 10/100 | Loss 0.301992\n",
      "Epoch 441 | Batch 20/100 | Loss 0.272165\n",
      "Epoch 441 | Batch 30/100 | Loss 0.253843\n",
      "Epoch 441 | Batch 40/100 | Loss 0.257165\n",
      "Epoch 441 | Batch 50/100 | Loss 0.256443\n",
      "Epoch 441 | Batch 60/100 | Loss 0.250022\n",
      "Epoch 441 | Batch 70/100 | Loss 0.248692\n",
      "Epoch 441 | Batch 80/100 | Loss 0.243057\n",
      "Epoch 441 | Batch 90/100 | Loss 0.238974\n",
      "Epoch 441 | Batch 100/100 | Loss 0.235771\n",
      "100 Test Protonet Acc = 81.15% +- 1.47%\n",
      "Epoch 442 | Batch 10/100 | Loss 0.246548\n",
      "Epoch 442 | Batch 20/100 | Loss 0.215223\n",
      "Epoch 442 | Batch 30/100 | Loss 0.230678\n",
      "Epoch 442 | Batch 40/100 | Loss 0.239155\n",
      "Epoch 442 | Batch 50/100 | Loss 0.241319\n",
      "Epoch 442 | Batch 60/100 | Loss 0.240281\n",
      "Epoch 442 | Batch 70/100 | Loss 0.239913\n",
      "Epoch 442 | Batch 80/100 | Loss 0.245891\n",
      "Epoch 442 | Batch 90/100 | Loss 0.245197\n",
      "Epoch 442 | Batch 100/100 | Loss 0.246060\n",
      "Epoch 443 | Batch 10/100 | Loss 0.266395\n",
      "Epoch 443 | Batch 20/100 | Loss 0.245435\n",
      "Epoch 443 | Batch 30/100 | Loss 0.225681\n",
      "Epoch 443 | Batch 40/100 | Loss 0.243839\n",
      "Epoch 443 | Batch 50/100 | Loss 0.232772\n",
      "Epoch 443 | Batch 60/100 | Loss 0.236089\n",
      "Epoch 443 | Batch 70/100 | Loss 0.241069\n",
      "Epoch 443 | Batch 80/100 | Loss 0.244588\n",
      "Epoch 443 | Batch 90/100 | Loss 0.240933\n",
      "Epoch 443 | Batch 100/100 | Loss 0.238857\n",
      "Epoch 444 | Batch 10/100 | Loss 0.205502\n",
      "Epoch 444 | Batch 20/100 | Loss 0.218422\n",
      "Epoch 444 | Batch 30/100 | Loss 0.227246\n",
      "Epoch 444 | Batch 40/100 | Loss 0.227900\n",
      "Epoch 444 | Batch 50/100 | Loss 0.239963\n",
      "Epoch 444 | Batch 60/100 | Loss 0.236335\n",
      "Epoch 444 | Batch 70/100 | Loss 0.231704\n",
      "Epoch 444 | Batch 80/100 | Loss 0.233032\n",
      "Epoch 444 | Batch 90/100 | Loss 0.236221\n",
      "Epoch 444 | Batch 100/100 | Loss 0.238432\n",
      "Epoch 445 | Batch 10/100 | Loss 0.302396\n",
      "Epoch 445 | Batch 20/100 | Loss 0.285134\n",
      "Epoch 445 | Batch 30/100 | Loss 0.268572\n",
      "Epoch 445 | Batch 40/100 | Loss 0.266869\n",
      "Epoch 445 | Batch 50/100 | Loss 0.272639\n",
      "Epoch 445 | Batch 60/100 | Loss 0.271420\n",
      "Epoch 445 | Batch 70/100 | Loss 0.262226\n",
      "Epoch 445 | Batch 80/100 | Loss 0.259990\n",
      "Epoch 445 | Batch 90/100 | Loss 0.261981\n",
      "Epoch 445 | Batch 100/100 | Loss 0.257897\n",
      "Epoch 446 | Batch 10/100 | Loss 0.290055\n",
      "Epoch 446 | Batch 20/100 | Loss 0.262129\n",
      "Epoch 446 | Batch 30/100 | Loss 0.262003\n",
      "Epoch 446 | Batch 40/100 | Loss 0.243901\n",
      "Epoch 446 | Batch 50/100 | Loss 0.238026\n",
      "Epoch 446 | Batch 60/100 | Loss 0.233629\n",
      "Epoch 446 | Batch 70/100 | Loss 0.227147\n",
      "Epoch 446 | Batch 80/100 | Loss 0.229018\n",
      "Epoch 446 | Batch 90/100 | Loss 0.237063\n",
      "Epoch 446 | Batch 100/100 | Loss 0.237454\n",
      "Epoch 447 | Batch 10/100 | Loss 0.293180\n",
      "Epoch 447 | Batch 20/100 | Loss 0.265369\n",
      "Epoch 447 | Batch 30/100 | Loss 0.257403\n",
      "Epoch 447 | Batch 40/100 | Loss 0.246977\n",
      "Epoch 447 | Batch 50/100 | Loss 0.236049\n",
      "Epoch 447 | Batch 60/100 | Loss 0.229131\n",
      "Epoch 447 | Batch 70/100 | Loss 0.228574\n",
      "Epoch 447 | Batch 80/100 | Loss 0.230711\n",
      "Epoch 447 | Batch 90/100 | Loss 0.241580\n",
      "Epoch 447 | Batch 100/100 | Loss 0.240734\n",
      "Epoch 448 | Batch 10/100 | Loss 0.199818\n",
      "Epoch 448 | Batch 20/100 | Loss 0.202774\n",
      "Epoch 448 | Batch 30/100 | Loss 0.218057\n",
      "Epoch 448 | Batch 40/100 | Loss 0.213514\n",
      "Epoch 448 | Batch 50/100 | Loss 0.229726\n",
      "Epoch 448 | Batch 60/100 | Loss 0.234244\n",
      "Epoch 448 | Batch 70/100 | Loss 0.235900\n",
      "Epoch 448 | Batch 80/100 | Loss 0.231883\n",
      "Epoch 448 | Batch 90/100 | Loss 0.230420\n",
      "Epoch 448 | Batch 100/100 | Loss 0.234702\n",
      "Epoch 449 | Batch 10/100 | Loss 0.282708\n",
      "Epoch 449 | Batch 20/100 | Loss 0.267187\n",
      "Epoch 449 | Batch 30/100 | Loss 0.254814\n",
      "Epoch 449 | Batch 40/100 | Loss 0.266733\n",
      "Epoch 449 | Batch 50/100 | Loss 0.250388\n",
      "Epoch 449 | Batch 60/100 | Loss 0.239047\n",
      "Epoch 449 | Batch 70/100 | Loss 0.236727\n",
      "Epoch 449 | Batch 80/100 | Loss 0.229677\n",
      "Epoch 449 | Batch 90/100 | Loss 0.230746\n",
      "Epoch 449 | Batch 100/100 | Loss 0.234666\n",
      "Epoch 450 | Batch 10/100 | Loss 0.256434\n",
      "Epoch 450 | Batch 20/100 | Loss 0.270969\n",
      "Epoch 450 | Batch 30/100 | Loss 0.277006\n",
      "Epoch 450 | Batch 40/100 | Loss 0.284148\n",
      "Epoch 450 | Batch 50/100 | Loss 0.275061\n",
      "Epoch 450 | Batch 60/100 | Loss 0.272577\n",
      "Epoch 450 | Batch 70/100 | Loss 0.270895\n",
      "Epoch 450 | Batch 80/100 | Loss 0.276442\n",
      "Epoch 450 | Batch 90/100 | Loss 0.276135\n",
      "Epoch 450 | Batch 100/100 | Loss 0.269591\n",
      "Epoch 451 | Batch 10/100 | Loss 0.249436\n",
      "Epoch 451 | Batch 20/100 | Loss 0.248982\n",
      "Epoch 451 | Batch 30/100 | Loss 0.238340\n",
      "Epoch 451 | Batch 40/100 | Loss 0.231159\n",
      "Epoch 451 | Batch 50/100 | Loss 0.238907\n",
      "Epoch 451 | Batch 60/100 | Loss 0.231124\n",
      "Epoch 451 | Batch 70/100 | Loss 0.236658\n",
      "Epoch 451 | Batch 80/100 | Loss 0.236811\n",
      "Epoch 451 | Batch 90/100 | Loss 0.239547\n",
      "Epoch 451 | Batch 100/100 | Loss 0.240418\n",
      "Epoch 452 | Batch 10/100 | Loss 0.214652\n",
      "Epoch 452 | Batch 20/100 | Loss 0.225907\n",
      "Epoch 452 | Batch 30/100 | Loss 0.261558\n",
      "Epoch 452 | Batch 40/100 | Loss 0.244467\n",
      "Epoch 452 | Batch 50/100 | Loss 0.244556\n",
      "Epoch 452 | Batch 60/100 | Loss 0.245404\n",
      "Epoch 452 | Batch 70/100 | Loss 0.239607\n",
      "Epoch 452 | Batch 80/100 | Loss 0.238722\n",
      "Epoch 452 | Batch 90/100 | Loss 0.246042\n",
      "Epoch 452 | Batch 100/100 | Loss 0.245730\n",
      "Epoch 453 | Batch 10/100 | Loss 0.223967\n",
      "Epoch 453 | Batch 20/100 | Loss 0.211640\n",
      "Epoch 453 | Batch 30/100 | Loss 0.203115\n",
      "Epoch 453 | Batch 40/100 | Loss 0.216086\n",
      "Epoch 453 | Batch 50/100 | Loss 0.212420\n",
      "Epoch 453 | Batch 60/100 | Loss 0.224905\n",
      "Epoch 453 | Batch 70/100 | Loss 0.215180\n",
      "Epoch 453 | Batch 80/100 | Loss 0.219119\n",
      "Epoch 453 | Batch 90/100 | Loss 0.223160\n",
      "Epoch 453 | Batch 100/100 | Loss 0.225390\n",
      "Epoch 454 | Batch 10/100 | Loss 0.257920\n",
      "Epoch 454 | Batch 20/100 | Loss 0.249402\n",
      "Epoch 454 | Batch 30/100 | Loss 0.257335\n",
      "Epoch 454 | Batch 40/100 | Loss 0.244678\n",
      "Epoch 454 | Batch 50/100 | Loss 0.244602\n",
      "Epoch 454 | Batch 60/100 | Loss 0.240339\n",
      "Epoch 454 | Batch 70/100 | Loss 0.243132\n",
      "Epoch 454 | Batch 80/100 | Loss 0.246078\n",
      "Epoch 454 | Batch 90/100 | Loss 0.248164\n",
      "Epoch 454 | Batch 100/100 | Loss 0.250793\n",
      "Epoch 455 | Batch 10/100 | Loss 0.239018\n",
      "Epoch 455 | Batch 20/100 | Loss 0.241779\n",
      "Epoch 455 | Batch 30/100 | Loss 0.226590\n",
      "Epoch 455 | Batch 40/100 | Loss 0.225783\n",
      "Epoch 455 | Batch 50/100 | Loss 0.227694\n",
      "Epoch 455 | Batch 60/100 | Loss 0.219689\n",
      "Epoch 455 | Batch 70/100 | Loss 0.217590\n",
      "Epoch 455 | Batch 80/100 | Loss 0.218498\n",
      "Epoch 455 | Batch 90/100 | Loss 0.220761\n",
      "Epoch 455 | Batch 100/100 | Loss 0.224679\n",
      "Epoch 456 | Batch 10/100 | Loss 0.214526\n",
      "Epoch 456 | Batch 20/100 | Loss 0.252356\n",
      "Epoch 456 | Batch 30/100 | Loss 0.243042\n",
      "Epoch 456 | Batch 40/100 | Loss 0.242543\n",
      "Epoch 456 | Batch 50/100 | Loss 0.249529\n",
      "Epoch 456 | Batch 60/100 | Loss 0.235858\n",
      "Epoch 456 | Batch 70/100 | Loss 0.237102\n",
      "Epoch 456 | Batch 80/100 | Loss 0.239969\n",
      "Epoch 456 | Batch 90/100 | Loss 0.238738\n",
      "Epoch 456 | Batch 100/100 | Loss 0.238021\n",
      "Epoch 457 | Batch 10/100 | Loss 0.173792\n",
      "Epoch 457 | Batch 20/100 | Loss 0.198453\n",
      "Epoch 457 | Batch 30/100 | Loss 0.198027\n",
      "Epoch 457 | Batch 40/100 | Loss 0.184062\n",
      "Epoch 457 | Batch 50/100 | Loss 0.192734\n",
      "Epoch 457 | Batch 60/100 | Loss 0.195801\n",
      "Epoch 457 | Batch 70/100 | Loss 0.214378\n",
      "Epoch 457 | Batch 80/100 | Loss 0.219011\n",
      "Epoch 457 | Batch 90/100 | Loss 0.221153\n",
      "Epoch 457 | Batch 100/100 | Loss 0.222966\n",
      "Epoch 458 | Batch 10/100 | Loss 0.244141\n",
      "Epoch 458 | Batch 20/100 | Loss 0.247317\n",
      "Epoch 458 | Batch 30/100 | Loss 0.239034\n",
      "Epoch 458 | Batch 40/100 | Loss 0.241678\n",
      "Epoch 458 | Batch 50/100 | Loss 0.233987\n",
      "Epoch 458 | Batch 60/100 | Loss 0.225863\n",
      "Epoch 458 | Batch 70/100 | Loss 0.226902\n",
      "Epoch 458 | Batch 80/100 | Loss 0.226492\n",
      "Epoch 458 | Batch 90/100 | Loss 0.224292\n",
      "Epoch 458 | Batch 100/100 | Loss 0.225312\n",
      "Epoch 459 | Batch 10/100 | Loss 0.215082\n",
      "Epoch 459 | Batch 20/100 | Loss 0.237354\n",
      "Epoch 459 | Batch 30/100 | Loss 0.262184\n",
      "Epoch 459 | Batch 40/100 | Loss 0.267783\n",
      "Epoch 459 | Batch 50/100 | Loss 0.258551\n",
      "Epoch 459 | Batch 60/100 | Loss 0.254472\n",
      "Epoch 459 | Batch 70/100 | Loss 0.251870\n",
      "Epoch 459 | Batch 80/100 | Loss 0.243887\n",
      "Epoch 459 | Batch 90/100 | Loss 0.246426\n",
      "Epoch 459 | Batch 100/100 | Loss 0.246987\n",
      "Epoch 460 | Batch 10/100 | Loss 0.278732\n",
      "Epoch 460 | Batch 20/100 | Loss 0.266456\n",
      "Epoch 460 | Batch 30/100 | Loss 0.242229\n",
      "Epoch 460 | Batch 40/100 | Loss 0.239683\n",
      "Epoch 460 | Batch 50/100 | Loss 0.248068\n",
      "Epoch 460 | Batch 60/100 | Loss 0.252547\n",
      "Epoch 460 | Batch 70/100 | Loss 0.247451\n",
      "Epoch 460 | Batch 80/100 | Loss 0.251006\n",
      "Epoch 460 | Batch 90/100 | Loss 0.250142\n",
      "Epoch 460 | Batch 100/100 | Loss 0.245855\n",
      "Epoch 461 | Batch 10/100 | Loss 0.156135\n",
      "Epoch 461 | Batch 20/100 | Loss 0.185744\n",
      "Epoch 461 | Batch 30/100 | Loss 0.190252\n",
      "Epoch 461 | Batch 40/100 | Loss 0.197965\n",
      "Epoch 461 | Batch 50/100 | Loss 0.198636\n",
      "Epoch 461 | Batch 60/100 | Loss 0.213783\n",
      "Epoch 461 | Batch 70/100 | Loss 0.215636\n",
      "Epoch 461 | Batch 80/100 | Loss 0.217556\n",
      "Epoch 461 | Batch 90/100 | Loss 0.214701\n",
      "Epoch 461 | Batch 100/100 | Loss 0.214841\n",
      "100 Test Protonet Acc = 81.62% +- 1.56%\n",
      "Epoch 462 | Batch 10/100 | Loss 0.224017\n",
      "Epoch 462 | Batch 20/100 | Loss 0.243834\n",
      "Epoch 462 | Batch 30/100 | Loss 0.259602\n",
      "Epoch 462 | Batch 40/100 | Loss 0.266364\n",
      "Epoch 462 | Batch 50/100 | Loss 0.252028\n",
      "Epoch 462 | Batch 60/100 | Loss 0.252790\n",
      "Epoch 462 | Batch 70/100 | Loss 0.248359\n",
      "Epoch 462 | Batch 80/100 | Loss 0.245268\n",
      "Epoch 462 | Batch 90/100 | Loss 0.248497\n",
      "Epoch 462 | Batch 100/100 | Loss 0.240427\n",
      "Epoch 463 | Batch 10/100 | Loss 0.191722\n",
      "Epoch 463 | Batch 20/100 | Loss 0.207271\n",
      "Epoch 463 | Batch 30/100 | Loss 0.216096\n",
      "Epoch 463 | Batch 40/100 | Loss 0.216811\n",
      "Epoch 463 | Batch 50/100 | Loss 0.213407\n",
      "Epoch 463 | Batch 60/100 | Loss 0.214410\n",
      "Epoch 463 | Batch 70/100 | Loss 0.222710\n",
      "Epoch 463 | Batch 80/100 | Loss 0.232820\n",
      "Epoch 463 | Batch 90/100 | Loss 0.229164\n",
      "Epoch 463 | Batch 100/100 | Loss 0.230652\n",
      "Epoch 464 | Batch 10/100 | Loss 0.250925\n",
      "Epoch 464 | Batch 20/100 | Loss 0.241553\n",
      "Epoch 464 | Batch 30/100 | Loss 0.246637\n",
      "Epoch 464 | Batch 40/100 | Loss 0.249220\n",
      "Epoch 464 | Batch 50/100 | Loss 0.252642\n",
      "Epoch 464 | Batch 60/100 | Loss 0.250770\n",
      "Epoch 464 | Batch 70/100 | Loss 0.252667\n",
      "Epoch 464 | Batch 80/100 | Loss 0.255783\n",
      "Epoch 464 | Batch 90/100 | Loss 0.250816\n",
      "Epoch 464 | Batch 100/100 | Loss 0.251955\n",
      "Epoch 465 | Batch 10/100 | Loss 0.178184\n",
      "Epoch 465 | Batch 20/100 | Loss 0.245731\n",
      "Epoch 465 | Batch 30/100 | Loss 0.255852\n",
      "Epoch 465 | Batch 40/100 | Loss 0.250626\n",
      "Epoch 465 | Batch 50/100 | Loss 0.244854\n",
      "Epoch 465 | Batch 60/100 | Loss 0.242053\n",
      "Epoch 465 | Batch 70/100 | Loss 0.242769\n",
      "Epoch 465 | Batch 80/100 | Loss 0.243114\n",
      "Epoch 465 | Batch 90/100 | Loss 0.241931\n",
      "Epoch 465 | Batch 100/100 | Loss 0.244648\n",
      "Epoch 466 | Batch 10/100 | Loss 0.261912\n",
      "Epoch 466 | Batch 20/100 | Loss 0.265710\n",
      "Epoch 466 | Batch 30/100 | Loss 0.251378\n",
      "Epoch 466 | Batch 40/100 | Loss 0.238168\n",
      "Epoch 466 | Batch 50/100 | Loss 0.232315\n",
      "Epoch 466 | Batch 60/100 | Loss 0.229151\n",
      "Epoch 466 | Batch 70/100 | Loss 0.220092\n",
      "Epoch 466 | Batch 80/100 | Loss 0.224018\n",
      "Epoch 466 | Batch 90/100 | Loss 0.223795\n",
      "Epoch 466 | Batch 100/100 | Loss 0.230269\n",
      "Epoch 467 | Batch 10/100 | Loss 0.200468\n",
      "Epoch 467 | Batch 20/100 | Loss 0.227525\n",
      "Epoch 467 | Batch 30/100 | Loss 0.217764\n",
      "Epoch 467 | Batch 40/100 | Loss 0.211619\n",
      "Epoch 467 | Batch 50/100 | Loss 0.208675\n",
      "Epoch 467 | Batch 60/100 | Loss 0.208234\n",
      "Epoch 467 | Batch 70/100 | Loss 0.213547\n",
      "Epoch 467 | Batch 80/100 | Loss 0.216838\n",
      "Epoch 467 | Batch 90/100 | Loss 0.216273\n",
      "Epoch 467 | Batch 100/100 | Loss 0.218552\n",
      "Epoch 468 | Batch 10/100 | Loss 0.237538\n",
      "Epoch 468 | Batch 20/100 | Loss 0.223247\n",
      "Epoch 468 | Batch 30/100 | Loss 0.209956\n",
      "Epoch 468 | Batch 40/100 | Loss 0.207477\n",
      "Epoch 468 | Batch 50/100 | Loss 0.210657\n",
      "Epoch 468 | Batch 60/100 | Loss 0.211572\n",
      "Epoch 468 | Batch 70/100 | Loss 0.208232\n",
      "Epoch 468 | Batch 80/100 | Loss 0.211248\n",
      "Epoch 468 | Batch 90/100 | Loss 0.220325\n",
      "Epoch 468 | Batch 100/100 | Loss 0.214603\n",
      "Epoch 469 | Batch 10/100 | Loss 0.296439\n",
      "Epoch 469 | Batch 20/100 | Loss 0.235345\n",
      "Epoch 469 | Batch 30/100 | Loss 0.249091\n",
      "Epoch 469 | Batch 40/100 | Loss 0.248566\n",
      "Epoch 469 | Batch 50/100 | Loss 0.253858\n",
      "Epoch 469 | Batch 60/100 | Loss 0.250357\n",
      "Epoch 469 | Batch 70/100 | Loss 0.247573\n",
      "Epoch 469 | Batch 80/100 | Loss 0.252372\n",
      "Epoch 469 | Batch 90/100 | Loss 0.247603\n",
      "Epoch 469 | Batch 100/100 | Loss 0.244540\n",
      "Epoch 470 | Batch 10/100 | Loss 0.207095\n",
      "Epoch 470 | Batch 20/100 | Loss 0.221467\n",
      "Epoch 470 | Batch 30/100 | Loss 0.240083\n",
      "Epoch 470 | Batch 40/100 | Loss 0.240768\n",
      "Epoch 470 | Batch 50/100 | Loss 0.223061\n",
      "Epoch 470 | Batch 60/100 | Loss 0.223266\n",
      "Epoch 470 | Batch 70/100 | Loss 0.222887\n",
      "Epoch 470 | Batch 80/100 | Loss 0.224544\n",
      "Epoch 470 | Batch 90/100 | Loss 0.226653\n",
      "Epoch 470 | Batch 100/100 | Loss 0.230566\n",
      "Epoch 471 | Batch 10/100 | Loss 0.171737\n",
      "Epoch 471 | Batch 20/100 | Loss 0.197461\n",
      "Epoch 471 | Batch 30/100 | Loss 0.204934\n",
      "Epoch 471 | Batch 40/100 | Loss 0.200832\n",
      "Epoch 471 | Batch 50/100 | Loss 0.202564\n",
      "Epoch 471 | Batch 60/100 | Loss 0.216724\n",
      "Epoch 471 | Batch 70/100 | Loss 0.223308\n",
      "Epoch 471 | Batch 80/100 | Loss 0.229000\n",
      "Epoch 471 | Batch 90/100 | Loss 0.228104\n",
      "Epoch 471 | Batch 100/100 | Loss 0.231934\n",
      "Epoch 472 | Batch 10/100 | Loss 0.218206\n",
      "Epoch 472 | Batch 20/100 | Loss 0.220737\n",
      "Epoch 472 | Batch 30/100 | Loss 0.243398\n",
      "Epoch 472 | Batch 40/100 | Loss 0.233591\n",
      "Epoch 472 | Batch 50/100 | Loss 0.222152\n",
      "Epoch 472 | Batch 60/100 | Loss 0.226343\n",
      "Epoch 472 | Batch 70/100 | Loss 0.224449\n",
      "Epoch 472 | Batch 80/100 | Loss 0.225708\n",
      "Epoch 472 | Batch 90/100 | Loss 0.225543\n",
      "Epoch 472 | Batch 100/100 | Loss 0.227800\n",
      "Epoch 473 | Batch 10/100 | Loss 0.220133\n",
      "Epoch 473 | Batch 20/100 | Loss 0.205188\n",
      "Epoch 473 | Batch 30/100 | Loss 0.222137\n",
      "Epoch 473 | Batch 40/100 | Loss 0.221695\n",
      "Epoch 473 | Batch 50/100 | Loss 0.223105\n",
      "Epoch 473 | Batch 60/100 | Loss 0.226111\n",
      "Epoch 473 | Batch 70/100 | Loss 0.229112\n",
      "Epoch 473 | Batch 80/100 | Loss 0.231206\n",
      "Epoch 473 | Batch 90/100 | Loss 0.225626\n",
      "Epoch 473 | Batch 100/100 | Loss 0.229773\n",
      "Epoch 474 | Batch 10/100 | Loss 0.211660\n",
      "Epoch 474 | Batch 20/100 | Loss 0.202780\n",
      "Epoch 474 | Batch 30/100 | Loss 0.225639\n",
      "Epoch 474 | Batch 40/100 | Loss 0.239228\n",
      "Epoch 474 | Batch 50/100 | Loss 0.232235\n",
      "Epoch 474 | Batch 60/100 | Loss 0.240183\n",
      "Epoch 474 | Batch 70/100 | Loss 0.233106\n",
      "Epoch 474 | Batch 80/100 | Loss 0.230950\n",
      "Epoch 474 | Batch 90/100 | Loss 0.231556\n",
      "Epoch 474 | Batch 100/100 | Loss 0.237940\n",
      "Epoch 475 | Batch 10/100 | Loss 0.241390\n",
      "Epoch 475 | Batch 20/100 | Loss 0.235806\n",
      "Epoch 475 | Batch 30/100 | Loss 0.231520\n",
      "Epoch 475 | Batch 40/100 | Loss 0.245275\n",
      "Epoch 475 | Batch 50/100 | Loss 0.238882\n",
      "Epoch 475 | Batch 60/100 | Loss 0.234735\n",
      "Epoch 475 | Batch 70/100 | Loss 0.237387\n",
      "Epoch 475 | Batch 80/100 | Loss 0.241452\n",
      "Epoch 475 | Batch 90/100 | Loss 0.239986\n",
      "Epoch 475 | Batch 100/100 | Loss 0.247472\n",
      "Epoch 476 | Batch 10/100 | Loss 0.215722\n",
      "Epoch 476 | Batch 20/100 | Loss 0.210605\n",
      "Epoch 476 | Batch 30/100 | Loss 0.226295\n",
      "Epoch 476 | Batch 40/100 | Loss 0.219565\n",
      "Epoch 476 | Batch 50/100 | Loss 0.215312\n",
      "Epoch 476 | Batch 60/100 | Loss 0.224617\n",
      "Epoch 476 | Batch 70/100 | Loss 0.233306\n",
      "Epoch 476 | Batch 80/100 | Loss 0.228396\n",
      "Epoch 476 | Batch 90/100 | Loss 0.228526\n",
      "Epoch 476 | Batch 100/100 | Loss 0.229409\n",
      "Epoch 477 | Batch 10/100 | Loss 0.225528\n",
      "Epoch 477 | Batch 20/100 | Loss 0.223138\n",
      "Epoch 477 | Batch 30/100 | Loss 0.226244\n",
      "Epoch 477 | Batch 40/100 | Loss 0.225658\n",
      "Epoch 477 | Batch 50/100 | Loss 0.225140\n",
      "Epoch 477 | Batch 60/100 | Loss 0.223778\n",
      "Epoch 477 | Batch 70/100 | Loss 0.213250\n",
      "Epoch 477 | Batch 80/100 | Loss 0.210686\n",
      "Epoch 477 | Batch 90/100 | Loss 0.210489\n",
      "Epoch 477 | Batch 100/100 | Loss 0.212682\n",
      "Epoch 478 | Batch 10/100 | Loss 0.224191\n",
      "Epoch 478 | Batch 20/100 | Loss 0.231471\n",
      "Epoch 478 | Batch 30/100 | Loss 0.216239\n",
      "Epoch 478 | Batch 40/100 | Loss 0.221510\n",
      "Epoch 478 | Batch 50/100 | Loss 0.223456\n",
      "Epoch 478 | Batch 60/100 | Loss 0.231514\n",
      "Epoch 478 | Batch 70/100 | Loss 0.230557\n",
      "Epoch 478 | Batch 80/100 | Loss 0.230189\n",
      "Epoch 478 | Batch 90/100 | Loss 0.226804\n",
      "Epoch 478 | Batch 100/100 | Loss 0.222156\n",
      "Epoch 479 | Batch 10/100 | Loss 0.260256\n",
      "Epoch 479 | Batch 20/100 | Loss 0.246197\n",
      "Epoch 479 | Batch 30/100 | Loss 0.219143\n",
      "Epoch 479 | Batch 40/100 | Loss 0.222941\n",
      "Epoch 479 | Batch 50/100 | Loss 0.221639\n",
      "Epoch 479 | Batch 60/100 | Loss 0.221528\n",
      "Epoch 479 | Batch 70/100 | Loss 0.226525\n",
      "Epoch 479 | Batch 80/100 | Loss 0.229520\n",
      "Epoch 479 | Batch 90/100 | Loss 0.233663\n",
      "Epoch 479 | Batch 100/100 | Loss 0.230463\n",
      "Epoch 480 | Batch 10/100 | Loss 0.231748\n",
      "Epoch 480 | Batch 20/100 | Loss 0.233152\n",
      "Epoch 480 | Batch 30/100 | Loss 0.221827\n",
      "Epoch 480 | Batch 40/100 | Loss 0.217741\n",
      "Epoch 480 | Batch 50/100 | Loss 0.214191\n",
      "Epoch 480 | Batch 60/100 | Loss 0.224405\n",
      "Epoch 480 | Batch 70/100 | Loss 0.226065\n",
      "Epoch 480 | Batch 80/100 | Loss 0.228973\n",
      "Epoch 480 | Batch 90/100 | Loss 0.228736\n",
      "Epoch 480 | Batch 100/100 | Loss 0.231860\n",
      "Epoch 481 | Batch 10/100 | Loss 0.206427\n",
      "Epoch 481 | Batch 20/100 | Loss 0.200557\n",
      "Epoch 481 | Batch 30/100 | Loss 0.207513\n",
      "Epoch 481 | Batch 40/100 | Loss 0.192650\n",
      "Epoch 481 | Batch 50/100 | Loss 0.197133\n",
      "Epoch 481 | Batch 60/100 | Loss 0.196676\n",
      "Epoch 481 | Batch 70/100 | Loss 0.198912\n",
      "Epoch 481 | Batch 80/100 | Loss 0.207009\n",
      "Epoch 481 | Batch 90/100 | Loss 0.204417\n",
      "Epoch 481 | Batch 100/100 | Loss 0.206872\n",
      "100 Test Protonet Acc = 80.34% +- 1.35%\n",
      "Epoch 482 | Batch 10/100 | Loss 0.239248\n",
      "Epoch 482 | Batch 20/100 | Loss 0.238986\n",
      "Epoch 482 | Batch 30/100 | Loss 0.227673\n",
      "Epoch 482 | Batch 40/100 | Loss 0.221851\n",
      "Epoch 482 | Batch 50/100 | Loss 0.218636\n",
      "Epoch 482 | Batch 60/100 | Loss 0.212058\n",
      "Epoch 482 | Batch 70/100 | Loss 0.208661\n",
      "Epoch 482 | Batch 80/100 | Loss 0.209176\n",
      "Epoch 482 | Batch 90/100 | Loss 0.208317\n",
      "Epoch 482 | Batch 100/100 | Loss 0.203522\n",
      "Epoch 483 | Batch 10/100 | Loss 0.197101\n",
      "Epoch 483 | Batch 20/100 | Loss 0.203944\n",
      "Epoch 483 | Batch 30/100 | Loss 0.218936\n",
      "Epoch 483 | Batch 40/100 | Loss 0.215837\n",
      "Epoch 483 | Batch 50/100 | Loss 0.220432\n",
      "Epoch 483 | Batch 60/100 | Loss 0.222028\n",
      "Epoch 483 | Batch 70/100 | Loss 0.223245\n",
      "Epoch 483 | Batch 80/100 | Loss 0.220560\n",
      "Epoch 483 | Batch 90/100 | Loss 0.221138\n",
      "Epoch 483 | Batch 100/100 | Loss 0.223003\n",
      "Epoch 484 | Batch 10/100 | Loss 0.191482\n",
      "Epoch 484 | Batch 20/100 | Loss 0.209269\n",
      "Epoch 484 | Batch 30/100 | Loss 0.212835\n",
      "Epoch 484 | Batch 40/100 | Loss 0.215791\n",
      "Epoch 484 | Batch 50/100 | Loss 0.215169\n",
      "Epoch 484 | Batch 60/100 | Loss 0.226614\n",
      "Epoch 484 | Batch 70/100 | Loss 0.224367\n",
      "Epoch 484 | Batch 80/100 | Loss 0.221604\n",
      "Epoch 484 | Batch 90/100 | Loss 0.229588\n",
      "Epoch 484 | Batch 100/100 | Loss 0.233802\n",
      "Epoch 485 | Batch 10/100 | Loss 0.188675\n",
      "Epoch 485 | Batch 20/100 | Loss 0.181954\n",
      "Epoch 485 | Batch 30/100 | Loss 0.200637\n",
      "Epoch 485 | Batch 40/100 | Loss 0.199385\n",
      "Epoch 485 | Batch 50/100 | Loss 0.201790\n",
      "Epoch 485 | Batch 60/100 | Loss 0.207657\n",
      "Epoch 485 | Batch 70/100 | Loss 0.211083\n",
      "Epoch 485 | Batch 80/100 | Loss 0.213711\n",
      "Epoch 485 | Batch 90/100 | Loss 0.212925\n",
      "Epoch 485 | Batch 100/100 | Loss 0.212707\n",
      "Epoch 486 | Batch 10/100 | Loss 0.196498\n",
      "Epoch 486 | Batch 20/100 | Loss 0.200238\n",
      "Epoch 486 | Batch 30/100 | Loss 0.224281\n",
      "Epoch 486 | Batch 40/100 | Loss 0.235440\n",
      "Epoch 486 | Batch 50/100 | Loss 0.233594\n",
      "Epoch 486 | Batch 60/100 | Loss 0.233329\n",
      "Epoch 486 | Batch 70/100 | Loss 0.236996\n",
      "Epoch 486 | Batch 80/100 | Loss 0.231574\n",
      "Epoch 486 | Batch 90/100 | Loss 0.226261\n",
      "Epoch 486 | Batch 100/100 | Loss 0.222605\n",
      "Epoch 487 | Batch 10/100 | Loss 0.203707\n",
      "Epoch 487 | Batch 20/100 | Loss 0.173498\n",
      "Epoch 487 | Batch 30/100 | Loss 0.177677\n",
      "Epoch 487 | Batch 40/100 | Loss 0.182946\n",
      "Epoch 487 | Batch 50/100 | Loss 0.191887\n",
      "Epoch 487 | Batch 60/100 | Loss 0.193214\n",
      "Epoch 487 | Batch 70/100 | Loss 0.194220\n",
      "Epoch 487 | Batch 80/100 | Loss 0.200214\n",
      "Epoch 487 | Batch 90/100 | Loss 0.201109\n",
      "Epoch 487 | Batch 100/100 | Loss 0.206030\n",
      "Epoch 488 | Batch 10/100 | Loss 0.230943\n",
      "Epoch 488 | Batch 20/100 | Loss 0.212445\n",
      "Epoch 488 | Batch 30/100 | Loss 0.217595\n",
      "Epoch 488 | Batch 40/100 | Loss 0.226390\n",
      "Epoch 488 | Batch 50/100 | Loss 0.227324\n",
      "Epoch 488 | Batch 60/100 | Loss 0.239074\n",
      "Epoch 488 | Batch 70/100 | Loss 0.227298\n",
      "Epoch 488 | Batch 80/100 | Loss 0.225396\n",
      "Epoch 488 | Batch 90/100 | Loss 0.228678\n",
      "Epoch 488 | Batch 100/100 | Loss 0.230695\n",
      "Epoch 489 | Batch 10/100 | Loss 0.241781\n",
      "Epoch 489 | Batch 20/100 | Loss 0.224541\n",
      "Epoch 489 | Batch 30/100 | Loss 0.213942\n",
      "Epoch 489 | Batch 40/100 | Loss 0.211774\n",
      "Epoch 489 | Batch 50/100 | Loss 0.209517\n",
      "Epoch 489 | Batch 60/100 | Loss 0.204553\n",
      "Epoch 489 | Batch 70/100 | Loss 0.213153\n",
      "Epoch 489 | Batch 80/100 | Loss 0.219832\n",
      "Epoch 489 | Batch 90/100 | Loss 0.220841\n",
      "Epoch 489 | Batch 100/100 | Loss 0.217368\n",
      "Epoch 490 | Batch 10/100 | Loss 0.186635\n",
      "Epoch 490 | Batch 20/100 | Loss 0.192564\n",
      "Epoch 490 | Batch 30/100 | Loss 0.189260\n",
      "Epoch 490 | Batch 40/100 | Loss 0.197204\n",
      "Epoch 490 | Batch 50/100 | Loss 0.214460\n",
      "Epoch 490 | Batch 60/100 | Loss 0.208922\n",
      "Epoch 490 | Batch 70/100 | Loss 0.211957\n",
      "Epoch 490 | Batch 80/100 | Loss 0.213641\n",
      "Epoch 490 | Batch 90/100 | Loss 0.218021\n",
      "Epoch 490 | Batch 100/100 | Loss 0.217927\n",
      "Epoch 491 | Batch 10/100 | Loss 0.246773\n",
      "Epoch 491 | Batch 20/100 | Loss 0.231031\n",
      "Epoch 491 | Batch 30/100 | Loss 0.238229\n",
      "Epoch 491 | Batch 40/100 | Loss 0.235776\n",
      "Epoch 491 | Batch 50/100 | Loss 0.232879\n",
      "Epoch 491 | Batch 60/100 | Loss 0.225156\n",
      "Epoch 491 | Batch 70/100 | Loss 0.225671\n",
      "Epoch 491 | Batch 80/100 | Loss 0.226271\n",
      "Epoch 491 | Batch 90/100 | Loss 0.224937\n",
      "Epoch 491 | Batch 100/100 | Loss 0.227586\n",
      "Epoch 492 | Batch 10/100 | Loss 0.244776\n",
      "Epoch 492 | Batch 20/100 | Loss 0.253202\n",
      "Epoch 492 | Batch 30/100 | Loss 0.239962\n",
      "Epoch 492 | Batch 40/100 | Loss 0.240984\n",
      "Epoch 492 | Batch 50/100 | Loss 0.232034\n",
      "Epoch 492 | Batch 60/100 | Loss 0.229951\n",
      "Epoch 492 | Batch 70/100 | Loss 0.231191\n",
      "Epoch 492 | Batch 80/100 | Loss 0.223795\n",
      "Epoch 492 | Batch 90/100 | Loss 0.224638\n",
      "Epoch 492 | Batch 100/100 | Loss 0.221051\n",
      "Epoch 493 | Batch 10/100 | Loss 0.249096\n",
      "Epoch 493 | Batch 20/100 | Loss 0.246644\n",
      "Epoch 493 | Batch 30/100 | Loss 0.249212\n",
      "Epoch 493 | Batch 40/100 | Loss 0.240140\n",
      "Epoch 493 | Batch 50/100 | Loss 0.232672\n",
      "Epoch 493 | Batch 60/100 | Loss 0.230261\n",
      "Epoch 493 | Batch 70/100 | Loss 0.223987\n",
      "Epoch 493 | Batch 80/100 | Loss 0.226227\n",
      "Epoch 493 | Batch 90/100 | Loss 0.219091\n",
      "Epoch 493 | Batch 100/100 | Loss 0.221250\n",
      "Epoch 494 | Batch 10/100 | Loss 0.169251\n",
      "Epoch 494 | Batch 20/100 | Loss 0.217091\n",
      "Epoch 494 | Batch 30/100 | Loss 0.254625\n",
      "Epoch 494 | Batch 40/100 | Loss 0.234720\n",
      "Epoch 494 | Batch 50/100 | Loss 0.231285\n",
      "Epoch 494 | Batch 60/100 | Loss 0.226806\n",
      "Epoch 494 | Batch 70/100 | Loss 0.230034\n",
      "Epoch 494 | Batch 80/100 | Loss 0.232391\n",
      "Epoch 494 | Batch 90/100 | Loss 0.228106\n",
      "Epoch 494 | Batch 100/100 | Loss 0.234473\n",
      "Epoch 495 | Batch 10/100 | Loss 0.187463\n",
      "Epoch 495 | Batch 20/100 | Loss 0.217169\n",
      "Epoch 495 | Batch 30/100 | Loss 0.216806\n",
      "Epoch 495 | Batch 40/100 | Loss 0.217293\n",
      "Epoch 495 | Batch 50/100 | Loss 0.221394\n",
      "Epoch 495 | Batch 60/100 | Loss 0.214859\n",
      "Epoch 495 | Batch 70/100 | Loss 0.216175\n",
      "Epoch 495 | Batch 80/100 | Loss 0.210241\n",
      "Epoch 495 | Batch 90/100 | Loss 0.210891\n",
      "Epoch 495 | Batch 100/100 | Loss 0.209350\n",
      "Epoch 496 | Batch 10/100 | Loss 0.258585\n",
      "Epoch 496 | Batch 20/100 | Loss 0.237933\n",
      "Epoch 496 | Batch 30/100 | Loss 0.227342\n",
      "Epoch 496 | Batch 40/100 | Loss 0.218775\n",
      "Epoch 496 | Batch 50/100 | Loss 0.219376\n",
      "Epoch 496 | Batch 60/100 | Loss 0.217746\n",
      "Epoch 496 | Batch 70/100 | Loss 0.212804\n",
      "Epoch 496 | Batch 80/100 | Loss 0.214325\n",
      "Epoch 496 | Batch 90/100 | Loss 0.212341\n",
      "Epoch 496 | Batch 100/100 | Loss 0.210451\n",
      "Epoch 497 | Batch 10/100 | Loss 0.217390\n",
      "Epoch 497 | Batch 20/100 | Loss 0.229345\n",
      "Epoch 497 | Batch 30/100 | Loss 0.250490\n",
      "Epoch 497 | Batch 40/100 | Loss 0.239969\n",
      "Epoch 497 | Batch 50/100 | Loss 0.225850\n",
      "Epoch 497 | Batch 60/100 | Loss 0.224302\n",
      "Epoch 497 | Batch 70/100 | Loss 0.221241\n",
      "Epoch 497 | Batch 80/100 | Loss 0.216701\n",
      "Epoch 497 | Batch 90/100 | Loss 0.209607\n",
      "Epoch 497 | Batch 100/100 | Loss 0.212225\n",
      "Epoch 498 | Batch 10/100 | Loss 0.222350\n",
      "Epoch 498 | Batch 20/100 | Loss 0.209473\n",
      "Epoch 498 | Batch 30/100 | Loss 0.196046\n",
      "Epoch 498 | Batch 40/100 | Loss 0.211205\n",
      "Epoch 498 | Batch 50/100 | Loss 0.216190\n",
      "Epoch 498 | Batch 60/100 | Loss 0.216046\n",
      "Epoch 498 | Batch 70/100 | Loss 0.223719\n",
      "Epoch 498 | Batch 80/100 | Loss 0.221236\n",
      "Epoch 498 | Batch 90/100 | Loss 0.216278\n",
      "Epoch 498 | Batch 100/100 | Loss 0.215554\n",
      "Epoch 499 | Batch 10/100 | Loss 0.238178\n",
      "Epoch 499 | Batch 20/100 | Loss 0.217007\n",
      "Epoch 499 | Batch 30/100 | Loss 0.213761\n",
      "Epoch 499 | Batch 40/100 | Loss 0.217323\n",
      "Epoch 499 | Batch 50/100 | Loss 0.212292\n",
      "Epoch 499 | Batch 60/100 | Loss 0.210796\n",
      "Epoch 499 | Batch 70/100 | Loss 0.215376\n",
      "Epoch 499 | Batch 80/100 | Loss 0.223657\n",
      "Epoch 499 | Batch 90/100 | Loss 0.222130\n",
      "Epoch 499 | Batch 100/100 | Loss 0.223529\n",
      "Epoch 500 | Batch 10/100 | Loss 0.259043\n",
      "Epoch 500 | Batch 20/100 | Loss 0.236607\n",
      "Epoch 500 | Batch 30/100 | Loss 0.249911\n",
      "Epoch 500 | Batch 40/100 | Loss 0.224328\n",
      "Epoch 500 | Batch 50/100 | Loss 0.215981\n",
      "Epoch 500 | Batch 60/100 | Loss 0.209483\n",
      "Epoch 500 | Batch 70/100 | Loss 0.213225\n",
      "Epoch 500 | Batch 80/100 | Loss 0.212914\n",
      "Epoch 500 | Batch 90/100 | Loss 0.227104\n",
      "Epoch 500 | Batch 100/100 | Loss 0.231524\n",
      "Epoch 501 | Batch 10/100 | Loss 0.173951\n",
      "Epoch 501 | Batch 20/100 | Loss 0.186734\n",
      "Epoch 501 | Batch 30/100 | Loss 0.208007\n",
      "Epoch 501 | Batch 40/100 | Loss 0.202553\n",
      "Epoch 501 | Batch 50/100 | Loss 0.203475\n",
      "Epoch 501 | Batch 60/100 | Loss 0.215331\n",
      "Epoch 501 | Batch 70/100 | Loss 0.211546\n",
      "Epoch 501 | Batch 80/100 | Loss 0.205932\n",
      "Epoch 501 | Batch 90/100 | Loss 0.203574\n",
      "Epoch 501 | Batch 100/100 | Loss 0.207496\n",
      "100 Test Protonet Acc = 82.17% +- 1.46%\n",
      "Epoch 502 | Batch 10/100 | Loss 0.222326\n",
      "Epoch 502 | Batch 20/100 | Loss 0.202432\n",
      "Epoch 502 | Batch 30/100 | Loss 0.200184\n",
      "Epoch 502 | Batch 40/100 | Loss 0.197788\n",
      "Epoch 502 | Batch 50/100 | Loss 0.204444\n",
      "Epoch 502 | Batch 60/100 | Loss 0.202600\n",
      "Epoch 502 | Batch 70/100 | Loss 0.200505\n",
      "Epoch 502 | Batch 80/100 | Loss 0.207346\n",
      "Epoch 502 | Batch 90/100 | Loss 0.211544\n",
      "Epoch 502 | Batch 100/100 | Loss 0.211937\n",
      "Epoch 503 | Batch 10/100 | Loss 0.217450\n",
      "Epoch 503 | Batch 20/100 | Loss 0.210427\n",
      "Epoch 503 | Batch 30/100 | Loss 0.219904\n",
      "Epoch 503 | Batch 40/100 | Loss 0.226556\n",
      "Epoch 503 | Batch 50/100 | Loss 0.223095\n",
      "Epoch 503 | Batch 60/100 | Loss 0.224229\n",
      "Epoch 503 | Batch 70/100 | Loss 0.229289\n",
      "Epoch 503 | Batch 80/100 | Loss 0.225235\n",
      "Epoch 503 | Batch 90/100 | Loss 0.224761\n",
      "Epoch 503 | Batch 100/100 | Loss 0.219518\n",
      "Epoch 504 | Batch 10/100 | Loss 0.223803\n",
      "Epoch 504 | Batch 20/100 | Loss 0.260588\n",
      "Epoch 504 | Batch 30/100 | Loss 0.242389\n",
      "Epoch 504 | Batch 40/100 | Loss 0.241181\n",
      "Epoch 504 | Batch 50/100 | Loss 0.235716\n",
      "Epoch 504 | Batch 60/100 | Loss 0.226166\n",
      "Epoch 504 | Batch 70/100 | Loss 0.217196\n",
      "Epoch 504 | Batch 80/100 | Loss 0.220550\n",
      "Epoch 504 | Batch 90/100 | Loss 0.222122\n",
      "Epoch 504 | Batch 100/100 | Loss 0.219132\n",
      "Epoch 505 | Batch 10/100 | Loss 0.180079\n",
      "Epoch 505 | Batch 20/100 | Loss 0.208798\n",
      "Epoch 505 | Batch 30/100 | Loss 0.218213\n",
      "Epoch 505 | Batch 40/100 | Loss 0.222147\n",
      "Epoch 505 | Batch 50/100 | Loss 0.222121\n",
      "Epoch 505 | Batch 60/100 | Loss 0.226637\n",
      "Epoch 505 | Batch 70/100 | Loss 0.223318\n",
      "Epoch 505 | Batch 80/100 | Loss 0.217857\n",
      "Epoch 505 | Batch 90/100 | Loss 0.218129\n",
      "Epoch 505 | Batch 100/100 | Loss 0.215608\n",
      "Epoch 506 | Batch 10/100 | Loss 0.161289\n",
      "Epoch 506 | Batch 20/100 | Loss 0.198628\n",
      "Epoch 506 | Batch 30/100 | Loss 0.193437\n",
      "Epoch 506 | Batch 40/100 | Loss 0.195965\n",
      "Epoch 506 | Batch 50/100 | Loss 0.201563\n",
      "Epoch 506 | Batch 60/100 | Loss 0.203134\n",
      "Epoch 506 | Batch 70/100 | Loss 0.201516\n",
      "Epoch 506 | Batch 80/100 | Loss 0.200807\n",
      "Epoch 506 | Batch 90/100 | Loss 0.199408\n",
      "Epoch 506 | Batch 100/100 | Loss 0.198821\n",
      "Epoch 507 | Batch 10/100 | Loss 0.260728\n",
      "Epoch 507 | Batch 20/100 | Loss 0.216806\n",
      "Epoch 507 | Batch 30/100 | Loss 0.202267\n",
      "Epoch 507 | Batch 40/100 | Loss 0.200373\n",
      "Epoch 507 | Batch 50/100 | Loss 0.197926\n",
      "Epoch 507 | Batch 60/100 | Loss 0.196114\n",
      "Epoch 507 | Batch 70/100 | Loss 0.197234\n",
      "Epoch 507 | Batch 80/100 | Loss 0.192331\n",
      "Epoch 507 | Batch 90/100 | Loss 0.197383\n",
      "Epoch 507 | Batch 100/100 | Loss 0.198432\n",
      "Epoch 508 | Batch 10/100 | Loss 0.243412\n",
      "Epoch 508 | Batch 20/100 | Loss 0.230472\n",
      "Epoch 508 | Batch 30/100 | Loss 0.212212\n",
      "Epoch 508 | Batch 40/100 | Loss 0.213448\n",
      "Epoch 508 | Batch 50/100 | Loss 0.221122\n",
      "Epoch 508 | Batch 60/100 | Loss 0.233971\n",
      "Epoch 508 | Batch 70/100 | Loss 0.230401\n",
      "Epoch 508 | Batch 80/100 | Loss 0.226540\n",
      "Epoch 508 | Batch 90/100 | Loss 0.219024\n",
      "Epoch 508 | Batch 100/100 | Loss 0.215176\n",
      "Epoch 509 | Batch 10/100 | Loss 0.219553\n",
      "Epoch 509 | Batch 20/100 | Loss 0.215918\n",
      "Epoch 509 | Batch 30/100 | Loss 0.207542\n",
      "Epoch 509 | Batch 40/100 | Loss 0.208018\n",
      "Epoch 509 | Batch 50/100 | Loss 0.213206\n",
      "Epoch 509 | Batch 60/100 | Loss 0.210474\n",
      "Epoch 509 | Batch 70/100 | Loss 0.211782\n",
      "Epoch 509 | Batch 80/100 | Loss 0.208538\n",
      "Epoch 509 | Batch 90/100 | Loss 0.210048\n",
      "Epoch 509 | Batch 100/100 | Loss 0.211896\n",
      "Epoch 510 | Batch 10/100 | Loss 0.186126\n",
      "Epoch 510 | Batch 20/100 | Loss 0.221432\n",
      "Epoch 510 | Batch 30/100 | Loss 0.209302\n",
      "Epoch 510 | Batch 40/100 | Loss 0.213934\n",
      "Epoch 510 | Batch 50/100 | Loss 0.220270\n",
      "Epoch 510 | Batch 60/100 | Loss 0.219551\n",
      "Epoch 510 | Batch 70/100 | Loss 0.215986\n",
      "Epoch 510 | Batch 80/100 | Loss 0.220942\n",
      "Epoch 510 | Batch 90/100 | Loss 0.220411\n",
      "Epoch 510 | Batch 100/100 | Loss 0.223132\n",
      "Epoch 511 | Batch 10/100 | Loss 0.246055\n",
      "Epoch 511 | Batch 20/100 | Loss 0.240147\n",
      "Epoch 511 | Batch 30/100 | Loss 0.248285\n",
      "Epoch 511 | Batch 40/100 | Loss 0.238606\n",
      "Epoch 511 | Batch 50/100 | Loss 0.235116\n",
      "Epoch 511 | Batch 60/100 | Loss 0.235842\n",
      "Epoch 511 | Batch 70/100 | Loss 0.230855\n",
      "Epoch 511 | Batch 80/100 | Loss 0.222995\n",
      "Epoch 511 | Batch 90/100 | Loss 0.225253\n",
      "Epoch 511 | Batch 100/100 | Loss 0.219638\n",
      "Epoch 512 | Batch 10/100 | Loss 0.171453\n",
      "Epoch 512 | Batch 20/100 | Loss 0.211228\n",
      "Epoch 512 | Batch 30/100 | Loss 0.203651\n",
      "Epoch 512 | Batch 40/100 | Loss 0.198469\n",
      "Epoch 512 | Batch 50/100 | Loss 0.191543\n",
      "Epoch 512 | Batch 60/100 | Loss 0.190261\n",
      "Epoch 512 | Batch 70/100 | Loss 0.194904\n",
      "Epoch 512 | Batch 80/100 | Loss 0.193096\n",
      "Epoch 512 | Batch 90/100 | Loss 0.194141\n",
      "Epoch 512 | Batch 100/100 | Loss 0.194747\n",
      "Epoch 513 | Batch 10/100 | Loss 0.171462\n",
      "Epoch 513 | Batch 20/100 | Loss 0.178386\n",
      "Epoch 513 | Batch 30/100 | Loss 0.194176\n",
      "Epoch 513 | Batch 40/100 | Loss 0.209036\n",
      "Epoch 513 | Batch 50/100 | Loss 0.224591\n",
      "Epoch 513 | Batch 60/100 | Loss 0.223549\n",
      "Epoch 513 | Batch 70/100 | Loss 0.223708\n",
      "Epoch 513 | Batch 80/100 | Loss 0.221100\n",
      "Epoch 513 | Batch 90/100 | Loss 0.219655\n",
      "Epoch 513 | Batch 100/100 | Loss 0.213612\n",
      "Epoch 514 | Batch 10/100 | Loss 0.249941\n",
      "Epoch 514 | Batch 20/100 | Loss 0.216992\n",
      "Epoch 514 | Batch 30/100 | Loss 0.211703\n",
      "Epoch 514 | Batch 40/100 | Loss 0.209429\n",
      "Epoch 514 | Batch 50/100 | Loss 0.214009\n",
      "Epoch 514 | Batch 60/100 | Loss 0.214275\n",
      "Epoch 514 | Batch 70/100 | Loss 0.218659\n",
      "Epoch 514 | Batch 80/100 | Loss 0.224677\n",
      "Epoch 514 | Batch 90/100 | Loss 0.225542\n",
      "Epoch 514 | Batch 100/100 | Loss 0.225001\n",
      "Epoch 515 | Batch 10/100 | Loss 0.185725\n",
      "Epoch 515 | Batch 20/100 | Loss 0.222404\n",
      "Epoch 515 | Batch 30/100 | Loss 0.216692\n",
      "Epoch 515 | Batch 40/100 | Loss 0.212087\n",
      "Epoch 515 | Batch 50/100 | Loss 0.213965\n",
      "Epoch 515 | Batch 60/100 | Loss 0.216394\n",
      "Epoch 515 | Batch 70/100 | Loss 0.213598\n",
      "Epoch 515 | Batch 80/100 | Loss 0.211914\n",
      "Epoch 515 | Batch 90/100 | Loss 0.207057\n",
      "Epoch 515 | Batch 100/100 | Loss 0.202152\n",
      "Epoch 516 | Batch 10/100 | Loss 0.206425\n",
      "Epoch 516 | Batch 20/100 | Loss 0.207683\n",
      "Epoch 516 | Batch 30/100 | Loss 0.200191\n",
      "Epoch 516 | Batch 40/100 | Loss 0.209405\n",
      "Epoch 516 | Batch 50/100 | Loss 0.218825\n",
      "Epoch 516 | Batch 60/100 | Loss 0.212976\n",
      "Epoch 516 | Batch 70/100 | Loss 0.215270\n",
      "Epoch 516 | Batch 80/100 | Loss 0.212897\n",
      "Epoch 516 | Batch 90/100 | Loss 0.212524\n",
      "Epoch 516 | Batch 100/100 | Loss 0.214820\n",
      "Epoch 517 | Batch 10/100 | Loss 0.162686\n",
      "Epoch 517 | Batch 20/100 | Loss 0.193906\n",
      "Epoch 517 | Batch 30/100 | Loss 0.191649\n",
      "Epoch 517 | Batch 40/100 | Loss 0.191684\n",
      "Epoch 517 | Batch 50/100 | Loss 0.196683\n",
      "Epoch 517 | Batch 60/100 | Loss 0.202812\n",
      "Epoch 517 | Batch 70/100 | Loss 0.213415\n",
      "Epoch 517 | Batch 80/100 | Loss 0.213989\n",
      "Epoch 517 | Batch 90/100 | Loss 0.207113\n",
      "Epoch 517 | Batch 100/100 | Loss 0.205091\n",
      "Epoch 518 | Batch 10/100 | Loss 0.251366\n",
      "Epoch 518 | Batch 20/100 | Loss 0.192183\n",
      "Epoch 518 | Batch 30/100 | Loss 0.207201\n",
      "Epoch 518 | Batch 40/100 | Loss 0.199410\n",
      "Epoch 518 | Batch 50/100 | Loss 0.195309\n",
      "Epoch 518 | Batch 60/100 | Loss 0.193237\n",
      "Epoch 518 | Batch 70/100 | Loss 0.192122\n",
      "Epoch 518 | Batch 80/100 | Loss 0.198171\n",
      "Epoch 518 | Batch 90/100 | Loss 0.196982\n",
      "Epoch 518 | Batch 100/100 | Loss 0.199630\n",
      "Epoch 519 | Batch 10/100 | Loss 0.182460\n",
      "Epoch 519 | Batch 20/100 | Loss 0.166700\n",
      "Epoch 519 | Batch 30/100 | Loss 0.182834\n",
      "Epoch 519 | Batch 40/100 | Loss 0.195973\n",
      "Epoch 519 | Batch 50/100 | Loss 0.188914\n",
      "Epoch 519 | Batch 60/100 | Loss 0.202631\n",
      "Epoch 519 | Batch 70/100 | Loss 0.210654\n",
      "Epoch 519 | Batch 80/100 | Loss 0.208815\n",
      "Epoch 519 | Batch 90/100 | Loss 0.209081\n",
      "Epoch 519 | Batch 100/100 | Loss 0.211066\n",
      "Epoch 520 | Batch 10/100 | Loss 0.222608\n",
      "Epoch 520 | Batch 20/100 | Loss 0.237520\n",
      "Epoch 520 | Batch 30/100 | Loss 0.224057\n",
      "Epoch 520 | Batch 40/100 | Loss 0.227662\n",
      "Epoch 520 | Batch 50/100 | Loss 0.228602\n",
      "Epoch 520 | Batch 60/100 | Loss 0.217865\n",
      "Epoch 520 | Batch 70/100 | Loss 0.220841\n",
      "Epoch 520 | Batch 80/100 | Loss 0.228614\n",
      "Epoch 520 | Batch 90/100 | Loss 0.231403\n",
      "Epoch 520 | Batch 100/100 | Loss 0.227263\n",
      "Epoch 521 | Batch 10/100 | Loss 0.185882\n",
      "Epoch 521 | Batch 20/100 | Loss 0.221108\n",
      "Epoch 521 | Batch 30/100 | Loss 0.220768\n",
      "Epoch 521 | Batch 40/100 | Loss 0.222012\n",
      "Epoch 521 | Batch 50/100 | Loss 0.227175\n",
      "Epoch 521 | Batch 60/100 | Loss 0.216993\n",
      "Epoch 521 | Batch 70/100 | Loss 0.222133\n",
      "Epoch 521 | Batch 80/100 | Loss 0.221109\n",
      "Epoch 521 | Batch 90/100 | Loss 0.222595\n",
      "Epoch 521 | Batch 100/100 | Loss 0.224326\n",
      "100 Test Protonet Acc = 83.50% +- 1.32%\n",
      "best model! save...\n",
      "Epoch 522 | Batch 10/100 | Loss 0.237061\n",
      "Epoch 522 | Batch 20/100 | Loss 0.205785\n",
      "Epoch 522 | Batch 30/100 | Loss 0.223365\n",
      "Epoch 522 | Batch 40/100 | Loss 0.218958\n",
      "Epoch 522 | Batch 50/100 | Loss 0.210912\n",
      "Epoch 522 | Batch 60/100 | Loss 0.213594\n",
      "Epoch 522 | Batch 70/100 | Loss 0.214286\n",
      "Epoch 522 | Batch 80/100 | Loss 0.213276\n",
      "Epoch 522 | Batch 90/100 | Loss 0.218685\n",
      "Epoch 522 | Batch 100/100 | Loss 0.223385\n",
      "Epoch 523 | Batch 10/100 | Loss 0.195406\n",
      "Epoch 523 | Batch 20/100 | Loss 0.202116\n",
      "Epoch 523 | Batch 30/100 | Loss 0.198787\n",
      "Epoch 523 | Batch 40/100 | Loss 0.210483\n",
      "Epoch 523 | Batch 50/100 | Loss 0.208774\n",
      "Epoch 523 | Batch 60/100 | Loss 0.212587\n",
      "Epoch 523 | Batch 70/100 | Loss 0.222186\n",
      "Epoch 523 | Batch 80/100 | Loss 0.225875\n",
      "Epoch 523 | Batch 90/100 | Loss 0.222776\n",
      "Epoch 523 | Batch 100/100 | Loss 0.224018\n",
      "Epoch 524 | Batch 10/100 | Loss 0.196078\n",
      "Epoch 524 | Batch 20/100 | Loss 0.247150\n",
      "Epoch 524 | Batch 30/100 | Loss 0.252332\n",
      "Epoch 524 | Batch 40/100 | Loss 0.253237\n",
      "Epoch 524 | Batch 50/100 | Loss 0.244847\n",
      "Epoch 524 | Batch 60/100 | Loss 0.237832\n",
      "Epoch 524 | Batch 70/100 | Loss 0.233770\n",
      "Epoch 524 | Batch 80/100 | Loss 0.232479\n",
      "Epoch 524 | Batch 90/100 | Loss 0.230677\n",
      "Epoch 524 | Batch 100/100 | Loss 0.235515\n",
      "Epoch 525 | Batch 10/100 | Loss 0.224479\n",
      "Epoch 525 | Batch 20/100 | Loss 0.200217\n",
      "Epoch 525 | Batch 30/100 | Loss 0.209100\n",
      "Epoch 525 | Batch 40/100 | Loss 0.219741\n",
      "Epoch 525 | Batch 50/100 | Loss 0.224307\n",
      "Epoch 525 | Batch 60/100 | Loss 0.228050\n",
      "Epoch 525 | Batch 70/100 | Loss 0.230369\n",
      "Epoch 525 | Batch 80/100 | Loss 0.221885\n",
      "Epoch 525 | Batch 90/100 | Loss 0.217420\n",
      "Epoch 525 | Batch 100/100 | Loss 0.220172\n",
      "Epoch 526 | Batch 10/100 | Loss 0.241187\n",
      "Epoch 526 | Batch 20/100 | Loss 0.239627\n",
      "Epoch 526 | Batch 30/100 | Loss 0.230934\n",
      "Epoch 526 | Batch 40/100 | Loss 0.224071\n",
      "Epoch 526 | Batch 50/100 | Loss 0.223129\n",
      "Epoch 526 | Batch 60/100 | Loss 0.236444\n",
      "Epoch 526 | Batch 70/100 | Loss 0.235283\n",
      "Epoch 526 | Batch 80/100 | Loss 0.233593\n",
      "Epoch 526 | Batch 90/100 | Loss 0.234426\n",
      "Epoch 526 | Batch 100/100 | Loss 0.234233\n",
      "Epoch 527 | Batch 10/100 | Loss 0.225492\n",
      "Epoch 527 | Batch 20/100 | Loss 0.216531\n",
      "Epoch 527 | Batch 30/100 | Loss 0.216078\n",
      "Epoch 527 | Batch 40/100 | Loss 0.226026\n",
      "Epoch 527 | Batch 50/100 | Loss 0.218997\n",
      "Epoch 527 | Batch 60/100 | Loss 0.224820\n",
      "Epoch 527 | Batch 70/100 | Loss 0.225313\n",
      "Epoch 527 | Batch 80/100 | Loss 0.222058\n",
      "Epoch 527 | Batch 90/100 | Loss 0.219610\n",
      "Epoch 527 | Batch 100/100 | Loss 0.217808\n",
      "Epoch 528 | Batch 10/100 | Loss 0.183882\n",
      "Epoch 528 | Batch 20/100 | Loss 0.193748\n",
      "Epoch 528 | Batch 30/100 | Loss 0.210074\n",
      "Epoch 528 | Batch 40/100 | Loss 0.197349\n",
      "Epoch 528 | Batch 50/100 | Loss 0.199122\n",
      "Epoch 528 | Batch 60/100 | Loss 0.203445\n",
      "Epoch 528 | Batch 70/100 | Loss 0.212370\n",
      "Epoch 528 | Batch 80/100 | Loss 0.207562\n",
      "Epoch 528 | Batch 90/100 | Loss 0.207000\n",
      "Epoch 528 | Batch 100/100 | Loss 0.208774\n",
      "Epoch 529 | Batch 10/100 | Loss 0.176055\n",
      "Epoch 529 | Batch 20/100 | Loss 0.175102\n",
      "Epoch 529 | Batch 30/100 | Loss 0.182612\n",
      "Epoch 529 | Batch 40/100 | Loss 0.192615\n",
      "Epoch 529 | Batch 50/100 | Loss 0.205782\n",
      "Epoch 529 | Batch 60/100 | Loss 0.209985\n",
      "Epoch 529 | Batch 70/100 | Loss 0.212213\n",
      "Epoch 529 | Batch 80/100 | Loss 0.217301\n",
      "Epoch 529 | Batch 90/100 | Loss 0.216015\n",
      "Epoch 529 | Batch 100/100 | Loss 0.213158\n",
      "Epoch 530 | Batch 10/100 | Loss 0.200725\n",
      "Epoch 530 | Batch 20/100 | Loss 0.233185\n",
      "Epoch 530 | Batch 30/100 | Loss 0.224726\n",
      "Epoch 530 | Batch 40/100 | Loss 0.210060\n",
      "Epoch 530 | Batch 50/100 | Loss 0.200170\n",
      "Epoch 530 | Batch 60/100 | Loss 0.212684\n",
      "Epoch 530 | Batch 70/100 | Loss 0.215566\n",
      "Epoch 530 | Batch 80/100 | Loss 0.216172\n",
      "Epoch 530 | Batch 90/100 | Loss 0.215662\n",
      "Epoch 530 | Batch 100/100 | Loss 0.218117\n",
      "Epoch 531 | Batch 10/100 | Loss 0.241029\n",
      "Epoch 531 | Batch 20/100 | Loss 0.223069\n",
      "Epoch 531 | Batch 30/100 | Loss 0.212388\n",
      "Epoch 531 | Batch 40/100 | Loss 0.210932\n",
      "Epoch 531 | Batch 50/100 | Loss 0.204405\n",
      "Epoch 531 | Batch 60/100 | Loss 0.210255\n",
      "Epoch 531 | Batch 70/100 | Loss 0.210452\n",
      "Epoch 531 | Batch 80/100 | Loss 0.210816\n",
      "Epoch 531 | Batch 90/100 | Loss 0.213283\n",
      "Epoch 531 | Batch 100/100 | Loss 0.213135\n",
      "Epoch 532 | Batch 10/100 | Loss 0.250020\n",
      "Epoch 532 | Batch 20/100 | Loss 0.238533\n",
      "Epoch 532 | Batch 30/100 | Loss 0.236951\n",
      "Epoch 532 | Batch 40/100 | Loss 0.243990\n",
      "Epoch 532 | Batch 50/100 | Loss 0.244911\n",
      "Epoch 532 | Batch 60/100 | Loss 0.239604\n",
      "Epoch 532 | Batch 70/100 | Loss 0.235758\n",
      "Epoch 532 | Batch 80/100 | Loss 0.239974\n",
      "Epoch 532 | Batch 90/100 | Loss 0.235351\n",
      "Epoch 532 | Batch 100/100 | Loss 0.231015\n",
      "Epoch 533 | Batch 10/100 | Loss 0.209857\n",
      "Epoch 533 | Batch 20/100 | Loss 0.214768\n",
      "Epoch 533 | Batch 30/100 | Loss 0.214561\n",
      "Epoch 533 | Batch 40/100 | Loss 0.224219\n",
      "Epoch 533 | Batch 50/100 | Loss 0.223840\n",
      "Epoch 533 | Batch 60/100 | Loss 0.220390\n",
      "Epoch 533 | Batch 70/100 | Loss 0.219228\n",
      "Epoch 533 | Batch 80/100 | Loss 0.213473\n",
      "Epoch 533 | Batch 90/100 | Loss 0.212309\n",
      "Epoch 533 | Batch 100/100 | Loss 0.210273\n",
      "Epoch 534 | Batch 10/100 | Loss 0.235416\n",
      "Epoch 534 | Batch 20/100 | Loss 0.224506\n",
      "Epoch 534 | Batch 30/100 | Loss 0.227043\n",
      "Epoch 534 | Batch 40/100 | Loss 0.224066\n",
      "Epoch 534 | Batch 50/100 | Loss 0.218685\n",
      "Epoch 534 | Batch 60/100 | Loss 0.216587\n",
      "Epoch 534 | Batch 70/100 | Loss 0.211975\n",
      "Epoch 534 | Batch 80/100 | Loss 0.209214\n",
      "Epoch 534 | Batch 90/100 | Loss 0.206675\n",
      "Epoch 534 | Batch 100/100 | Loss 0.208722\n",
      "Epoch 535 | Batch 10/100 | Loss 0.250150\n",
      "Epoch 535 | Batch 20/100 | Loss 0.238133\n",
      "Epoch 535 | Batch 30/100 | Loss 0.224375\n",
      "Epoch 535 | Batch 40/100 | Loss 0.218592\n",
      "Epoch 535 | Batch 50/100 | Loss 0.215553\n",
      "Epoch 535 | Batch 60/100 | Loss 0.215749\n",
      "Epoch 535 | Batch 70/100 | Loss 0.216342\n",
      "Epoch 535 | Batch 80/100 | Loss 0.218264\n",
      "Epoch 535 | Batch 90/100 | Loss 0.218581\n",
      "Epoch 535 | Batch 100/100 | Loss 0.221715\n",
      "Epoch 536 | Batch 10/100 | Loss 0.215078\n",
      "Epoch 536 | Batch 20/100 | Loss 0.204485\n",
      "Epoch 536 | Batch 30/100 | Loss 0.222417\n",
      "Epoch 536 | Batch 40/100 | Loss 0.217733\n",
      "Epoch 536 | Batch 50/100 | Loss 0.227556\n",
      "Epoch 536 | Batch 60/100 | Loss 0.223693\n",
      "Epoch 536 | Batch 70/100 | Loss 0.224764\n",
      "Epoch 536 | Batch 80/100 | Loss 0.227682\n",
      "Epoch 536 | Batch 90/100 | Loss 0.220126\n",
      "Epoch 536 | Batch 100/100 | Loss 0.220260\n",
      "Epoch 537 | Batch 10/100 | Loss 0.225898\n",
      "Epoch 537 | Batch 20/100 | Loss 0.253551\n",
      "Epoch 537 | Batch 30/100 | Loss 0.258912\n",
      "Epoch 537 | Batch 40/100 | Loss 0.246330\n",
      "Epoch 537 | Batch 50/100 | Loss 0.231460\n",
      "Epoch 537 | Batch 60/100 | Loss 0.224284\n",
      "Epoch 537 | Batch 70/100 | Loss 0.223183\n",
      "Epoch 537 | Batch 80/100 | Loss 0.222024\n",
      "Epoch 537 | Batch 90/100 | Loss 0.221470\n",
      "Epoch 537 | Batch 100/100 | Loss 0.223464\n",
      "Epoch 538 | Batch 10/100 | Loss 0.223658\n",
      "Epoch 538 | Batch 20/100 | Loss 0.211907\n",
      "Epoch 538 | Batch 30/100 | Loss 0.204517\n",
      "Epoch 538 | Batch 40/100 | Loss 0.211854\n",
      "Epoch 538 | Batch 50/100 | Loss 0.212880\n",
      "Epoch 538 | Batch 60/100 | Loss 0.216961\n",
      "Epoch 538 | Batch 70/100 | Loss 0.223574\n",
      "Epoch 538 | Batch 80/100 | Loss 0.228230\n",
      "Epoch 538 | Batch 90/100 | Loss 0.227516\n",
      "Epoch 538 | Batch 100/100 | Loss 0.226251\n",
      "Epoch 539 | Batch 10/100 | Loss 0.202063\n",
      "Epoch 539 | Batch 20/100 | Loss 0.222096\n",
      "Epoch 539 | Batch 30/100 | Loss 0.225186\n",
      "Epoch 539 | Batch 40/100 | Loss 0.231319\n",
      "Epoch 539 | Batch 50/100 | Loss 0.238519\n",
      "Epoch 539 | Batch 60/100 | Loss 0.233138\n",
      "Epoch 539 | Batch 70/100 | Loss 0.225642\n",
      "Epoch 539 | Batch 80/100 | Loss 0.222909\n",
      "Epoch 539 | Batch 90/100 | Loss 0.228647\n",
      "Epoch 539 | Batch 100/100 | Loss 0.225333\n",
      "Epoch 540 | Batch 10/100 | Loss 0.248680\n",
      "Epoch 540 | Batch 20/100 | Loss 0.236915\n",
      "Epoch 540 | Batch 30/100 | Loss 0.225093\n",
      "Epoch 540 | Batch 40/100 | Loss 0.221382\n",
      "Epoch 540 | Batch 50/100 | Loss 0.220904\n",
      "Epoch 540 | Batch 60/100 | Loss 0.224060\n",
      "Epoch 540 | Batch 70/100 | Loss 0.231471\n",
      "Epoch 540 | Batch 80/100 | Loss 0.233012\n",
      "Epoch 540 | Batch 90/100 | Loss 0.236985\n",
      "Epoch 540 | Batch 100/100 | Loss 0.230525\n",
      "Epoch 541 | Batch 10/100 | Loss 0.274200\n",
      "Epoch 541 | Batch 20/100 | Loss 0.238593\n",
      "Epoch 541 | Batch 30/100 | Loss 0.241775\n",
      "Epoch 541 | Batch 40/100 | Loss 0.239103\n",
      "Epoch 541 | Batch 50/100 | Loss 0.231104\n",
      "Epoch 541 | Batch 60/100 | Loss 0.236621\n",
      "Epoch 541 | Batch 70/100 | Loss 0.236049\n",
      "Epoch 541 | Batch 80/100 | Loss 0.231912\n",
      "Epoch 541 | Batch 90/100 | Loss 0.225764\n",
      "Epoch 541 | Batch 100/100 | Loss 0.222002\n",
      "100 Test Protonet Acc = 82.06% +- 1.30%\n",
      "Epoch 542 | Batch 10/100 | Loss 0.261100\n",
      "Epoch 542 | Batch 20/100 | Loss 0.233427\n",
      "Epoch 542 | Batch 30/100 | Loss 0.219169\n",
      "Epoch 542 | Batch 40/100 | Loss 0.220912\n",
      "Epoch 542 | Batch 50/100 | Loss 0.213631\n",
      "Epoch 542 | Batch 60/100 | Loss 0.219060\n",
      "Epoch 542 | Batch 70/100 | Loss 0.211271\n",
      "Epoch 542 | Batch 80/100 | Loss 0.210131\n",
      "Epoch 542 | Batch 90/100 | Loss 0.209750\n",
      "Epoch 542 | Batch 100/100 | Loss 0.216442\n",
      "Epoch 543 | Batch 10/100 | Loss 0.236949\n",
      "Epoch 543 | Batch 20/100 | Loss 0.218025\n",
      "Epoch 543 | Batch 30/100 | Loss 0.206447\n",
      "Epoch 543 | Batch 40/100 | Loss 0.218555\n",
      "Epoch 543 | Batch 50/100 | Loss 0.220266\n",
      "Epoch 543 | Batch 60/100 | Loss 0.223263\n",
      "Epoch 543 | Batch 70/100 | Loss 0.215761\n",
      "Epoch 543 | Batch 80/100 | Loss 0.215453\n",
      "Epoch 543 | Batch 90/100 | Loss 0.216935\n",
      "Epoch 543 | Batch 100/100 | Loss 0.218241\n",
      "Epoch 544 | Batch 10/100 | Loss 0.184547\n",
      "Epoch 544 | Batch 20/100 | Loss 0.219214\n",
      "Epoch 544 | Batch 30/100 | Loss 0.218048\n",
      "Epoch 544 | Batch 40/100 | Loss 0.216768\n",
      "Epoch 544 | Batch 50/100 | Loss 0.206236\n",
      "Epoch 544 | Batch 60/100 | Loss 0.198415\n",
      "Epoch 544 | Batch 70/100 | Loss 0.205971\n",
      "Epoch 544 | Batch 80/100 | Loss 0.206951\n",
      "Epoch 544 | Batch 90/100 | Loss 0.212099\n",
      "Epoch 544 | Batch 100/100 | Loss 0.211191\n",
      "Epoch 545 | Batch 10/100 | Loss 0.306339\n",
      "Epoch 545 | Batch 20/100 | Loss 0.253353\n",
      "Epoch 545 | Batch 30/100 | Loss 0.259053\n",
      "Epoch 545 | Batch 40/100 | Loss 0.248414\n",
      "Epoch 545 | Batch 50/100 | Loss 0.245410\n",
      "Epoch 545 | Batch 60/100 | Loss 0.243574\n",
      "Epoch 545 | Batch 70/100 | Loss 0.240387\n",
      "Epoch 545 | Batch 80/100 | Loss 0.233547\n",
      "Epoch 545 | Batch 90/100 | Loss 0.231346\n",
      "Epoch 545 | Batch 100/100 | Loss 0.230745\n",
      "Epoch 546 | Batch 10/100 | Loss 0.240402\n",
      "Epoch 546 | Batch 20/100 | Loss 0.234591\n",
      "Epoch 546 | Batch 30/100 | Loss 0.227267\n",
      "Epoch 546 | Batch 40/100 | Loss 0.237731\n",
      "Epoch 546 | Batch 50/100 | Loss 0.232095\n",
      "Epoch 546 | Batch 60/100 | Loss 0.225386\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset dogs --train_aug --resume --resume_wandb_id 11vqc3r9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
