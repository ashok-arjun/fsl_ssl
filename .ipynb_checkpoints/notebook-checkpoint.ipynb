{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link './filelists/flowers/images/images': Read-only file system\n"
     ]
    }
   ],
   "source": [
    "# %cd ./filelists/CUB/\n",
    "# !ln -s  ../../../CUB_200_2011/images ./images\n",
    "# %cd ../../\n",
    "\n",
    "# !ln -s /kaggle/input/caltech-birds-2011-dataset/CUB_200_2011/images ./filelists/CUB/images\n",
    "\n",
    "# !ln -s /kaggle/input/miniimagenet/miniImageNet/images ./filelists/miniImagenet/images\n",
    "# !ln -s /kaggle/input/d/arjun2000ashok/tieredimagenet/tiered_imagenet/ ./filelists/tieredImagenet/images\n",
    "# !ln -s /kaggle/input/stanford-dogs-dataset/images/Images ./filelists/dogs/Images\n",
    "# !ln -s /kaggle/input/stanford-cars-dataset/cars_train/cars_train ./filelists/cars/images \n",
    "# !ln -s /kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data ./filelists/cars/images \n",
    "# !ln -s /kaggle/input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images ./filelists/aircrafts/images\n",
    "# !ln -s /kaggle/input/vggflowers/images ./filelists/flowers/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/fsl_ssl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_jigsaw_lbda0.50Adam_lr0.0010\n",
      "Fresh wandb run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20210608_075433-2otl6oqp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgood-sea-65\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl/runs/2otl6oqp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "About to start training. Last model and best model will be saved in wandb at every model save. \n",
      " Run save_features.py and test.py after the training completes, with the same arguments\n",
      "Epoch 0 | Batch 10/100 | Loss 12.797639 | Loss Proto 21.995384 | Loss Jigsaw 3.599893\n",
      "Epoch 0 | Batch 20/100 | Loss 8.834058 | Loss Proto 14.097727 | Loss Jigsaw 3.570389\n",
      "Epoch 0 | Batch 30/100 | Loss 7.000640 | Loss Proto 10.440374 | Loss Jigsaw 3.560908\n",
      "Epoch 0 | Batch 40/100 | Loss 5.926154 | Loss Proto 8.295093 | Loss Jigsaw 3.557215\n",
      "Epoch 0 | Batch 50/100 | Loss 5.227403 | Loss Proto 6.901468 | Loss Jigsaw 3.553335\n",
      "Epoch 0 | Batch 60/100 | Loss 4.766126 | Loss Proto 5.978988 | Loss Jigsaw 3.553263\n",
      "Epoch 0 | Batch 70/100 | Loss 4.427951 | Loss Proto 5.302829 | Loss Jigsaw 3.553074\n",
      "Epoch 0 | Batch 80/100 | Loss 4.177951 | Loss Proto 4.801950 | Loss Jigsaw 3.553952\n",
      "Epoch 0 | Batch 90/100 | Loss 3.976990 | Loss Proto 4.398911 | Loss Jigsaw 3.555070\n",
      "Epoch 0 | Batch 100/100 | Loss 3.816976 | Loss Proto 4.078184 | Loss Jigsaw 3.555768\n",
      "Epoch 1 | Batch 10/100 | Loss 2.333742 | Loss Proto 1.118193 | Loss Jigsaw 3.549290\n",
      "Epoch 1 | Batch 20/100 | Loss 2.321323 | Loss Proto 1.091826 | Loss Jigsaw 3.550819\n",
      "Epoch 1 | Batch 30/100 | Loss 2.313749 | Loss Proto 1.076132 | Loss Jigsaw 3.551366\n",
      "Epoch 1 | Batch 40/100 | Loss 2.325126 | Loss Proto 1.096921 | Loss Jigsaw 3.553331\n",
      "Epoch 1 | Batch 50/100 | Loss 2.328167 | Loss Proto 1.104093 | Loss Jigsaw 3.552240\n",
      "Epoch 1 | Batch 60/100 | Loss 2.325093 | Loss Proto 1.097928 | Loss Jigsaw 3.552258\n",
      "Epoch 1 | Batch 70/100 | Loss 2.325119 | Loss Proto 1.098055 | Loss Jigsaw 3.552182\n",
      "Epoch 1 | Batch 80/100 | Loss 2.324527 | Loss Proto 1.096615 | Loss Jigsaw 3.552439\n",
      "Epoch 1 | Batch 90/100 | Loss 2.321889 | Loss Proto 1.090510 | Loss Jigsaw 3.553269\n",
      "Epoch 1 | Batch 100/100 | Loss 2.312350 | Loss Proto 1.070696 | Loss Jigsaw 3.554004\n",
      "100 Test Protonet Acc = 63.44% +- 1.93%\n",
      "100 Test Jigsaw Acc = 3.66% +- 0.32%\n",
      "best model! save...\n",
      "Epoch 2 | Batch 10/100 | Loss 2.258460 | Loss Proto 0.973392 | Loss Jigsaw 3.543528\n",
      "Epoch 2 | Batch 20/100 | Loss 2.307984 | Loss Proto 1.069916 | Loss Jigsaw 3.546053\n",
      "Epoch 2 | Batch 30/100 | Loss 2.296966 | Loss Proto 1.046898 | Loss Jigsaw 3.547035\n",
      "Epoch 2 | Batch 40/100 | Loss 2.301122 | Loss Proto 1.052527 | Loss Jigsaw 3.549717\n",
      "Epoch 2 | Batch 50/100 | Loss 2.303423 | Loss Proto 1.058268 | Loss Jigsaw 3.548579\n",
      "Epoch 2 | Batch 60/100 | Loss 2.293672 | Loss Proto 1.037915 | Loss Jigsaw 3.549430\n",
      "Epoch 2 | Batch 70/100 | Loss 2.285028 | Loss Proto 1.020366 | Loss Jigsaw 3.549689\n",
      "Epoch 2 | Batch 80/100 | Loss 2.278735 | Loss Proto 1.007620 | Loss Jigsaw 3.549851\n",
      "Epoch 2 | Batch 90/100 | Loss 2.274804 | Loss Proto 0.998753 | Loss Jigsaw 3.550856\n",
      "Epoch 2 | Batch 100/100 | Loss 2.278841 | Loss Proto 1.006015 | Loss Jigsaw 3.551667\n",
      "Epoch 3 | Batch 10/100 | Loss 2.280269 | Loss Proto 1.019644 | Loss Jigsaw 3.540894\n",
      "Epoch 3 | Batch 20/100 | Loss 2.214992 | Loss Proto 0.886452 | Loss Jigsaw 3.543532\n",
      "Epoch 3 | Batch 30/100 | Loss 2.237593 | Loss Proto 0.930447 | Loss Jigsaw 3.544739\n",
      "Epoch 3 | Batch 40/100 | Loss 2.251423 | Loss Proto 0.955182 | Loss Jigsaw 3.547663\n",
      "Epoch 3 | Batch 50/100 | Loss 2.252905 | Loss Proto 0.959337 | Loss Jigsaw 3.546474\n",
      "Epoch 3 | Batch 60/100 | Loss 2.258155 | Loss Proto 0.968542 | Loss Jigsaw 3.547768\n",
      "Epoch 3 | Batch 70/100 | Loss 2.258624 | Loss Proto 0.968850 | Loss Jigsaw 3.548397\n",
      "Epoch 3 | Batch 80/100 | Loss 2.248989 | Loss Proto 0.949370 | Loss Jigsaw 3.548607\n",
      "Epoch 3 | Batch 90/100 | Loss 2.245485 | Loss Proto 0.941445 | Loss Jigsaw 3.549522\n",
      "Epoch 3 | Batch 100/100 | Loss 2.245565 | Loss Proto 0.940745 | Loss Jigsaw 3.550385\n",
      "Epoch 4 | Batch 10/100 | Loss 2.249469 | Loss Proto 0.959760 | Loss Jigsaw 3.539177\n",
      "Epoch 4 | Batch 20/100 | Loss 2.231377 | Loss Proto 0.920863 | Loss Jigsaw 3.541891\n",
      "Epoch 4 | Batch 30/100 | Loss 2.221968 | Loss Proto 0.900614 | Loss Jigsaw 3.543322\n",
      "Epoch 4 | Batch 40/100 | Loss 2.225255 | Loss Proto 0.904177 | Loss Jigsaw 3.546333\n",
      "Epoch 4 | Batch 50/100 | Loss 2.228047 | Loss Proto 0.910970 | Loss Jigsaw 3.545125\n",
      "Epoch 4 | Batch 60/100 | Loss 2.228322 | Loss Proto 0.910117 | Loss Jigsaw 3.546528\n",
      "Epoch 4 | Batch 70/100 | Loss 2.226491 | Loss Proto 0.905676 | Loss Jigsaw 3.547308\n",
      "Epoch 4 | Batch 80/100 | Loss 2.234053 | Loss Proto 0.920594 | Loss Jigsaw 3.547514\n",
      "Epoch 4 | Batch 90/100 | Loss 2.236927 | Loss Proto 0.925327 | Loss Jigsaw 3.548528\n",
      "Epoch 4 | Batch 100/100 | Loss 2.229244 | Loss Proto 0.909003 | Loss Jigsaw 3.549486\n",
      "Epoch 5 | Batch 10/100 | Loss 2.267877 | Loss Proto 0.997095 | Loss Jigsaw 3.538659\n",
      "Epoch 5 | Batch 20/100 | Loss 2.237943 | Loss Proto 0.934320 | Loss Jigsaw 3.541566\n",
      "Epoch 5 | Batch 30/100 | Loss 2.208110 | Loss Proto 0.873112 | Loss Jigsaw 3.543108\n",
      "Epoch 5 | Batch 40/100 | Loss 2.204216 | Loss Proto 0.862290 | Loss Jigsaw 3.546141\n",
      "Epoch 5 | Batch 50/100 | Loss 2.205818 | Loss Proto 0.866807 | Loss Jigsaw 3.544829\n",
      "Epoch 5 | Batch 60/100 | Loss 2.209945 | Loss Proto 0.873573 | Loss Jigsaw 3.546318\n",
      "Epoch 5 | Batch 70/100 | Loss 2.214976 | Loss Proto 0.883072 | Loss Jigsaw 3.546880\n",
      "Epoch 5 | Batch 80/100 | Loss 2.220677 | Loss Proto 0.894308 | Loss Jigsaw 3.547046\n",
      "Epoch 5 | Batch 90/100 | Loss 2.218356 | Loss Proto 0.888606 | Loss Jigsaw 3.548105\n",
      "Epoch 5 | Batch 100/100 | Loss 2.217762 | Loss Proto 0.886419 | Loss Jigsaw 3.549104\n",
      "Epoch 6 | Batch 10/100 | Loss 2.241591 | Loss Proto 0.945481 | Loss Jigsaw 3.537700\n",
      "Epoch 6 | Batch 20/100 | Loss 2.235038 | Loss Proto 0.929372 | Loss Jigsaw 3.540704\n",
      "Epoch 6 | Batch 30/100 | Loss 2.225448 | Loss Proto 0.908506 | Loss Jigsaw 3.542391\n",
      "Epoch 6 | Batch 40/100 | Loss 2.203742 | Loss Proto 0.862292 | Loss Jigsaw 3.545191\n",
      "Epoch 6 | Batch 50/100 | Loss 2.207405 | Loss Proto 0.870654 | Loss Jigsaw 3.544155\n",
      "Epoch 6 | Batch 60/100 | Loss 2.200425 | Loss Proto 0.855101 | Loss Jigsaw 3.545749\n",
      "Epoch 6 | Batch 70/100 | Loss 2.205520 | Loss Proto 0.864537 | Loss Jigsaw 3.546504\n",
      "Epoch 6 | Batch 80/100 | Loss 2.200180 | Loss Proto 0.853662 | Loss Jigsaw 3.546699\n",
      "Epoch 6 | Batch 90/100 | Loss 2.196778 | Loss Proto 0.845688 | Loss Jigsaw 3.547868\n",
      "Epoch 6 | Batch 100/100 | Loss 2.193834 | Loss Proto 0.838769 | Loss Jigsaw 3.548899\n",
      "Epoch 7 | Batch 10/100 | Loss 2.234192 | Loss Proto 0.934626 | Loss Jigsaw 3.533758\n",
      "Epoch 7 | Batch 20/100 | Loss 2.208091 | Loss Proto 0.877495 | Loss Jigsaw 3.538687\n",
      "Epoch 7 | Batch 30/100 | Loss 2.210796 | Loss Proto 0.880105 | Loss Jigsaw 3.541487\n",
      "Epoch 7 | Batch 40/100 | Loss 2.197907 | Loss Proto 0.850180 | Loss Jigsaw 3.545635\n",
      "Epoch 7 | Batch 50/100 | Loss 2.200097 | Loss Proto 0.854584 | Loss Jigsaw 3.545610\n",
      "Epoch 7 | Batch 60/100 | Loss 2.198081 | Loss Proto 0.848872 | Loss Jigsaw 3.547289\n",
      "Epoch 7 | Batch 70/100 | Loss 2.198476 | Loss Proto 0.849077 | Loss Jigsaw 3.547875\n",
      "Epoch 7 | Batch 80/100 | Loss 2.201657 | Loss Proto 0.854923 | Loss Jigsaw 3.548391\n",
      "Epoch 7 | Batch 90/100 | Loss 2.197480 | Loss Proto 0.845678 | Loss Jigsaw 3.549282\n",
      "Epoch 7 | Batch 100/100 | Loss 2.194831 | Loss Proto 0.839351 | Loss Jigsaw 3.550311\n",
      "Epoch 8 | Batch 10/100 | Loss 2.155467 | Loss Proto 0.773097 | Loss Jigsaw 3.537837\n",
      "Epoch 8 | Batch 20/100 | Loss 2.164509 | Loss Proto 0.787706 | Loss Jigsaw 3.541312\n",
      "Epoch 8 | Batch 30/100 | Loss 2.164283 | Loss Proto 0.785328 | Loss Jigsaw 3.543237\n",
      "Epoch 8 | Batch 40/100 | Loss 2.166016 | Loss Proto 0.785952 | Loss Jigsaw 3.546081\n",
      "Epoch 8 | Batch 50/100 | Loss 2.161231 | Loss Proto 0.777633 | Loss Jigsaw 3.544828\n",
      "Epoch 8 | Batch 60/100 | Loss 2.165129 | Loss Proto 0.783896 | Loss Jigsaw 3.546363\n",
      "Epoch 8 | Batch 70/100 | Loss 2.174800 | Loss Proto 0.802713 | Loss Jigsaw 3.546888\n",
      "Epoch 8 | Batch 80/100 | Loss 2.179937 | Loss Proto 0.812753 | Loss Jigsaw 3.547122\n",
      "Epoch 8 | Batch 90/100 | Loss 2.181531 | Loss Proto 0.815036 | Loss Jigsaw 3.548026\n",
      "Epoch 8 | Batch 100/100 | Loss 2.175928 | Loss Proto 0.802807 | Loss Jigsaw 3.549048\n",
      "Epoch 9 | Batch 10/100 | Loss 2.238434 | Loss Proto 0.942678 | Loss Jigsaw 3.534190\n",
      "Epoch 9 | Batch 20/100 | Loss 2.201643 | Loss Proto 0.865055 | Loss Jigsaw 3.538232\n",
      "Epoch 9 | Batch 30/100 | Loss 2.196012 | Loss Proto 0.851501 | Loss Jigsaw 3.540522\n",
      "Epoch 9 | Batch 40/100 | Loss 2.192174 | Loss Proto 0.840558 | Loss Jigsaw 3.543789\n",
      "Epoch 9 | Batch 50/100 | Loss 2.184736 | Loss Proto 0.825953 | Loss Jigsaw 3.543518\n",
      "Epoch 9 | Batch 60/100 | Loss 2.183821 | Loss Proto 0.821501 | Loss Jigsaw 3.546142\n",
      "Epoch 9 | Batch 70/100 | Loss 2.184961 | Loss Proto 0.822655 | Loss Jigsaw 3.547267\n",
      "Epoch 9 | Batch 80/100 | Loss 2.184395 | Loss Proto 0.820782 | Loss Jigsaw 3.548008\n",
      "Epoch 9 | Batch 90/100 | Loss 2.182223 | Loss Proto 0.815606 | Loss Jigsaw 3.548841\n",
      "Epoch 9 | Batch 100/100 | Loss 2.184865 | Loss Proto 0.820051 | Loss Jigsaw 3.549680\n",
      "Epoch 10 | Batch 10/100 | Loss 2.161634 | Loss Proto 0.790147 | Loss Jigsaw 3.533122\n",
      "Epoch 10 | Batch 20/100 | Loss 2.150087 | Loss Proto 0.761013 | Loss Jigsaw 3.539160\n",
      "Epoch 10 | Batch 30/100 | Loss 2.121473 | Loss Proto 0.700848 | Loss Jigsaw 3.542098\n",
      "Epoch 10 | Batch 40/100 | Loss 2.130071 | Loss Proto 0.714409 | Loss Jigsaw 3.545732\n",
      "Epoch 10 | Batch 50/100 | Loss 2.143355 | Loss Proto 0.741847 | Loss Jigsaw 3.544862\n",
      "Epoch 10 | Batch 60/100 | Loss 2.154423 | Loss Proto 0.762418 | Loss Jigsaw 3.546427\n",
      "Epoch 10 | Batch 70/100 | Loss 2.155738 | Loss Proto 0.764531 | Loss Jigsaw 3.546946\n",
      "Epoch 10 | Batch 80/100 | Loss 2.150828 | Loss Proto 0.754654 | Loss Jigsaw 3.547003\n",
      "Epoch 10 | Batch 90/100 | Loss 2.148699 | Loss Proto 0.749629 | Loss Jigsaw 3.547770\n",
      "Epoch 10 | Batch 100/100 | Loss 2.148054 | Loss Proto 0.747499 | Loss Jigsaw 3.548609\n",
      "Epoch 11 | Batch 10/100 | Loss 2.132496 | Loss Proto 0.731507 | Loss Jigsaw 3.533485\n",
      "Epoch 11 | Batch 20/100 | Loss 2.131091 | Loss Proto 0.724258 | Loss Jigsaw 3.537924\n",
      "Epoch 11 | Batch 30/100 | Loss 2.124579 | Loss Proto 0.708386 | Loss Jigsaw 3.540772\n",
      "Epoch 11 | Batch 40/100 | Loss 2.121581 | Loss Proto 0.698751 | Loss Jigsaw 3.544410\n",
      "Epoch 11 | Batch 50/100 | Loss 2.129919 | Loss Proto 0.716285 | Loss Jigsaw 3.543554\n",
      "Epoch 11 | Batch 60/100 | Loss 2.132499 | Loss Proto 0.719735 | Loss Jigsaw 3.545263\n",
      "Epoch 11 | Batch 70/100 | Loss 2.126748 | Loss Proto 0.707590 | Loss Jigsaw 3.545907\n",
      "Epoch 11 | Batch 80/100 | Loss 2.126339 | Loss Proto 0.706888 | Loss Jigsaw 3.545789\n",
      "Epoch 11 | Batch 90/100 | Loss 2.136889 | Loss Proto 0.727243 | Loss Jigsaw 3.546536\n",
      "Epoch 11 | Batch 100/100 | Loss 2.142166 | Loss Proto 0.736882 | Loss Jigsaw 3.547451\n",
      "Epoch 12 | Batch 10/100 | Loss 2.164740 | Loss Proto 0.796560 | Loss Jigsaw 3.532920\n",
      "Epoch 12 | Batch 20/100 | Loss 2.183226 | Loss Proto 0.828755 | Loss Jigsaw 3.537697\n",
      "Epoch 12 | Batch 30/100 | Loss 2.165801 | Loss Proto 0.793455 | Loss Jigsaw 3.538148\n",
      "Epoch 12 | Batch 40/100 | Loss 2.158445 | Loss Proto 0.776401 | Loss Jigsaw 3.540488\n",
      "Epoch 12 | Batch 50/100 | Loss 2.152420 | Loss Proto 0.764799 | Loss Jigsaw 3.540041\n",
      "Epoch 12 | Batch 60/100 | Loss 2.158714 | Loss Proto 0.774743 | Loss Jigsaw 3.542685\n",
      "Epoch 12 | Batch 70/100 | Loss 2.156486 | Loss Proto 0.769989 | Loss Jigsaw 3.542984\n",
      "Epoch 12 | Batch 80/100 | Loss 2.149370 | Loss Proto 0.753394 | Loss Jigsaw 3.545346\n",
      "Epoch 12 | Batch 90/100 | Loss 2.146566 | Loss Proto 0.746103 | Loss Jigsaw 3.547031\n",
      "Epoch 12 | Batch 100/100 | Loss 2.148449 | Loss Proto 0.748576 | Loss Jigsaw 3.548323\n",
      "Epoch 13 | Batch 10/100 | Loss 2.117167 | Loss Proto 0.705874 | Loss Jigsaw 3.528460\n",
      "Epoch 13 | Batch 20/100 | Loss 2.118023 | Loss Proto 0.699887 | Loss Jigsaw 3.536160\n",
      "Epoch 13 | Batch 30/100 | Loss 2.119544 | Loss Proto 0.699174 | Loss Jigsaw 3.539916\n",
      "Epoch 13 | Batch 40/100 | Loss 2.130362 | Loss Proto 0.715926 | Loss Jigsaw 3.544799\n",
      "Epoch 13 | Batch 50/100 | Loss 2.128766 | Loss Proto 0.712624 | Loss Jigsaw 3.544909\n",
      "Epoch 13 | Batch 60/100 | Loss 2.124973 | Loss Proto 0.702942 | Loss Jigsaw 3.547006\n",
      "Epoch 13 | Batch 70/100 | Loss 2.126029 | Loss Proto 0.704339 | Loss Jigsaw 3.547720\n",
      "Epoch 13 | Batch 80/100 | Loss 2.121710 | Loss Proto 0.695806 | Loss Jigsaw 3.547616\n",
      "Epoch 13 | Batch 90/100 | Loss 2.124218 | Loss Proto 0.699964 | Loss Jigsaw 3.548474\n",
      "Epoch 13 | Batch 100/100 | Loss 2.130317 | Loss Proto 0.711375 | Loss Jigsaw 3.549261\n",
      "Epoch 14 | Batch 10/100 | Loss 2.113105 | Loss Proto 0.692209 | Loss Jigsaw 3.534000\n",
      "Epoch 14 | Batch 20/100 | Loss 2.095529 | Loss Proto 0.653577 | Loss Jigsaw 3.537481\n",
      "Epoch 14 | Batch 30/100 | Loss 2.112801 | Loss Proto 0.685855 | Loss Jigsaw 3.539747\n",
      "Epoch 14 | Batch 40/100 | Loss 2.133547 | Loss Proto 0.723659 | Loss Jigsaw 3.543435\n",
      "Epoch 14 | Batch 50/100 | Loss 2.139922 | Loss Proto 0.736899 | Loss Jigsaw 3.542945\n",
      "Epoch 14 | Batch 60/100 | Loss 2.139066 | Loss Proto 0.733107 | Loss Jigsaw 3.545026\n",
      "Epoch 14 | Batch 70/100 | Loss 2.141507 | Loss Proto 0.736966 | Loss Jigsaw 3.546049\n",
      "Epoch 14 | Batch 80/100 | Loss 2.135918 | Loss Proto 0.725922 | Loss Jigsaw 3.545915\n",
      "Epoch 14 | Batch 90/100 | Loss 2.142125 | Loss Proto 0.737502 | Loss Jigsaw 3.546749\n",
      "Epoch 14 | Batch 100/100 | Loss 2.143610 | Loss Proto 0.739651 | Loss Jigsaw 3.547568\n",
      "Epoch 15 | Batch 10/100 | Loss 2.104031 | Loss Proto 0.675436 | Loss Jigsaw 3.532626\n",
      "Epoch 15 | Batch 20/100 | Loss 2.144532 | Loss Proto 0.752270 | Loss Jigsaw 3.536793\n",
      "Epoch 15 | Batch 30/100 | Loss 2.145281 | Loss Proto 0.751144 | Loss Jigsaw 3.539418\n",
      "Epoch 15 | Batch 40/100 | Loss 2.151088 | Loss Proto 0.758970 | Loss Jigsaw 3.543205\n",
      "Epoch 15 | Batch 50/100 | Loss 2.152318 | Loss Proto 0.762014 | Loss Jigsaw 3.542621\n",
      "Epoch 15 | Batch 60/100 | Loss 2.152298 | Loss Proto 0.760070 | Loss Jigsaw 3.544526\n",
      "Epoch 15 | Batch 70/100 | Loss 2.147385 | Loss Proto 0.749323 | Loss Jigsaw 3.545448\n",
      "Epoch 15 | Batch 80/100 | Loss 2.147342 | Loss Proto 0.749276 | Loss Jigsaw 3.545408\n",
      "Epoch 15 | Batch 90/100 | Loss 2.141457 | Loss Proto 0.736604 | Loss Jigsaw 3.546309\n",
      "Epoch 15 | Batch 100/100 | Loss 2.143714 | Loss Proto 0.740236 | Loss Jigsaw 3.547192\n",
      "Epoch 16 | Batch 10/100 | Loss 2.105436 | Loss Proto 0.678573 | Loss Jigsaw 3.532299\n",
      "Epoch 16 | Batch 20/100 | Loss 2.113804 | Loss Proto 0.690754 | Loss Jigsaw 3.536854\n",
      "Epoch 16 | Batch 30/100 | Loss 2.124020 | Loss Proto 0.708300 | Loss Jigsaw 3.539740\n",
      "Epoch 16 | Batch 40/100 | Loss 2.112996 | Loss Proto 0.682594 | Loss Jigsaw 3.543396\n",
      "Epoch 16 | Batch 50/100 | Loss 2.120585 | Loss Proto 0.698687 | Loss Jigsaw 3.542482\n",
      "Epoch 16 | Batch 60/100 | Loss 2.124978 | Loss Proto 0.705641 | Loss Jigsaw 3.544316\n",
      "Epoch 16 | Batch 70/100 | Loss 2.119009 | Loss Proto 0.692879 | Loss Jigsaw 3.545140\n",
      "Epoch 16 | Batch 80/100 | Loss 2.126107 | Loss Proto 0.707181 | Loss Jigsaw 3.545032\n",
      "Epoch 16 | Batch 90/100 | Loss 2.128687 | Loss Proto 0.711449 | Loss Jigsaw 3.545925\n",
      "Epoch 16 | Batch 100/100 | Loss 2.125493 | Loss Proto 0.704140 | Loss Jigsaw 3.546846\n",
      "Epoch 17 | Batch 10/100 | Loss 2.193179 | Loss Proto 0.853691 | Loss Jigsaw 3.532666\n",
      "Epoch 17 | Batch 20/100 | Loss 2.178327 | Loss Proto 0.819651 | Loss Jigsaw 3.537002\n",
      "Epoch 17 | Batch 30/100 | Loss 2.160341 | Loss Proto 0.780994 | Loss Jigsaw 3.539688\n",
      "Epoch 17 | Batch 40/100 | Loss 2.159922 | Loss Proto 0.776560 | Loss Jigsaw 3.543285\n",
      "Epoch 17 | Batch 50/100 | Loss 2.150704 | Loss Proto 0.759015 | Loss Jigsaw 3.542393\n",
      "Epoch 17 | Batch 60/100 | Loss 2.143841 | Loss Proto 0.743380 | Loss Jigsaw 3.544304\n",
      "Epoch 17 | Batch 70/100 | Loss 2.144331 | Loss Proto 0.743584 | Loss Jigsaw 3.545079\n",
      "Epoch 17 | Batch 80/100 | Loss 2.142758 | Loss Proto 0.740541 | Loss Jigsaw 3.544976\n",
      "Epoch 17 | Batch 90/100 | Loss 2.140449 | Loss Proto 0.734972 | Loss Jigsaw 3.545927\n",
      "Epoch 17 | Batch 100/100 | Loss 2.135157 | Loss Proto 0.723471 | Loss Jigsaw 3.546843\n",
      "Epoch 18 | Batch 10/100 | Loss 2.076995 | Loss Proto 0.622522 | Loss Jigsaw 3.531468\n",
      "Epoch 18 | Batch 20/100 | Loss 2.131597 | Loss Proto 0.727196 | Loss Jigsaw 3.535999\n",
      "Epoch 18 | Batch 30/100 | Loss 2.146157 | Loss Proto 0.753433 | Loss Jigsaw 3.538881\n",
      "Epoch 18 | Batch 40/100 | Loss 2.138242 | Loss Proto 0.734324 | Loss Jigsaw 3.542160\n",
      "Epoch 18 | Batch 50/100 | Loss 2.126033 | Loss Proto 0.710138 | Loss Jigsaw 3.541928\n",
      "Epoch 18 | Batch 60/100 | Loss 2.129547 | Loss Proto 0.714660 | Loss Jigsaw 3.544435\n",
      "Epoch 18 | Batch 70/100 | Loss 2.120116 | Loss Proto 0.694516 | Loss Jigsaw 3.545717\n",
      "Epoch 18 | Batch 80/100 | Loss 2.120118 | Loss Proto 0.694163 | Loss Jigsaw 3.546074\n",
      "Epoch 18 | Batch 90/100 | Loss 2.122292 | Loss Proto 0.697378 | Loss Jigsaw 3.547205\n",
      "Epoch 18 | Batch 100/100 | Loss 2.124462 | Loss Proto 0.700865 | Loss Jigsaw 3.548058\n",
      "Epoch 19 | Batch 10/100 | Loss 2.103546 | Loss Proto 0.675408 | Loss Jigsaw 3.531685\n",
      "Epoch 19 | Batch 20/100 | Loss 2.121975 | Loss Proto 0.707530 | Loss Jigsaw 3.536421\n",
      "Epoch 19 | Batch 30/100 | Loss 2.096633 | Loss Proto 0.653689 | Loss Jigsaw 3.539578\n",
      "Epoch 19 | Batch 40/100 | Loss 2.102785 | Loss Proto 0.661911 | Loss Jigsaw 3.543658\n",
      "Epoch 19 | Batch 50/100 | Loss 2.103309 | Loss Proto 0.663867 | Loss Jigsaw 3.542750\n",
      "Epoch 19 | Batch 60/100 | Loss 2.108399 | Loss Proto 0.672326 | Loss Jigsaw 3.544472\n",
      "Epoch 19 | Batch 70/100 | Loss 2.102947 | Loss Proto 0.660726 | Loss Jigsaw 3.545166\n",
      "Epoch 19 | Batch 80/100 | Loss 2.116266 | Loss Proto 0.687590 | Loss Jigsaw 3.544941\n",
      "Epoch 19 | Batch 90/100 | Loss 2.114490 | Loss Proto 0.683219 | Loss Jigsaw 3.545759\n",
      "Epoch 19 | Batch 100/100 | Loss 2.115163 | Loss Proto 0.683686 | Loss Jigsaw 3.546640\n",
      "Epoch 20 | Batch 10/100 | Loss 2.117763 | Loss Proto 0.703352 | Loss Jigsaw 3.532176\n",
      "Epoch 20 | Batch 20/100 | Loss 2.130253 | Loss Proto 0.724213 | Loss Jigsaw 3.536293\n",
      "Epoch 20 | Batch 30/100 | Loss 2.114331 | Loss Proto 0.689416 | Loss Jigsaw 3.539246\n",
      "Epoch 20 | Batch 40/100 | Loss 2.119829 | Loss Proto 0.696556 | Loss Jigsaw 3.543103\n",
      "Epoch 20 | Batch 50/100 | Loss 2.116938 | Loss Proto 0.691654 | Loss Jigsaw 3.542221\n",
      "Epoch 20 | Batch 60/100 | Loss 2.116202 | Loss Proto 0.688339 | Loss Jigsaw 3.544065\n",
      "Epoch 20 | Batch 70/100 | Loss 2.121184 | Loss Proto 0.697596 | Loss Jigsaw 3.544773\n",
      "Epoch 20 | Batch 80/100 | Loss 2.125176 | Loss Proto 0.705728 | Loss Jigsaw 3.544623\n",
      "Epoch 20 | Batch 90/100 | Loss 2.127279 | Loss Proto 0.709020 | Loss Jigsaw 3.545538\n",
      "Epoch 20 | Batch 100/100 | Loss 2.125930 | Loss Proto 0.705475 | Loss Jigsaw 3.546386\n",
      "Epoch 21 | Batch 10/100 | Loss 2.157734 | Loss Proto 0.788858 | Loss Jigsaw 3.526610\n",
      "Epoch 21 | Batch 20/100 | Loss 2.114790 | Loss Proto 0.694821 | Loss Jigsaw 3.534758\n",
      "Epoch 21 | Batch 30/100 | Loss 2.115400 | Loss Proto 0.691594 | Loss Jigsaw 3.539207\n",
      "Epoch 21 | Batch 40/100 | Loss 2.118398 | Loss Proto 0.692880 | Loss Jigsaw 3.543915\n",
      "Epoch 21 | Batch 50/100 | Loss 2.102333 | Loss Proto 0.660963 | Loss Jigsaw 3.543704\n",
      "Epoch 21 | Batch 60/100 | Loss 2.105671 | Loss Proto 0.665909 | Loss Jigsaw 3.545433\n",
      "Epoch 21 | Batch 70/100 | Loss 2.098789 | Loss Proto 0.651328 | Loss Jigsaw 3.546249\n",
      "Epoch 21 | Batch 80/100 | Loss 2.094735 | Loss Proto 0.642255 | Loss Jigsaw 3.547214\n",
      "Epoch 21 | Batch 90/100 | Loss 2.096969 | Loss Proto 0.645651 | Loss Jigsaw 3.548286\n",
      "Epoch 21 | Batch 100/100 | Loss 2.097551 | Loss Proto 0.645803 | Loss Jigsaw 3.549299\n",
      "100 Test Protonet Acc = 70.60% +- 1.77%\n",
      "100 Test Jigsaw Acc = 3.89% +- 0.39%\n",
      "best model! save...\n",
      "Epoch 22 | Batch 10/100 | Loss 2.098659 | Loss Proto 0.663015 | Loss Jigsaw 3.534302\n",
      "Epoch 22 | Batch 20/100 | Loss 2.119551 | Loss Proto 0.700794 | Loss Jigsaw 3.538308\n",
      "Epoch 22 | Batch 30/100 | Loss 2.124356 | Loss Proto 0.707720 | Loss Jigsaw 3.540993\n",
      "Epoch 22 | Batch 40/100 | Loss 2.115963 | Loss Proto 0.687032 | Loss Jigsaw 3.544895\n",
      "Epoch 22 | Batch 50/100 | Loss 2.121798 | Loss Proto 0.699573 | Loss Jigsaw 3.544023\n",
      "Epoch 22 | Batch 60/100 | Loss 2.116725 | Loss Proto 0.688005 | Loss Jigsaw 3.545445\n",
      "Epoch 22 | Batch 70/100 | Loss 2.113919 | Loss Proto 0.681947 | Loss Jigsaw 3.545892\n",
      "Epoch 22 | Batch 80/100 | Loss 2.118277 | Loss Proto 0.691010 | Loss Jigsaw 3.545544\n",
      "Epoch 22 | Batch 90/100 | Loss 2.116747 | Loss Proto 0.687219 | Loss Jigsaw 3.546276\n",
      "Epoch 22 | Batch 100/100 | Loss 2.121032 | Loss Proto 0.694886 | Loss Jigsaw 3.547179\n",
      "Epoch 23 | Batch 10/100 | Loss 2.100822 | Loss Proto 0.669064 | Loss Jigsaw 3.532580\n",
      "Epoch 23 | Batch 20/100 | Loss 2.081842 | Loss Proto 0.627093 | Loss Jigsaw 3.536591\n",
      "Epoch 23 | Batch 30/100 | Loss 2.101371 | Loss Proto 0.663714 | Loss Jigsaw 3.539029\n",
      "Epoch 23 | Batch 40/100 | Loss 2.103458 | Loss Proto 0.663889 | Loss Jigsaw 3.543029\n",
      "Epoch 23 | Batch 50/100 | Loss 2.084886 | Loss Proto 0.627654 | Loss Jigsaw 3.542118\n",
      "Epoch 23 | Batch 60/100 | Loss 2.083772 | Loss Proto 0.623456 | Loss Jigsaw 3.544088\n",
      "Epoch 23 | Batch 70/100 | Loss 2.082483 | Loss Proto 0.620077 | Loss Jigsaw 3.544890\n",
      "Epoch 23 | Batch 80/100 | Loss 2.086711 | Loss Proto 0.628660 | Loss Jigsaw 3.544764\n",
      "Epoch 23 | Batch 90/100 | Loss 2.088719 | Loss Proto 0.631757 | Loss Jigsaw 3.545681\n",
      "Epoch 23 | Batch 100/100 | Loss 2.094143 | Loss Proto 0.641638 | Loss Jigsaw 3.546648\n",
      "Epoch 24 | Batch 10/100 | Loss 2.034765 | Loss Proto 0.536743 | Loss Jigsaw 3.532787\n",
      "Epoch 24 | Batch 20/100 | Loss 2.038675 | Loss Proto 0.540547 | Loss Jigsaw 3.536803\n",
      "Epoch 24 | Batch 30/100 | Loss 2.042487 | Loss Proto 0.545482 | Loss Jigsaw 3.539492\n",
      "Epoch 24 | Batch 40/100 | Loss 2.046667 | Loss Proto 0.550323 | Loss Jigsaw 3.543011\n",
      "Epoch 24 | Batch 50/100 | Loss 2.062055 | Loss Proto 0.582141 | Loss Jigsaw 3.541968\n",
      "Epoch 24 | Batch 60/100 | Loss 2.059848 | Loss Proto 0.575983 | Loss Jigsaw 3.543712\n",
      "Epoch 24 | Batch 70/100 | Loss 2.067749 | Loss Proto 0.591063 | Loss Jigsaw 3.544435\n",
      "Epoch 24 | Batch 80/100 | Loss 2.074532 | Loss Proto 0.604737 | Loss Jigsaw 3.544327\n",
      "Epoch 24 | Batch 90/100 | Loss 2.080437 | Loss Proto 0.615627 | Loss Jigsaw 3.545246\n",
      "Epoch 24 | Batch 100/100 | Loss 2.079629 | Loss Proto 0.612989 | Loss Jigsaw 3.546269\n",
      "Epoch 25 | Batch 10/100 | Loss 2.120136 | Loss Proto 0.707958 | Loss Jigsaw 3.532315\n",
      "Epoch 25 | Batch 20/100 | Loss 2.093079 | Loss Proto 0.649748 | Loss Jigsaw 3.536409\n",
      "Epoch 25 | Batch 30/100 | Loss 2.104009 | Loss Proto 0.668964 | Loss Jigsaw 3.539054\n",
      "Epoch 25 | Batch 40/100 | Loss 2.111444 | Loss Proto 0.680186 | Loss Jigsaw 3.542702\n",
      "Epoch 25 | Batch 50/100 | Loss 2.108056 | Loss Proto 0.674501 | Loss Jigsaw 3.541610\n",
      "Epoch 25 | Batch 60/100 | Loss 2.106441 | Loss Proto 0.669348 | Loss Jigsaw 3.543535\n",
      "Epoch 25 | Batch 70/100 | Loss 2.110715 | Loss Proto 0.677041 | Loss Jigsaw 3.544389\n",
      "Epoch 25 | Batch 80/100 | Loss 2.110108 | Loss Proto 0.675917 | Loss Jigsaw 3.544299\n",
      "Epoch 25 | Batch 90/100 | Loss 2.112281 | Loss Proto 0.679289 | Loss Jigsaw 3.545273\n",
      "Epoch 25 | Batch 100/100 | Loss 2.107658 | Loss Proto 0.669075 | Loss Jigsaw 3.546241\n",
      "Epoch 26 | Batch 10/100 | Loss 2.156269 | Loss Proto 0.780514 | Loss Jigsaw 3.532025\n",
      "Epoch 26 | Batch 20/100 | Loss 2.078430 | Loss Proto 0.620839 | Loss Jigsaw 3.536022\n",
      "Epoch 26 | Batch 30/100 | Loss 2.093806 | Loss Proto 0.648987 | Loss Jigsaw 3.538626\n",
      "Epoch 26 | Batch 40/100 | Loss 2.084734 | Loss Proto 0.627084 | Loss Jigsaw 3.542384\n",
      "Epoch 26 | Batch 50/100 | Loss 2.073023 | Loss Proto 0.604719 | Loss Jigsaw 3.541328\n",
      "Epoch 26 | Batch 60/100 | Loss 2.070532 | Loss Proto 0.598066 | Loss Jigsaw 3.542998\n",
      "Epoch 26 | Batch 70/100 | Loss 2.070771 | Loss Proto 0.597869 | Loss Jigsaw 3.543674\n",
      "Epoch 26 | Batch 80/100 | Loss 2.076635 | Loss Proto 0.609177 | Loss Jigsaw 3.544094\n",
      "Epoch 26 | Batch 90/100 | Loss 2.081702 | Loss Proto 0.617909 | Loss Jigsaw 3.545496\n",
      "Epoch 26 | Batch 100/100 | Loss 2.081271 | Loss Proto 0.615567 | Loss Jigsaw 3.546973\n",
      "Epoch 27 | Batch 10/100 | Loss 2.139498 | Loss Proto 0.746390 | Loss Jigsaw 3.532607\n",
      "Epoch 27 | Batch 20/100 | Loss 2.135514 | Loss Proto 0.734307 | Loss Jigsaw 3.536720\n",
      "Epoch 27 | Batch 30/100 | Loss 2.121593 | Loss Proto 0.704064 | Loss Jigsaw 3.539122\n",
      "Epoch 27 | Batch 40/100 | Loss 2.122036 | Loss Proto 0.701361 | Loss Jigsaw 3.542710\n",
      "Epoch 27 | Batch 50/100 | Loss 2.116156 | Loss Proto 0.690494 | Loss Jigsaw 3.541817\n",
      "Epoch 27 | Batch 60/100 | Loss 2.115206 | Loss Proto 0.686677 | Loss Jigsaw 3.543735\n",
      "Epoch 27 | Batch 70/100 | Loss 2.101834 | Loss Proto 0.659026 | Loss Jigsaw 3.544642\n",
      "Epoch 27 | Batch 80/100 | Loss 2.094859 | Loss Proto 0.645124 | Loss Jigsaw 3.544594\n",
      "Epoch 27 | Batch 90/100 | Loss 2.090098 | Loss Proto 0.634575 | Loss Jigsaw 3.545621\n",
      "Epoch 27 | Batch 100/100 | Loss 2.088576 | Loss Proto 0.630590 | Loss Jigsaw 3.546561\n",
      "Epoch 28 | Batch 10/100 | Loss 2.107619 | Loss Proto 0.683438 | Loss Jigsaw 3.531799\n",
      "Epoch 28 | Batch 20/100 | Loss 2.112056 | Loss Proto 0.688261 | Loss Jigsaw 3.535851\n",
      "Epoch 28 | Batch 30/100 | Loss 2.096595 | Loss Proto 0.654711 | Loss Jigsaw 3.538479\n",
      "Epoch 28 | Batch 40/100 | Loss 2.108511 | Loss Proto 0.674863 | Loss Jigsaw 3.542159\n",
      "Epoch 28 | Batch 50/100 | Loss 2.100455 | Loss Proto 0.659633 | Loss Jigsaw 3.541277\n",
      "Epoch 28 | Batch 60/100 | Loss 2.097657 | Loss Proto 0.651946 | Loss Jigsaw 3.543367\n",
      "Epoch 28 | Batch 70/100 | Loss 2.105200 | Loss Proto 0.666126 | Loss Jigsaw 3.544273\n",
      "Epoch 28 | Batch 80/100 | Loss 2.101670 | Loss Proto 0.659112 | Loss Jigsaw 3.544227\n",
      "Epoch 28 | Batch 90/100 | Loss 2.104431 | Loss Proto 0.663648 | Loss Jigsaw 3.545213\n",
      "Epoch 28 | Batch 100/100 | Loss 2.098714 | Loss Proto 0.651275 | Loss Jigsaw 3.546153\n",
      "Epoch 29 | Batch 10/100 | Loss 2.037935 | Loss Proto 0.544344 | Loss Jigsaw 3.531526\n",
      "Epoch 29 | Batch 20/100 | Loss 2.029476 | Loss Proto 0.523272 | Loss Jigsaw 3.535680\n",
      "Epoch 29 | Batch 30/100 | Loss 2.016151 | Loss Proto 0.494039 | Loss Jigsaw 3.538265\n",
      "Epoch 29 | Batch 40/100 | Loss 2.029270 | Loss Proto 0.516397 | Loss Jigsaw 3.542143\n",
      "Epoch 29 | Batch 50/100 | Loss 2.041227 | Loss Proto 0.541124 | Loss Jigsaw 3.541331\n",
      "Epoch 29 | Batch 60/100 | Loss 2.052642 | Loss Proto 0.562014 | Loss Jigsaw 3.543272\n",
      "Epoch 29 | Batch 70/100 | Loss 2.050273 | Loss Proto 0.556202 | Loss Jigsaw 3.544346\n",
      "Epoch 29 | Batch 80/100 | Loss 2.057033 | Loss Proto 0.569743 | Loss Jigsaw 3.544324\n",
      "Epoch 29 | Batch 90/100 | Loss 2.055551 | Loss Proto 0.565789 | Loss Jigsaw 3.545314\n",
      "Epoch 29 | Batch 100/100 | Loss 2.053726 | Loss Proto 0.561184 | Loss Jigsaw 3.546270\n",
      "Epoch 30 | Batch 10/100 | Loss 2.017465 | Loss Proto 0.503658 | Loss Jigsaw 3.531273\n",
      "Epoch 30 | Batch 20/100 | Loss 2.033669 | Loss Proto 0.531909 | Loss Jigsaw 3.535429\n",
      "Epoch 30 | Batch 30/100 | Loss 2.076186 | Loss Proto 0.614248 | Loss Jigsaw 3.538125\n",
      "Epoch 30 | Batch 40/100 | Loss 2.061517 | Loss Proto 0.581128 | Loss Jigsaw 3.541906\n",
      "Epoch 30 | Batch 50/100 | Loss 2.063663 | Loss Proto 0.586391 | Loss Jigsaw 3.540935\n",
      "Epoch 30 | Batch 60/100 | Loss 2.062014 | Loss Proto 0.581083 | Loss Jigsaw 3.542944\n",
      "Epoch 30 | Batch 70/100 | Loss 2.073738 | Loss Proto 0.603565 | Loss Jigsaw 3.543911\n",
      "Epoch 30 | Batch 80/100 | Loss 2.066488 | Loss Proto 0.589137 | Loss Jigsaw 3.543839\n",
      "Epoch 30 | Batch 90/100 | Loss 2.069003 | Loss Proto 0.593089 | Loss Jigsaw 3.544918\n",
      "Epoch 30 | Batch 100/100 | Loss 2.075500 | Loss Proto 0.605101 | Loss Jigsaw 3.545899\n",
      "Epoch 31 | Batch 10/100 | Loss 2.017872 | Loss Proto 0.504463 | Loss Jigsaw 3.531281\n",
      "Epoch 31 | Batch 20/100 | Loss 2.039293 | Loss Proto 0.543122 | Loss Jigsaw 3.535464\n",
      "Epoch 31 | Batch 30/100 | Loss 2.046036 | Loss Proto 0.553976 | Loss Jigsaw 3.538095\n",
      "Epoch 31 | Batch 40/100 | Loss 2.065192 | Loss Proto 0.588439 | Loss Jigsaw 3.541945\n",
      "Epoch 31 | Batch 50/100 | Loss 2.072049 | Loss Proto 0.603152 | Loss Jigsaw 3.540946\n",
      "Epoch 31 | Batch 60/100 | Loss 2.077627 | Loss Proto 0.612274 | Loss Jigsaw 3.542980\n",
      "Epoch 31 | Batch 70/100 | Loss 2.078037 | Loss Proto 0.612133 | Loss Jigsaw 3.543941\n",
      "Epoch 31 | Batch 80/100 | Loss 2.069328 | Loss Proto 0.594740 | Loss Jigsaw 3.543915\n",
      "Epoch 31 | Batch 90/100 | Loss 2.070780 | Loss Proto 0.596630 | Loss Jigsaw 3.544929\n",
      "Epoch 31 | Batch 100/100 | Loss 2.067883 | Loss Proto 0.589831 | Loss Jigsaw 3.545934\n",
      "Epoch 32 | Batch 10/100 | Loss 2.030972 | Loss Proto 0.530504 | Loss Jigsaw 3.531439\n",
      "Epoch 32 | Batch 20/100 | Loss 2.010003 | Loss Proto 0.484474 | Loss Jigsaw 3.535533\n",
      "Epoch 32 | Batch 30/100 | Loss 2.025714 | Loss Proto 0.513281 | Loss Jigsaw 3.538147\n",
      "Epoch 32 | Batch 40/100 | Loss 2.023663 | Loss Proto 0.505344 | Loss Jigsaw 3.541981\n",
      "Epoch 32 | Batch 50/100 | Loss 2.035757 | Loss Proto 0.530573 | Loss Jigsaw 3.540941\n",
      "Epoch 32 | Batch 60/100 | Loss 2.034192 | Loss Proto 0.525488 | Loss Jigsaw 3.542896\n",
      "Epoch 32 | Batch 70/100 | Loss 2.038223 | Loss Proto 0.532554 | Loss Jigsaw 3.543893\n",
      "Epoch 32 | Batch 80/100 | Loss 2.045837 | Loss Proto 0.547838 | Loss Jigsaw 3.543837\n",
      "Epoch 32 | Batch 90/100 | Loss 2.050458 | Loss Proto 0.556066 | Loss Jigsaw 3.544849\n",
      "Epoch 32 | Batch 100/100 | Loss 2.050644 | Loss Proto 0.555443 | Loss Jigsaw 3.545846\n",
      "Epoch 33 | Batch 10/100 | Loss 2.142115 | Loss Proto 0.753597 | Loss Jigsaw 3.530632\n",
      "Epoch 33 | Batch 20/100 | Loss 2.136245 | Loss Proto 0.737418 | Loss Jigsaw 3.535072\n",
      "Epoch 33 | Batch 30/100 | Loss 2.128892 | Loss Proto 0.720055 | Loss Jigsaw 3.537729\n",
      "Epoch 33 | Batch 40/100 | Loss 2.124086 | Loss Proto 0.706621 | Loss Jigsaw 3.541551\n",
      "Epoch 33 | Batch 50/100 | Loss 2.108310 | Loss Proto 0.676187 | Loss Jigsaw 3.540433\n",
      "Epoch 33 | Batch 60/100 | Loss 2.087607 | Loss Proto 0.632576 | Loss Jigsaw 3.542639\n",
      "Epoch 33 | Batch 70/100 | Loss 2.080377 | Loss Proto 0.616962 | Loss Jigsaw 3.543792\n",
      "Epoch 33 | Batch 80/100 | Loss 2.074432 | Loss Proto 0.604825 | Loss Jigsaw 3.544039\n",
      "Epoch 33 | Batch 90/100 | Loss 2.066093 | Loss Proto 0.586997 | Loss Jigsaw 3.545189\n",
      "Epoch 33 | Batch 100/100 | Loss 2.066667 | Loss Proto 0.587117 | Loss Jigsaw 3.546218\n",
      "Epoch 34 | Batch 10/100 | Loss 2.093915 | Loss Proto 0.656598 | Loss Jigsaw 3.531231\n",
      "Epoch 34 | Batch 20/100 | Loss 2.064488 | Loss Proto 0.593528 | Loss Jigsaw 3.535447\n",
      "Epoch 34 | Batch 30/100 | Loss 2.069820 | Loss Proto 0.601436 | Loss Jigsaw 3.538204\n",
      "Epoch 34 | Batch 40/100 | Loss 2.082018 | Loss Proto 0.621933 | Loss Jigsaw 3.542103\n",
      "Epoch 34 | Batch 50/100 | Loss 2.077263 | Loss Proto 0.613416 | Loss Jigsaw 3.541111\n",
      "Epoch 34 | Batch 60/100 | Loss 2.074021 | Loss Proto 0.604952 | Loss Jigsaw 3.543091\n",
      "Epoch 34 | Batch 70/100 | Loss 2.067744 | Loss Proto 0.591422 | Loss Jigsaw 3.544066\n",
      "Epoch 34 | Batch 80/100 | Loss 2.060509 | Loss Proto 0.577049 | Loss Jigsaw 3.543968\n",
      "Epoch 34 | Batch 90/100 | Loss 2.060501 | Loss Proto 0.575998 | Loss Jigsaw 3.545004\n",
      "Epoch 34 | Batch 100/100 | Loss 2.065008 | Loss Proto 0.584082 | Loss Jigsaw 3.545933\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset flowers --train_aug --method protonet --committed --stop_epoch 400 --jigsaw --lbda 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmulti-input\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20210608_051230-14zlsjib\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mflowers protonet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/multi-input/fsl_ssl/runs/14zlsjib\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
      "\n",
      "Checkpoint restored at  ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/last_model.tar\n",
      "The model's epoch is  319\n",
      "Please rename it to continue training\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 837\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20210608_051230-14zlsjib/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20210608_051230-14zlsjib/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        _step 32589\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val/acc 90.4125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime 29963\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp 1623121580\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/loss 0.05676136165857315\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mflowers protonet\u001b[0m: \u001b[34mhttps://wandb.ai/multi-input/fsl_ssl/runs/14zlsjib\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !python wandb_restore.py --id 14zlsjib --path ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/last_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit after saving features and testing.\n",
      "checkpoint_dir: ckpts/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010\n",
      "USE BN: True\n",
      "outfile is features/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/novel.hdf5\n",
      "0/31\n",
      "10/31\n",
      "20/31\n",
      "30/31\n"
     ]
    }
   ],
   "source": [
    "# Do again with 200 dimensions\n",
    "# Do again with 399.tar\n",
    "\n",
    "!python save_features.py --dataset flowers --train_aug --method protonet --committed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing involves fine-tuning. Commit after testing.\n",
      "novel_file features/flowers/_resnet18_protonet_aug_5way_5shot_16query_tracking_lr0.0010/novel.hdf5\n",
      "600 Test Acc = 89.92% +- 0.51%\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataset flowers --train_aug --method protonet --committed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
